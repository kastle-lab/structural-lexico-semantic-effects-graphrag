{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc5a13d-de87-4e16-b693-8e15a8060440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "\n",
    "# Jonckheere–Terpstra might not exist on older SciPy; handle gracefully\n",
    "try:\n",
    "    from scipy.stats import jonckheere_terpstra\n",
    "    HAS_JT = True\n",
    "except Exception:\n",
    "    HAS_JT = False\n",
    "\n",
    "from scipy.stats import ConstantInputWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConstantInputWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42245dd6-e650-4d4b-a63c-08dc37fb1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(path: Path):\n",
    "    \"\"\"Load sheets and forward-fill KG/Model labels.\"\"\"\n",
    "    screen = pd.read_excel(path, sheet_name=\"Screen\")\n",
    "    screen_c = pd.read_excel(path, sheet_name=\"Screen with C\")\n",
    "    h1 = pd.read_excel(path, sheet_name=\"H1-AM\")\n",
    "    h1_c = pd.read_excel(path, sheet_name=\"H1-AM&C\")\n",
    "\n",
    "    for df in (screen, screen_c, h1, h1_c):\n",
    "        for col in [\"KG\", \"Model\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].ffill()\n",
    "\n",
    "    return screen, screen_c, h1, h1_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b0e2b6-f49f-41d2-acc5-1936e47bce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_screen_with_complexity(screen_c: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add per-complexity SCREEN rates based on the known CQ split:\n",
    "      - 12 Simple, 11 Moderate, 10 Complex (total 33).\n",
    "\n",
    "    Full Validity already implies syntax recognized + satisfiable + deterministic.\n",
    "    FV_rate is:\n",
    "        Full Validity / (#CQs at that complexity)\n",
    "    VS_rate is:\n",
    "        Valid Syntax recognized / (#CQs at that complexity)\n",
    "    \"\"\"\n",
    "    df = screen_c.copy()\n",
    "\n",
    "    df[\"denom\"] = df[\"Complexity\"].map(CQS_PER_COMPLEXITY).astype(float)\n",
    "\n",
    "    # Full-validity rate out of all CQs for that complexity\n",
    "    df[\"FV_rate\"] = df[\"Full Validity\"] / df[\"denom\"]\n",
    "    # Syntax-recognized rate out of all CQs for that complexity\n",
    "    df[\"VS_rate\"] = df[\"Valid Syntax recognized\"] / df[\"denom\"]\n",
    "\n",
    "    df[\"ComplexityLevel\"] = df[\"Complexity\"].map(\n",
    "        {\"Simple\": 1, \"Moderate\": 2, \"Complex\": 3}\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4673064-b95b-48ea-b4ab-97250410ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_h1_with_complexity(h1_c: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add numeric complexity level for H1 metrics.\n",
    "\n",
    "    H1 rate metrics:\n",
    "        syntax_ok_rate, satisfiable_rate, deterministic_rate, h1_overall.\n",
    "    Other H1 metrics (semantic ones) are already over valid queries only.\n",
    "    \"\"\"\n",
    "    df = h1_c.copy()\n",
    "    df[\"ComplexityLevel\"] = df[\"Complexity\"].map(\n",
    "        {\"Simple\": 1, \"Moderate\": 2, \"Complex\": 3}\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadaf46e-ca47-421d-b2fb-b073fbcf7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  ANALYSIS FUNCTIONS\n",
    "# =====================================================================\n",
    "\n",
    "def analyze_screen(screen_c_clean: pd.DataFrame):\n",
    "    print(\"\\n=== Part A: SCREEN metrics (task adherence) ===\\n\")\n",
    "\n",
    "    valid = screen_c_clean.dropna(subset=[\"FV_rate\", \"VS_rate\", \"Complexity\"])\n",
    "\n",
    "    # ----- Global Kruskal–Wallis tests on FV_rate and VS_rate -----\n",
    "    fv_groups = [g[\"FV_rate\"].values for _, g in valid.groupby(\"Complexity\")]\n",
    "    vs_groups = [g[\"VS_rate\"].values for _, g in valid.groupby(\"Complexity\")]\n",
    "\n",
    "    if len(fv_groups) >= 2:\n",
    "        kw_fv, kw_fv_p = stats.kruskal(*fv_groups)\n",
    "        print(\"Global Kruskal–Wallis on FV_rate across Complexity:\")\n",
    "        print(f\"  H = {kw_fv:.3f}, p = {kw_fv_p:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"Not enough groups for Kruskal–Wallis on FV_rate.\\n\")\n",
    "\n",
    "    if len(vs_groups) >= 2:\n",
    "        kw_vs, kw_vs_p = stats.kruskal(*vs_groups)\n",
    "        print(\"Global Kruskal–Wallis on VS_rate (syntax recognition) across Complexity:\")\n",
    "        print(f\"  H = {kw_vs:.3f}, p = {kw_vs_p:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"Not enough groups for Kruskal–Wallis on VS_rate.\\n\")\n",
    "\n",
    "    # ----- Global Spearman correlations -----\n",
    "    rho_fv, rho_fv_p = stats.spearmanr(\n",
    "        valid[\"ComplexityLevel\"], valid[\"FV_rate\"]\n",
    "    )\n",
    "    print(\"Global Spearman between ComplexityLevel and FV_rate:\")\n",
    "    print(f\"  rho = {rho_fv:.3f}, p = {rho_fv_p:.4f}\\n\")\n",
    "\n",
    "    rho_vs, rho_vs_p = stats.spearmanr(\n",
    "        valid[\"ComplexityLevel\"], valid[\"VS_rate\"]\n",
    "    )\n",
    "    print(\"Global Spearman between ComplexityLevel and VS_rate:\")\n",
    "    print(f\"  rho = {rho_vs:.3f}, p = {rho_vs_p:.4f}\\n\")\n",
    "\n",
    "    # ----- Per (KG, Model) Spearman correlations -----\n",
    "    per_pair = []\n",
    "    for (kg, model), grp in screen_c_clean.groupby([\"KG\", \"Model\"]):\n",
    "        g = grp.dropna(subset=[\"FV_rate\", \"VS_rate\", \"ComplexityLevel\"])\n",
    "        if g[\"ComplexityLevel\"].nunique() >= 2 and g[\"FV_rate\"].notna().sum() >= 2:\n",
    "            r_fv, p_fv = stats.spearmanr(g[\"ComplexityLevel\"], g[\"FV_rate\"])\n",
    "            r_vs, p_vs = stats.spearmanr(g[\"ComplexityLevel\"], g[\"VS_rate\"])\n",
    "            if not np.isnan(r_fv):\n",
    "                per_pair.append(\n",
    "                    {\n",
    "                        \"KG\": kg,\n",
    "                        \"Model\": model,\n",
    "                        \"rho_FV\": r_fv,\n",
    "                        \"p_FV\": p_fv,\n",
    "                        \"rho_VS\": r_vs,\n",
    "                        \"p_VS\": p_vs,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if per_pair:\n",
    "        sc_df = pd.DataFrame(per_pair)\n",
    "        print(\"Per (KG, Model) Spearman correlations for SCREEN (FV_rate & VS_rate):\")\n",
    "        print(sc_df.to_string(index=False))\n",
    "        print()\n",
    "\n",
    "        # Sign test: negative vs positive FV_rate correlations (ignoring zeros)\n",
    "        neg = (sc_df[\"rho_FV\"] < 0).sum()\n",
    "        pos = (sc_df[\"rho_FV\"] > 0).sum()\n",
    "        n = neg + pos\n",
    "        if n > 0:\n",
    "            p_sign = stats.binomtest(neg, n, 0.5, alternative=\"greater\").pvalue\n",
    "            print(\"Sign test for FV_rate correlations (negative > positive):\")\n",
    "            print(f\"  neg = {neg}, pos = {pos}, n = {n}, p = {p_sign:.4f}\\n\")\n",
    "        else:\n",
    "            print(\"No non-zero FV_rate correlations to test.\\n\")\n",
    "    else:\n",
    "        sc_df = pd.DataFrame()\n",
    "        print(\"No usable per-(KG,Model) correlations for SCREEN.\\n\")\n",
    "\n",
    "    return sc_df  # per-pair SCREEN correlations, for plotting/inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831135bc-f9e2-4d68-bf6d-2a7f7d374ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_h1(h1_c_clean: pd.DataFrame):\n",
    "    print(\"\\n=== Part B/C: H1 metrics (path reasoning) ===\\n\")\n",
    "\n",
    "    metrics = [\n",
    "        \"syntax_ok_rate\",\n",
    "        \"satisfiable_rate\",\n",
    "        \"deterministic_rate\",\n",
    "        \"h1_overall\",\n",
    "    ]\n",
    "\n",
    "    # ----- Global Kruskal–Wallis per metric across complexity -----\n",
    "    print(\"Global Kruskal–Wallis tests across Complexity for H1 metrics:\")\n",
    "    for metric in metrics:\n",
    "        valid = h1_c_clean.dropna(subset=[metric])\n",
    "        groups = [g[metric].values for _, g in valid.groupby(\"Complexity\")]\n",
    "        if len(groups) >= 2:\n",
    "            stat, p = stats.kruskal(*groups)\n",
    "            print(f\"  {metric:22s} H = {stat:.3f}, p = {p:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # ----- Per (KG, Model) Spearman correlations vs Complexity -----\n",
    "    per_pair = []\n",
    "    for (kg, model), grp in h1_c_clean.groupby([\"KG\", \"Model\"]):\n",
    "        if grp[\"ComplexityLevel\"].nunique() < 2:\n",
    "            continue\n",
    "        for metric in metrics:\n",
    "            g = grp.dropna(subset=[metric])\n",
    "            if g.shape[0] >= 2:\n",
    "                r, p = stats.spearmanr(g[\"ComplexityLevel\"], g[metric])\n",
    "                if not np.isnan(r):\n",
    "                    per_pair.append(\n",
    "                        {\n",
    "                            \"KG\": kg,\n",
    "                            \"Model\": model,\n",
    "                            \"metric\": metric,\n",
    "                            \"rho\": r,\n",
    "                            \"p\": p,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    per_pair_df = pd.DataFrame(per_pair)\n",
    "    if per_pair_df.empty:\n",
    "        print(\"No usable per-(KG,Model) correlations for H1 metrics.\\n\")\n",
    "        return per_pair_df\n",
    "\n",
    "    print(\"Per (KG, Model) Spearman correlations for H1 metrics (first few rows):\")\n",
    "    print(per_pair_df.head(20).to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # ----- Summary of correlations by metric -----\n",
    "    summary_corr = per_pair_df.groupby(\"metric\")[\"rho\"].agg(\n",
    "        [\"count\", \"mean\", \"median\"]\n",
    "    )\n",
    "    print(\"Summary of Spearman rho per metric (H1):\")\n",
    "    print(summary_corr.to_string())\n",
    "    print()\n",
    "\n",
    "    # ----- Sign tests: negative vs positive correlations -----\n",
    "    print(\"Sign tests for H1 metrics (is negative trend more common than positive?):\")\n",
    "    for metric, grp in per_pair_df.groupby(\"metric\"):\n",
    "        neg = (grp[\"rho\"] < 0).sum()\n",
    "        pos = (grp[\"rho\"] > 0).sum()\n",
    "        n = neg + pos\n",
    "        if n > 0:\n",
    "            p_val = stats.binomtest(neg, n, 0.5, alternative=\"greater\").pvalue\n",
    "            print(\n",
    "                f\"  {metric:22s} neg = {neg:2d}, pos = {pos:2d}, n = {n:2d}, p = {p_val:.4f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  {metric:22s} no non-zero correlations\")\n",
    "    print()\n",
    "\n",
    "    # ----- KG-level comparison: Big vs Small -----\n",
    "    print(\"Big vs Small KG comparison on H1 metrics (Mann–Whitney U):\")\n",
    "    small_big_results = []\n",
    "    for metric in metrics:\n",
    "        big_vals = h1_c_clean[h1_c_clean[\"KG\"] == \"Big\"][metric].dropna()\n",
    "        small_vals = h1_c_clean[h1_c_clean[\"KG\"] == \"Small\"][metric].dropna()\n",
    "        if len(big_vals) > 0 and len(small_vals) > 0:\n",
    "            u_stat, p = stats.mannwhitneyu(big_vals, small_vals, alternative=\"two-sided\")\n",
    "            small_big_results.append(\n",
    "                {\n",
    "                    \"metric\": metric,\n",
    "                    \"Big_mean\": big_vals.mean(),\n",
    "                    \"Small_mean\": small_vals.mean(),\n",
    "                    \"U\": u_stat,\n",
    "                    \"p\": p,\n",
    "                    \"n_big\": len(big_vals),\n",
    "                    \"n_small\": len(small_vals),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if small_big_results:\n",
    "        kg_df = pd.DataFrame(small_big_results)\n",
    "        print(kg_df.to_string(index=False))\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Not enough data for Big vs Small KG comparison.\\n\")\n",
    "\n",
    "    return per_pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db592f8-4e2d-4b0b-9a0f-0ee8ca3b9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  PLOTTING FUNCTIONS\n",
    "# =====================================================================\n",
    "\n",
    "def plot_screen_boxplots(screen_c_clean: pd.DataFrame):\n",
    "    \"\"\"Boxplots of FV_rate and VS_rate by Complexity.\"\"\"\n",
    "    for metric in [\"FV_rate\", \"VS_rate\"]:\n",
    "        plt.figure()\n",
    "        data = [g[metric].values for _, g in screen_c_clean.groupby(\"Complexity\")]\n",
    "        labels = [name for name, _ in screen_c_clean.groupby(\"Complexity\")]\n",
    "        plt.boxplot(data)\n",
    "        plt.xticks(range(1, len(labels) + 1), labels)\n",
    "        plt.ylabel(metric)\n",
    "        # plt.title(f\"{metric} by Complexity\")\n",
    "        out_path = OUT_DIR / f\"boxplot_SCREEN_{metric}.png\"\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccf50cb1-dcc6-458d-bfd9-47ac6aa4c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_screen_boxplots_by_kg(screen_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Boxplots of FV_rate and VS_rate by Complexity, separately for each KG.\n",
    "    Saves one PNG per (metric, KG).\n",
    "    \"\"\"\n",
    "    for metric in [\"FV_rate\", \"VS_rate\"]:\n",
    "        for kg, df_kg in screen_c_clean.groupby(\"KG\"):\n",
    "            plt.figure()\n",
    "            data = [g[metric].values for _, g in df_kg.groupby(\"Complexity\")]\n",
    "            labels = [name for name, _ in df_kg.groupby(\"Complexity\")]\n",
    "            plt.boxplot(data)\n",
    "            plt.xticks(range(1, len(labels) + 1), labels)\n",
    "            plt.ylabel(metric)\n",
    "            # plt.title(f\"{metric} by Complexity (KG={kg})\")  # optional\n",
    "            out_path = OUT_DIR / f\"boxplot_SCREEN_{metric}_byKG_{kg}.png\"\n",
    "            plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83969d9-effd-44ce-a2e8-a96892474e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  ADDITIONAL PLOTS: H1 metrics by KG × Complexity (ADDITIONS ONLY)\n",
    "# =====================================================================\n",
    "\n",
    "def plot_h1_boxplots_by_kg(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each KG, make boxplots of each H1 metric by Complexity.\n",
    "    Saves one PNG per (metric, KG).\n",
    "    \"\"\"\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        for kg, df_kg in h1_c_clean.groupby(\"KG\"):\n",
    "            plt.figure()\n",
    "            data = [g[metric].values for _, g in df_kg.groupby(\"Complexity\")]\n",
    "            labels = [name for name, _ in df_kg.groupby(\"Complexity\")]\n",
    "            plt.boxplot(data)\n",
    "            plt.xticks(range(1, len(labels) + 1), labels)\n",
    "            plt.ylabel(metric)\n",
    "            plt.title(f\"{metric} by Complexity (KG={kg})\")\n",
    "            out_path = OUT_DIR / f\"boxplot_H1_{metric}_byKG_{kg}.png\"\n",
    "            plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def plot_h1_boxplots_kg_effect_per_complexity(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each Complexity, make boxplots of each H1 metric by KG (Small vs Big).\n",
    "    Saves one PNG per (metric, Complexity).\n",
    "    \"\"\"\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "    kgs = [\"Small\", \"Big\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        for comp, df_comp in h1_c_clean.groupby(\"Complexity\"):\n",
    "            plt.figure()\n",
    "            data = [df_comp[df_comp[\"KG\"] == kg][metric].dropna().values for kg in kgs]\n",
    "            plt.boxplot(data)\n",
    "            plt.xticks([1, 2], kgs)\n",
    "            plt.ylabel(metric)\n",
    "            plt.title(f\"{metric} by KG (Complexity={comp})\")\n",
    "            out_path = OUT_DIR / f\"boxplot_H1_{metric}_KGeffect_{comp}.png\"\n",
    "            plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  ADDITIONAL STATS: H1 KG effect (Big vs Small) (ADDITIONS ONLY)\n",
    "# =====================================================================\n",
    "\n",
    "def _cliffs_delta(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cliff's delta effect size for two independent samples.\n",
    "    Returns delta in [-1, 1], where positive means x > y.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    x = x[~np.isnan(x)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # O(n*m) but small here\n",
    "    gt = 0\n",
    "    lt = 0\n",
    "    for xi in x:\n",
    "        gt += np.sum(xi > y)\n",
    "        lt += np.sum(xi < y)\n",
    "    return (gt - lt) / (len(x) * len(y))\n",
    "\n",
    "\n",
    "def h1_kg_effect_stats(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Summarize KG effect on H1 metrics:\n",
    "      1) Overall Big vs Small (Mann–Whitney U) pooled across complexities\n",
    "      2) Big vs Small within each Complexity\n",
    "    Also computes Cliff's delta as a nonparametric effect size.\n",
    "    Saves a CSV for inspection.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: H1 KG effect summary (Big vs Small) ===\\n\")\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "    complexities = [\"Simple\", \"Moderate\", \"Complex\"]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # (A) Overall pooled across complexities\n",
    "    for metric in metrics:\n",
    "        big_vals = h1_c_clean[h1_c_clean[\"KG\"] == \"Big\"][metric].dropna().values\n",
    "        small_vals = h1_c_clean[h1_c_clean[\"KG\"] == \"Small\"][metric].dropna().values\n",
    "        if len(big_vals) > 0 and len(small_vals) > 0:\n",
    "            u_stat, p = stats.mannwhitneyu(big_vals, small_vals, alternative=\"two-sided\")\n",
    "            # Cliff's delta: use (Big - Small) direction for interpretability\n",
    "            delta = _cliffs_delta(big_vals, small_vals)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"scope\": \"overall\",\n",
    "                    \"Complexity\": \"ALL\",\n",
    "                    \"metric\": metric,\n",
    "                    \"Big_mean\": np.mean(big_vals),\n",
    "                    \"Small_mean\": np.mean(small_vals),\n",
    "                    \"n_big\": len(big_vals),\n",
    "                    \"n_small\": len(small_vals),\n",
    "                    \"U\": u_stat,\n",
    "                    \"p\": p,\n",
    "                    \"cliffs_delta(Big-Small)\": delta,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # (B) Within each complexity\n",
    "    for comp in complexities:\n",
    "        df_comp = h1_c_clean[h1_c_clean[\"Complexity\"] == comp]\n",
    "        for metric in metrics:\n",
    "            big_vals = df_comp[df_comp[\"KG\"] == \"Big\"][metric].dropna().values\n",
    "            small_vals = df_comp[df_comp[\"KG\"] == \"Small\"][metric].dropna().values\n",
    "            if len(big_vals) > 0 and len(small_vals) > 0:\n",
    "                u_stat, p = stats.mannwhitneyu(big_vals, small_vals, alternative=\"two-sided\")\n",
    "                delta = _cliffs_delta(big_vals, small_vals)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"scope\": \"within_complexity\",\n",
    "                        \"Complexity\": comp,\n",
    "                        \"metric\": metric,\n",
    "                        \"Big_mean\": np.mean(big_vals),\n",
    "                        \"Small_mean\": np.mean(small_vals),\n",
    "                        \"n_big\": len(big_vals),\n",
    "                        \"n_small\": len(small_vals),\n",
    "                        \"U\": u_stat,\n",
    "                        \"p\": p,\n",
    "                        \"cliffs_delta(Big-Small)\": delta,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if not rows:\n",
    "        print(\"Not enough data to compute KG effect stats.\\n\")\n",
    "        return None\n",
    "\n",
    "    kg_effect_df = pd.DataFrame(rows)\n",
    "    print(kg_effect_df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    out_path = OUT_DIR / \"kg_effect_H1_mannwhitney_cliffsdelta.csv\"\n",
    "    kg_effect_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved KG effect stats to: {out_path}\\n\")\n",
    "\n",
    "    return kg_effect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b98190-867c-49a4-b4ea-0abb3aabd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h1_boxplots(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"Boxplots of each H1 metric by Complexity.\"\"\"\n",
    "    metrics = [\n",
    "        \"syntax_ok_rate\",\n",
    "        \"satisfiable_rate\",\n",
    "        \"deterministic_rate\",\n",
    "        \"h1_overall\",\n",
    "    ]\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        data = [g[metric].values for _, g in h1_c_clean.groupby(\"Complexity\")]\n",
    "        labels = [name for name, _ in h1_c_clean.groupby(\"Complexity\")]\n",
    "        plt.boxplot(data)\n",
    "        plt.xticks(range(1, len(labels) + 1), labels)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{metric} by Complexity\")\n",
    "        out_path = OUT_DIR / f\"boxplot_H1_{metric}.png\"\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e57e8993-6558-441a-a75e-5e2614d59f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h1_trendlines(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Line plots: ComplexityLevel vs metric for each Model (separate lines).\n",
    "    One plot per metric. Big+Small KG together, but labeled by model.\n",
    "    \"\"\"\n",
    "    metrics = [\n",
    "        \"syntax_ok_rate\",\n",
    "        \"satisfiable_rate\",\n",
    "        \"deterministic_rate\",\n",
    "        \"h1_overall\",\n",
    "    ]\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        for (kg, model), grp in h1_c_clean.groupby([\"KG\", \"Model\"]):\n",
    "            # Sort by ComplexityLevel to make lines consistent\n",
    "            g = grp.sort_values(\"ComplexityLevel\")\n",
    "            x = g[\"ComplexityLevel\"].values\n",
    "            y = g[metric].values\n",
    "            if len(x) >= 2 and not np.all(np.isnan(y)):\n",
    "                # Plot a line for this KG–model\n",
    "                label = f\"{kg}-{model}\"\n",
    "                plt.plot(x, y, marker=\"o\", label=label)\n",
    "        plt.xticks([1, 2, 3], [\"Simple\", \"Moderate\", \"Complex\"])\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{metric} vs Complexity (per KG–Model)\")\n",
    "        # If too many models, legend can get big; you can comment this out if noisy\n",
    "        plt.legend(fontsize=\"x-small\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        out_path = OUT_DIR / f\"trend_H1_{metric}.png\"\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec50e5a1-1280-4572-abef-bf149df1a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h1_rho_heatmap(per_pair_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Simple heatmap of rho values:\n",
    "      rows: (KG,Model)\n",
    "      cols: metrics\n",
    "    using imshow.\n",
    "    \"\"\"\n",
    "    if per_pair_df.empty:\n",
    "        return\n",
    "\n",
    "    metrics = [\n",
    "        \"syntax_ok_rate\",\n",
    "        \"satisfiable_rate\",\n",
    "        \"deterministic_rate\",\n",
    "        \"h1_overall\",\n",
    "    ]\n",
    "    # Pivot to (KG,Model) as index, metrics as columns\n",
    "    pivot_data = []\n",
    "    row_labels = []\n",
    "    for (kg, model), grp in per_pair_df.groupby([\"KG\", \"Model\"]):\n",
    "        row_labels.append(f\"{kg}-{model}\")\n",
    "        row = []\n",
    "        for m in metrics:\n",
    "            sub = grp[grp[\"metric\"] == m]\n",
    "            if not sub.empty:\n",
    "                row.append(sub[\"rho\"].iloc[0])\n",
    "            else:\n",
    "                row.append(np.nan)\n",
    "        pivot_data.append(row)\n",
    "\n",
    "    data = np.array(pivot_data, dtype=float)\n",
    "\n",
    "    plt.figure()\n",
    "    # Mask NaNs by turning them into 0, but you can also leave them; they'll appear as default color\n",
    "    nan_mask = np.isnan(data)\n",
    "    data_display = np.where(nan_mask, 0.0, data)\n",
    "\n",
    "    im = plt.imshow(data_display, aspect=\"auto\")\n",
    "    plt.colorbar(im, label=\"Spearman rho\")\n",
    "    plt.yticks(range(len(row_labels)), row_labels, fontsize=\"x-small\")\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Spearman rho (Complexity vs H1 metrics)\\nper KG–Model\")\n",
    "    out_path = OUT_DIR / \"heatmap_H1_rho.png\"\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af31135a-17aa-43be-b587-141fbd763a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_sensitivity(per_pair_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Summarize how sensitive each (KG,Model) is to complexity.\n",
    "    Uses the Spearman rho values from H1 analysis.\n",
    "    - Negative mean_rho  => performance tends to drop with complexity\n",
    "    - Near-zero mean_rho => relatively robust\n",
    "    \"\"\"\n",
    "    if per_pair_df.empty:\n",
    "        print(\"No per-pair H1 correlations to summarize.\\n\")\n",
    "        return None\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    # pivot to (KG,Model) x metric with rho values\n",
    "    pivot = (\n",
    "        per_pair_df\n",
    "        .pivot_table(index=[\"KG\", \"Model\"], columns=\"metric\", values=\"rho\")\n",
    "        .reindex(columns=metrics)\n",
    "    )\n",
    "\n",
    "    # average rho across metrics as a simple sensitivity score\n",
    "    pivot[\"mean_rho_across_H1\"] = pivot.mean(axis=1)\n",
    "\n",
    "    print(\"\\n=== Model complexity sensitivity (mean Spearman rho across H1 metrics) ===\")\n",
    "    print(\"(More negative = more performance drop as complexity increases)\\n\")\n",
    "    print(pivot.sort_values(\"mean_rho_across_H1\").to_string())\n",
    "    print()\n",
    "\n",
    "    # save for later inspection\n",
    "    out_path = OUT_DIR / \"model_complexity_sensitivity_H1.csv\"\n",
    "    pivot.to_csv(out_path)\n",
    "    print(f\"Saved model sensitivity table to: {out_path}\\n\")\n",
    "\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28ecd93-4fc0-4d4a-826b-53b10e2eef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_size_simple_vs_complex(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute Simple vs Complex effect sizes (Cohen's d)\n",
    "    for each H1 metric. This complements Kruskal–Wallis + rho\n",
    "    by giving a magnitude of the drop.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Simple vs Complex effect sizes (H1 metrics) ===\\n\")\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        simple = h1_c_clean[h1_c_clean[\"Complexity\"] == \"Simple\"][metric].dropna()\n",
    "        complex_ = h1_c_clean[h1_c_clean[\"Complexity\"] == \"Complex\"][metric].dropna()\n",
    "\n",
    "        if len(simple) < 2 or len(complex_) < 2:\n",
    "            print(f\"{metric:22s}: not enough data for effect size.\")\n",
    "            continue\n",
    "\n",
    "        mean_simple = simple.mean()\n",
    "        mean_complex = complex_.mean()\n",
    "        sd_simple = simple.std(ddof=1)\n",
    "        sd_complex = complex_.std(ddof=1)\n",
    "\n",
    "        # pooled standard deviation\n",
    "        n1, n2 = len(simple), len(complex_)\n",
    "        pooled_var = ((n1 - 1) * sd_simple**2 + (n2 - 1) * sd_complex**2) / (n1 + n2 - 2)\n",
    "        pooled_sd = np.sqrt(pooled_var)\n",
    "\n",
    "        d = (mean_simple - mean_complex) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        print(\n",
    "            f\"{metric:22s}: \"\n",
    "            f\"mean_Simple = {mean_simple:.3f}, mean_Complex = {mean_complex:.3f}, \"\n",
    "            f\"Cohen_d = {d:.3f} (n1={n1}, n2={n2})\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abed2cfc-ee0e-4aba-b3a2-c07b4e3b3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_screen_and_h1(sc_df: pd.DataFrame, per_pair_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Connect SCREEN and H1:\n",
    "    - Join per (KG,Model) FV_rate rho with H1 rhos.\n",
    "    - Check whether models whose SCREEN performance drops with complexity\n",
    "      also show strong H1 degradation.\n",
    "\n",
    "    Uses:\n",
    "      sc_df: output of analyze_screen (per-pair SCREEN correlations)\n",
    "      per_pair_df: output of analyze_h1 (per-pair H1 correlations)\n",
    "    \"\"\"\n",
    "    if sc_df.empty or per_pair_df.empty:\n",
    "        print(\"Need non-empty SCREEN and H1 per-pair data for correlation.\\n\")\n",
    "        return None\n",
    "\n",
    "    # We mostly care about overall H1 behaviour; start with h1_overall\n",
    "    h1_overall_rho = per_pair_df[per_pair_df[\"metric\"] == \"h1_overall\"].copy()\n",
    "\n",
    "    merged = h1_overall_rho.merge(\n",
    "        sc_df[[\"KG\", \"Model\", \"rho_FV\", \"rho_VS\"]],\n",
    "        on=[\"KG\", \"Model\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    if merged.empty:\n",
    "        print(\"No overlapping (KG,Model) rows between SCREEN and H1.\\n\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n=== Relationship between SCREEN and H1 complexity sensitivity ===\\n\")\n",
    "    print(\"Merged (KG,Model) rows (first few):\")\n",
    "    print(merged.head().to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Correlation of SCREEN FV_rate rho with H1 h1_overall rho\n",
    "    r_fv, p_fv = stats.pearsonr(merged[\"rho_FV\"], merged[\"rho\"])\n",
    "    r_vs, p_vs = stats.pearsonr(merged[\"rho_VS\"], merged[\"rho\"])\n",
    "\n",
    "    print(\"Correlation between SCREEN sensitivity and H1 sensitivity (per KG–Model):\")\n",
    "    print(\n",
    "        f\"  rho_FV (SCREEN)  vs rho(h1_overall): r = {r_fv:.3f}, p = {p_fv:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  rho_VS (SCREEN)  vs rho(h1_overall): r = {r_vs:.3f}, p = {p_vs:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "    # Save merged table for inspection\n",
    "    out_path = OUT_DIR / \"screen_vs_h1_sensitivity.csv\"\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"Saved SCREEN–H1 sensitivity table to: {out_path}\\n\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce2832e2-0203-40be-a044-a81fc0df583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_models_by_complexity(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each complexity level, compute mean h1_overall per Model\n",
    "    (aggregated over KGs) and print a ranking. This shows which models\n",
    "    remain robust as graphs get more complex.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Model rankings by h1_overall for each Complexity ===\\n\")\n",
    "\n",
    "    rows = []\n",
    "    for comp, grp in h1_c_clean.groupby(\"Complexity\"):\n",
    "        tmp = (\n",
    "            grp.groupby(\"Model\")[\"h1_overall\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"h1_overall\": \"mean_h1_overall\"})\n",
    "        )\n",
    "        tmp[\"Complexity\"] = comp\n",
    "        rows.append(tmp)\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No data to rank models.\\n\")\n",
    "        return None\n",
    "\n",
    "    ranking_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    for comp in [\"Simple\", \"Moderate\", \"Complex\"]:\n",
    "        sub = ranking_df[ranking_df[\"Complexity\"] == comp].sort_values(\n",
    "            \"mean_h1_overall\", ascending=False\n",
    "        )\n",
    "        print(f\"--- {comp} ---\")\n",
    "        print(sub.to_string(index=False))\n",
    "        print()\n",
    "\n",
    "    out_path = OUT_DIR / \"model_rankings_by_complexity_h1_overall.csv\"\n",
    "    ranking_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved rankings to: {out_path}\\n\")\n",
    "\n",
    "    return ranking_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0181bb3d-5bb5-4921-8b06-27dca00806f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  EXTRA STATISTICAL ANALYSES (do not modify existing ones)\n",
    "# =====================================================================\n",
    "\n",
    "def friedman_test_h1(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Friedman repeated-measures test across complexity levels, using\n",
    "    (KG,Model) pairs as blocks.\n",
    "\n",
    "    Tests whether Simple/Moderate/Complex differ for each H1 metric,\n",
    "    treating each model as its own control.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: Friedman Test (Repeated-Measures across models) ===\\n\")\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        # rows: (KG,Model)  cols: Simple/Moderate/Complex\n",
    "        pivot = h1_c_clean.pivot_table(\n",
    "            index=[\"KG\", \"Model\"],\n",
    "            columns=\"Complexity\",\n",
    "            values=metric\n",
    "        ).dropna(subset=[\"Simple\", \"Moderate\", \"Complex\"])\n",
    "\n",
    "        if pivot.shape[0] < 3:\n",
    "            print(f\"{metric:22s}: not enough (KG,Model) with all three complexities.\")\n",
    "            continue\n",
    "\n",
    "        S = pivot[\"Simple\"].values\n",
    "        M = pivot[\"Moderate\"].values\n",
    "        C = pivot[\"Complex\"].values\n",
    "\n",
    "        stat, p = friedmanchisquare(S, M, C)\n",
    "        print(f\"{metric:22s}: Friedman χ² = {stat:.3f}, p = {p:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def jt_test_h1(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Jonckheere–Terpstra trend test for ordered categories:\n",
    "    tests monotonic *decreasing* trend Simple -> Moderate -> Complex\n",
    "    for each H1 metric.\n",
    "\n",
    "    This directly encodes H1's directional hypothesis.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: Jonckheere–Terpstra Trend Test (monotonic decreasing) ===\\n\")\n",
    "\n",
    "    if not HAS_JT:\n",
    "        print(\"SciPy version does not have jonckheere_terpstra; skipping this test.\\n\")\n",
    "        return\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        g_simple   = h1_c_clean[h1_c_clean[\"Complexity\"] == \"Simple\"][metric].dropna()\n",
    "        g_moderate = h1_c_clean[h1_c_clean[\"Complexity\"] == \"Moderate\"][metric].dropna()\n",
    "        g_complex  = h1_c_clean[h1_c_clean[\"Complexity\"] == \"Complex\"][metric].dropna()\n",
    "\n",
    "        groups = [g_simple, g_moderate, g_complex]\n",
    "\n",
    "        if any(len(g) == 0 for g in groups):\n",
    "            print(f\"{metric:22s}: insufficient data for JT test.\")\n",
    "            continue\n",
    "\n",
    "        jt_stat, p = jonckheere_terpstra(\n",
    "            groups,\n",
    "            alternative=\"decreasing\"   # we expect performance to decrease as complexity increases\n",
    "        )\n",
    "        print(f\"{metric:22s}: JT = {jt_stat:.3f}, p = {p:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def kendalls_w_h1(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute Kendall's W (coefficient of concordance) across complexity levels\n",
    "    for each H1 metric.\n",
    "\n",
    "    W in [0,1]; higher means stronger agreement among (KG,Model) pairs about\n",
    "    the relative ranking of Simple/Moderate/Complex.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: Kendall's W (agreement across complexity levels) ===\\n\")\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        pivot = h1_c_clean.pivot_table(\n",
    "            index=[\"KG\", \"Model\"],\n",
    "            columns=\"Complexity\",\n",
    "            values=metric\n",
    "        ).dropna(subset=[\"Simple\", \"Moderate\", \"Complex\"])\n",
    "\n",
    "        if pivot.shape[0] < 3:\n",
    "            print(f\"{metric:22s}: insufficient models for Kendall's W.\")\n",
    "            continue\n",
    "\n",
    "        # ranks per row (each (KG,Model) ranks the three complexities)\n",
    "        ranks = pivot.rank(axis=1, ascending=True).values  # shape: n × k\n",
    "        n, k = ranks.shape\n",
    "\n",
    "        # Sum of ranks for each column (complexity)\n",
    "        R_j = np.sum(ranks, axis=0)\n",
    "        R_bar = n * (k + 1) / 2.0\n",
    "\n",
    "        S = np.sum((R_j - R_bar) ** 2)\n",
    "        W = 12 * S / (n**2 * (k**2 - 1))\n",
    "\n",
    "        print(f\"{metric:22s}: Kendall's W = {W:.3f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def pairwise_wilcoxon_h1(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Paired Wilcoxon tests per H1 metric:\n",
    "      - Simple vs Moderate\n",
    "      - Moderate vs Complex\n",
    "      - Simple vs Complex\n",
    "\n",
    "    Uses (KG,Model) as the paired unit.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: Pairwise Wilcoxon tests (Simple vs Moderate vs Complex) ===\\n\")\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        pivot = h1_c_clean.pivot_table(\n",
    "            index=[\"KG\", \"Model\"],\n",
    "            columns=\"Complexity\",\n",
    "            values=metric\n",
    "        ).dropna(subset=[\"Simple\", \"Moderate\", \"Complex\"])\n",
    "\n",
    "        if pivot.shape[0] < 3:\n",
    "            print(f\"{metric:22s}: not enough paired (KG,Model) rows.\")\n",
    "            continue\n",
    "\n",
    "        S = pivot[\"Simple\"]\n",
    "        M = pivot[\"Moderate\"]\n",
    "        C = pivot[\"Complex\"]\n",
    "\n",
    "        stat_SM, p_SM = wilcoxon(S, M)\n",
    "        stat_MC, p_MC = wilcoxon(M, C)\n",
    "        stat_SC, p_SC = wilcoxon(S, C)\n",
    "\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Simple vs Moderate : W = {stat_SM:.3f}, p = {p_SM:.4f}\")\n",
    "        print(f\"  Moderate vs Complex: W = {stat_MC:.3f}, p = {p_MC:.4f}\")\n",
    "        print(f\"  Simple vs Complex  : W = {stat_SC:.3f}, p = {p_SC:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e9fc212-0239-4454-aeda-89c4a320691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kg_complexity_interaction_tests(h1_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extra H1 analyses:\n",
    "      1) Big vs Small KG at each Complexity (Simple/Moderate/Complex)\n",
    "         - Wilcoxon with Model as the paired unit.\n",
    "      2) Complexity effect within each KG\n",
    "         - Friedman across Simple/Moderate/Complex (Model-level).\n",
    "      3) KG × Complexity interaction proxies:\n",
    "         - Compare Simple→Moderate drop on Small vs Big\n",
    "         - Compare Moderate→Complex drop on Small vs Big\n",
    "         - Compare Simple→Complex drop on Small vs Big\n",
    "\n",
    "    Assumes a balanced design over the 4 models:\n",
    "      Models: Claude, GPT5, Deepseek R1, GPT OSS\n",
    "      KGs:    Small, Big\n",
    "      Complexity: Simple, Moderate, Complex\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: KG × Complexity analyses for H1 metrics ===\\n\")\n",
    "\n",
    "    metrics = [\"syntax_ok_rate\", \"satisfiable_rate\", \"deterministic_rate\", \"h1_overall\"]\n",
    "    complexities = [\"Simple\", \"Moderate\", \"Complex\"]\n",
    "    kgs = [\"Small\", \"Big\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        print(f\"\\n--- Metric: {metric} ---\")\n",
    "\n",
    "        # Pivot: index = Model, columns = (KG, Complexity)\n",
    "        pivot = h1_c_clean.pivot_table(\n",
    "            index=\"Model\",\n",
    "            columns=[\"KG\", \"Complexity\"],\n",
    "            values=metric\n",
    "        )\n",
    "\n",
    "        # Ensure we have all KG/Complexity combos\n",
    "        missing_cols = []\n",
    "        for kg in kgs:\n",
    "            for comp in complexities:\n",
    "                if (kg, comp) not in pivot.columns:\n",
    "                    missing_cols.append((kg, comp))\n",
    "        if missing_cols:\n",
    "            print(f\"  Skipping {metric}: missing cells {missing_cols}\")\n",
    "            continue\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # (1) Big vs Small at each Complexity level\n",
    "        # ------------------------------------------------------\n",
    "        print(\"  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\")\n",
    "        for comp in complexities:\n",
    "            small_vals = pivot[(\"Small\", comp)].dropna()\n",
    "            big_vals   = pivot[(\"Big\", comp)].dropna()\n",
    "\n",
    "            # Need at least 2 paired models\n",
    "            common_index = small_vals.index.intersection(big_vals.index)\n",
    "            if len(common_index) < 2:\n",
    "                print(f\"    {comp:9s}: not enough paired models.\")\n",
    "                continue\n",
    "\n",
    "            s = small_vals.loc[common_index]\n",
    "            b = big_vals.loc[common_index]\n",
    "\n",
    "            stat, p = wilcoxon(s, b)\n",
    "            print(\n",
    "                f\"    {comp:9s}: \"\n",
    "                f\"Small_mean = {s.mean():.3f}, Big_mean = {b.mean():.3f}, \"\n",
    "                f\"W = {stat:.3f}, p = {p:.4f}\"\n",
    "            )\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # (2) Complexity effect within each KG (Friedman)\n",
    "        # ------------------------------------------------------\n",
    "        print(\"  Complexity effect within each KG (Friedman, Model-level):\")\n",
    "        for kg in kgs:\n",
    "            sub = pivot[kg].copy()  # columns: Simple, Moderate, Complex\n",
    "            sub = sub.dropna(subset=complexities)\n",
    "            if sub.shape[0] < 2:\n",
    "                print(f\"    {kg:5s}: not enough complete models for Friedman.\")\n",
    "                continue\n",
    "\n",
    "            S = sub[\"Simple\"].values\n",
    "            M = sub[\"Moderate\"].values\n",
    "            C = sub[\"Complex\"].values\n",
    "\n",
    "            stat, p = friedmanchisquare(S, M, C)\n",
    "            print(f\"    {kg:5s}: Friedman χ² = {stat:.3f}, p = {p:.4f}\")\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # (3) KG × Complexity interaction proxies:\n",
    "        #     Simple→Moderate, Moderate→Complex, Simple→Complex\n",
    "        # ------------------------------------------------------\n",
    "        print(\"  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\")\n",
    "\n",
    "        small_simple   = pivot[(\"Small\", \"Simple\")].dropna()\n",
    "        small_moderate = pivot[(\"Small\", \"Moderate\")].dropna()\n",
    "        small_complex  = pivot[(\"Small\", \"Complex\")].dropna()\n",
    "        big_simple     = pivot[(\"Big\", \"Simple\")].dropna()\n",
    "        big_moderate   = pivot[(\"Big\", \"Moderate\")].dropna()\n",
    "        big_complex    = pivot[(\"Big\", \"Complex\")].dropna()\n",
    "\n",
    "        # Align indices across all six cells\n",
    "        common_index = (\n",
    "            small_simple.index\n",
    "            .intersection(small_moderate.index)\n",
    "            .intersection(small_complex.index)\n",
    "            .intersection(big_simple.index)\n",
    "            .intersection(big_moderate.index)\n",
    "            .intersection(big_complex.index)\n",
    "        )\n",
    "\n",
    "        if len(common_index) < 2:\n",
    "            print(\"    Not enough models with all six cells (Small/Big × Simple/Moderate/Complex).\")\n",
    "            continue\n",
    "\n",
    "        ss = small_simple.loc[common_index]\n",
    "        sm = small_moderate.loc[common_index]\n",
    "        sc = small_complex.loc[common_index]\n",
    "        bs = big_simple.loc[common_index]\n",
    "        bm = big_moderate.loc[common_index]\n",
    "        bc = big_complex.loc[common_index]\n",
    "\n",
    "        # Simple -> Moderate drop\n",
    "        drop_small_SM = ss - sm\n",
    "        drop_big_SM   = bs - bm\n",
    "        stat_sm, p_sm = wilcoxon(drop_small_SM, drop_big_SM)\n",
    "        print(\n",
    "            f\"    Simple→Moderate drop: \"\n",
    "            f\"mean_drop_Small = {drop_small_SM.mean():.3f}, \"\n",
    "            f\"mean_drop_Big = {drop_big_SM.mean():.3f}, \"\n",
    "            f\"W = {stat_sm:.3f}, p = {p_sm:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Moderate -> Complex drop\n",
    "        drop_small_MC = sm - sc\n",
    "        drop_big_MC   = bm - bc\n",
    "        stat_mc, p_mc = wilcoxon(drop_small_MC, drop_big_MC)\n",
    "        print(\n",
    "            f\"    Moderate→Complex drop: \"\n",
    "            f\"mean_drop_Small = {drop_small_MC.mean():.3f}, \"\n",
    "            f\"mean_drop_Big = {drop_big_MC.mean():.3f}, \"\n",
    "            f\"W = {stat_mc:.3f}, p = {p_mc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Simple -> Complex drop\n",
    "        drop_small_SC = ss - sc\n",
    "        drop_big_SC   = bs - bc\n",
    "        stat_sc2, p_sc2 = wilcoxon(drop_small_SC, drop_big_SC)\n",
    "        print(\n",
    "            f\"    Simple→Complex drop: \"\n",
    "            f\"mean_drop_Small = {drop_small_SC.mean():.3f}, \"\n",
    "            f\"mean_drop_Big = {drop_big_SC.mean():.3f}, \"\n",
    "            f\"W = {stat_sc2:.3f}, p = {p_sc2:.4f}\"\n",
    "        )\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717326ac-4b90-42ea-b933-fbed3391435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: consolidated_results_filter_only4.xlsx\n",
      "\n",
      "=== Part A: SCREEN metrics (task adherence) ===\n",
      "\n",
      "Global Kruskal–Wallis on FV_rate across Complexity:\n",
      "  H = 2.550, p = 0.2794\n",
      "\n",
      "Global Kruskal–Wallis on VS_rate (syntax recognition) across Complexity:\n",
      "  H = 3.016, p = 0.2214\n",
      "\n",
      "Global Spearman between ComplexityLevel and FV_rate:\n",
      "  rho = -0.295, p = 0.1610\n",
      "\n",
      "Global Spearman between ComplexityLevel and VS_rate:\n",
      "  rho = -0.362, p = 0.0825\n",
      "\n",
      "Per (KG, Model) Spearman correlations for SCREEN (FV_rate & VS_rate):\n",
      "   KG       Model  rho_FV     p_FV    rho_VS     p_VS\n",
      "  Big      Claude    -1.0 0.000000 -1.000000 0.000000\n",
      "  Big Deepseek R1     0.5 0.666667  0.500000 0.666667\n",
      "  Big     GPT OSS     1.0 0.000000  0.500000 0.666667\n",
      "  Big        GPT5    -0.5 0.666667 -1.000000 0.000000\n",
      "Small      Claude    -1.0 0.000000 -1.000000 0.000000\n",
      "Small Deepseek R1    -1.0 0.000000 -1.000000 0.000000\n",
      "Small     GPT OSS    -1.0 0.000000 -0.866025 0.333333\n",
      "Small        GPT5    -1.0 0.000000 -1.000000 0.000000\n",
      "\n",
      "Sign test for FV_rate correlations (negative > positive):\n",
      "  neg = 6, pos = 2, n = 8, p = 0.1445\n",
      "\n",
      "\n",
      "=== Part B/C: H1 metrics (path reasoning) ===\n",
      "\n",
      "Global Kruskal–Wallis tests across Complexity for H1 metrics:\n",
      "  syntax_ok_rate         H = 3.016, p = 0.2214\n",
      "  satisfiable_rate       H = 2.550, p = 0.2794\n",
      "  deterministic_rate     H = 3.016, p = 0.2214\n",
      "  h1_overall             H = 9.608, p = 0.0082\n",
      "\n",
      "Per (KG, Model) Spearman correlations for H1 metrics (first few rows):\n",
      "   KG       Model             metric  rho        p\n",
      "  Big      Claude     syntax_ok_rate -1.0 0.000000\n",
      "  Big      Claude   satisfiable_rate -1.0 0.000000\n",
      "  Big      Claude deterministic_rate -1.0 0.000000\n",
      "  Big      Claude         h1_overall -1.0 0.000000\n",
      "  Big Deepseek R1     syntax_ok_rate  0.5 0.666667\n",
      "  Big Deepseek R1   satisfiable_rate  0.5 0.666667\n",
      "  Big Deepseek R1 deterministic_rate  0.5 0.666667\n",
      "  Big Deepseek R1         h1_overall -0.5 0.666667\n",
      "  Big     GPT OSS     syntax_ok_rate  0.5 0.666667\n",
      "  Big     GPT OSS   satisfiable_rate  1.0 0.000000\n",
      "  Big     GPT OSS deterministic_rate  0.5 0.666667\n",
      "  Big     GPT OSS         h1_overall -1.0 0.000000\n",
      "  Big        GPT5     syntax_ok_rate -1.0 0.000000\n",
      "  Big        GPT5   satisfiable_rate -0.5 0.666667\n",
      "  Big        GPT5 deterministic_rate -1.0 0.000000\n",
      "  Big        GPT5         h1_overall -1.0 0.000000\n",
      "Small      Claude     syntax_ok_rate -1.0 0.000000\n",
      "Small      Claude   satisfiable_rate -1.0 0.000000\n",
      "Small      Claude deterministic_rate -1.0 0.000000\n",
      "Small      Claude         h1_overall -1.0 0.000000\n",
      "\n",
      "Summary of Spearman rho per metric (H1):\n",
      "                    count      mean  median\n",
      "metric                                     \n",
      "deterministic_rate      8 -0.608253    -1.0\n",
      "h1_overall              7 -0.857143    -1.0\n",
      "satisfiable_rate        8 -0.500000    -1.0\n",
      "syntax_ok_rate          8 -0.608253    -1.0\n",
      "\n",
      "Sign tests for H1 metrics (is negative trend more common than positive?):\n",
      "  deterministic_rate     neg =  6, pos =  2, n =  8, p = 0.1445\n",
      "  h1_overall             neg =  7, pos =  0, n =  7, p = 0.0078\n",
      "  satisfiable_rate       neg =  6, pos =  2, n =  8, p = 0.1445\n",
      "  syntax_ok_rate         neg =  6, pos =  2, n =  8, p = 0.1445\n",
      "\n",
      "Big vs Small KG comparison on H1 metrics (Mann–Whitney U):\n",
      "            metric  Big_mean  Small_mean    U        p  n_big  n_small\n",
      "    syntax_ok_rate  0.711364    0.790152 63.0 0.619696     12       12\n",
      "  satisfiable_rate  0.614394    0.635859 69.0 0.885010     12       12\n",
      "deterministic_rate  0.711364    0.790152 63.0 0.619696     12       12\n",
      "        h1_overall  0.856630    0.946916 54.0 0.306035     12       12\n",
      "\n",
      "\n",
      "=== Model complexity sensitivity (mean Spearman rho across H1 metrics) ===\n",
      "(More negative = more performance drop as complexity increases)\n",
      "\n",
      "metric             syntax_ok_rate  satisfiable_rate  deterministic_rate  h1_overall  mean_rho_across_H1\n",
      "KG    Model                                                                                            \n",
      "Big   Claude            -1.000000              -1.0           -1.000000        -1.0           -1.000000\n",
      "Small GPT5              -1.000000              -1.0           -1.000000        -1.0           -1.000000\n",
      "      Deepseek R1       -1.000000              -1.0           -1.000000         NaN           -1.000000\n",
      "      Claude            -1.000000              -1.0           -1.000000        -1.0           -1.000000\n",
      "Big   GPT5              -1.000000              -0.5           -1.000000        -1.0           -0.875000\n",
      "Small GPT OSS           -0.866025              -1.0           -0.866025        -0.5           -0.808013\n",
      "Big   GPT OSS            0.500000               1.0            0.500000        -1.0            0.250000\n",
      "      Deepseek R1        0.500000               0.5            0.500000        -0.5            0.250000\n",
      "\n",
      "Saved model sensitivity table to: analysis_plots/model_complexity_sensitivity_H1.csv\n",
      "\n",
      "\n",
      "=== Simple vs Complex effect sizes (H1 metrics) ===\n",
      "\n",
      "syntax_ok_rate        : mean_Simple = 0.812, mean_Complex = 0.713, Cohen_d = 0.364 (n1=8, n2=8)\n",
      "satisfiable_rate      : mean_Simple = 0.708, mean_Complex = 0.588, Cohen_d = 0.382 (n1=8, n2=8)\n",
      "deterministic_rate    : mean_Simple = 0.812, mean_Complex = 0.713, Cohen_d = 0.364 (n1=8, n2=8)\n",
      "h1_overall            : mean_Simple = 0.988, mean_Complex = 0.871, Cohen_d = 1.720 (n1=8, n2=8)\n",
      "\n",
      "\n",
      "=== Relationship between SCREEN and H1 complexity sensitivity ===\n",
      "\n",
      "Merged (KG,Model) rows (first few):\n",
      "   KG       Model     metric  rho        p  rho_FV  rho_VS\n",
      "  Big      Claude h1_overall -1.0 0.000000    -1.0    -1.0\n",
      "  Big Deepseek R1 h1_overall -0.5 0.666667     0.5     0.5\n",
      "  Big     GPT OSS h1_overall -1.0 0.000000     1.0     0.5\n",
      "  Big        GPT5 h1_overall -1.0 0.000000    -0.5    -1.0\n",
      "Small      Claude h1_overall -1.0 0.000000    -1.0    -1.0\n",
      "\n",
      "Correlation between SCREEN sensitivity and H1 sensitivity (per KG–Model):\n",
      "  rho_FV (SCREEN)  vs rho(h1_overall): r = 0.146, p = 0.7555\n",
      "  rho_VS (SCREEN)  vs rho(h1_overall): r = 0.350, p = 0.4414\n",
      "\n",
      "Saved SCREEN–H1 sensitivity table to: analysis_plots/screen_vs_h1_sensitivity.csv\n",
      "\n",
      "\n",
      "=== Model rankings by h1_overall for each Complexity ===\n",
      "\n",
      "--- Simple ---\n",
      "      Model  mean_h1_overall Complexity\n",
      "     Claude         1.000000     Simple\n",
      "Deepseek R1         1.000000     Simple\n",
      "    GPT OSS         0.982639     Simple\n",
      "       GPT5         0.969167     Simple\n",
      "\n",
      "--- Moderate ---\n",
      "      Model  mean_h1_overall Complexity\n",
      "     Claude         0.984375   Moderate\n",
      "       GPT5         0.950694   Moderate\n",
      "    GPT OSS         0.948743   Moderate\n",
      "Deepseek R1         0.500000   Moderate\n",
      "\n",
      "--- Complex ---\n",
      "      Model  mean_h1_overall Complexity\n",
      "Deepseek R1         0.968750    Complex\n",
      "     Claude         0.897959    Complex\n",
      "       GPT5         0.882701    Complex\n",
      "    GPT OSS         0.736243    Complex\n",
      "\n",
      "Saved rankings to: analysis_plots/model_rankings_by_complexity_h1_overall.csv\n",
      "\n",
      "Saving plots to: /home/shared_projects/jupyter/scripts/Chris_Thesis_Stuff/Analysis/analysis_plots\n",
      "\n",
      "=== EXTRA: H1 KG effect summary (Big vs Small) ===\n",
      "\n",
      "            scope Complexity             metric  Big_mean  Small_mean  n_big  n_small    U        p  cliffs_delta(Big-Small)\n",
      "          overall        ALL     syntax_ok_rate  0.711364    0.790152     12       12 63.0 0.619696                -0.125000\n",
      "          overall        ALL   satisfiable_rate  0.614394    0.635859     12       12 69.0 0.885010                -0.041667\n",
      "          overall        ALL deterministic_rate  0.711364    0.790152     12       12 63.0 0.619696                -0.125000\n",
      "          overall        ALL         h1_overall  0.856630    0.946916     12       12 54.0 0.306035                -0.250000\n",
      "within_complexity     Simple     syntax_ok_rate  0.750000    0.875000      4        4  4.5 0.356170                -0.437500\n",
      "within_complexity     Simple   satisfiable_rate  0.625000    0.791667      4        4  3.5 0.242526                -0.562500\n",
      "within_complexity     Simple deterministic_rate  0.750000    0.875000      4        4  4.5 0.356170                -0.437500\n",
      "within_complexity     Simple         h1_overall  0.988750    0.987153      4        4  9.0 0.868661                 0.125000\n",
      "within_complexity   Moderate     syntax_ok_rate  0.659091    0.795455      4        4  6.5 0.765955                -0.187500\n",
      "within_complexity   Moderate   satisfiable_rate  0.568182    0.590909      4        4  8.5 1.000000                 0.062500\n",
      "within_complexity   Moderate deterministic_rate  0.659091    0.795455      4        4  6.5 0.765955                -0.187500\n",
      "within_complexity   Moderate         h1_overall  0.710793    0.981114      4        4  2.5 0.146489                -0.687500\n",
      "within_complexity    Complex     syntax_ok_rate  0.725000    0.700000      4        4  8.5 1.000000                 0.062500\n",
      "within_complexity    Complex   satisfiable_rate  0.650000    0.525000      4        4 11.0 0.465124                 0.375000\n",
      "within_complexity    Complex deterministic_rate  0.725000    0.700000      4        4  8.5 1.000000                 0.062500\n",
      "within_complexity    Complex         h1_overall  0.870346    0.872481      4        4  7.5 1.000000                -0.062500\n",
      "\n",
      "Saved KG effect stats to: analysis_plots/kg_effect_H1_mannwhitney_cliffsdelta.csv\n",
      "\n",
      "Done generating plots.\n",
      "\n",
      "=== EXTRA: Friedman Test (Repeated-Measures across models) ===\n",
      "\n",
      "syntax_ok_rate        : Friedman χ² = 6.258, p = 0.0438\n",
      "satisfiable_rate      : Friedman χ² = 4.750, p = 0.0930\n",
      "deterministic_rate    : Friedman χ² = 6.258, p = 0.0438\n",
      "h1_overall            : Friedman χ² = 10.286, p = 0.0058\n",
      "\n",
      "\n",
      "=== EXTRA: Jonckheere–Terpstra Trend Test (monotonic decreasing) ===\n",
      "\n",
      "SciPy version does not have jonckheere_terpstra; skipping this test.\n",
      "\n",
      "\n",
      "=== EXTRA: Kendall's W (agreement across complexity levels) ===\n",
      "\n",
      "syntax_ok_rate        : Kendall's W = 1.137\n",
      "satisfiable_rate      : Kendall's W = 0.891\n",
      "deterministic_rate    : Kendall's W = 1.137\n",
      "h1_overall            : Kendall's W = 1.688\n",
      "\n",
      "\n",
      "=== EXTRA: Pairwise Wilcoxon tests (Simple vs Moderate vs Complex) ===\n",
      "\n",
      "syntax_ok_rate:\n",
      "  Simple vs Moderate : W = 0.000, p = 0.0178\n",
      "  Moderate vs Complex: W = 12.000, p = 0.4609\n",
      "  Simple vs Complex  : W = 5.000, p = 0.0781\n",
      "\n",
      "satisfiable_rate:\n",
      "  Simple vs Moderate : W = 7.000, p = 0.1484\n",
      "  Moderate vs Complex: W = 17.000, p = 0.9453\n",
      "  Simple vs Complex  : W = 9.000, p = 0.2500\n",
      "\n",
      "deterministic_rate:\n",
      "  Simple vs Moderate : W = 0.000, p = 0.0178\n",
      "  Moderate vs Complex: W = 12.000, p = 0.4609\n",
      "  Simple vs Complex  : W = 5.000, p = 0.0781\n",
      "\n",
      "h1_overall:\n",
      "  Simple vs Moderate : W = 1.000, p = 0.0277\n",
      "  Moderate vs Complex: W = 7.000, p = 0.2359\n",
      "  Simple vs Complex  : W = 0.000, p = 0.0178\n",
      "\n",
      "\n",
      "=== EXTRA: KG × Complexity analyses for H1 metrics ===\n",
      "\n",
      "\n",
      "--- Metric: syntax_ok_rate ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean = 0.875, Big_mean = 0.750, W = 0.000, p = 0.1088\n",
      "    Moderate : Small_mean = 0.795, Big_mean = 0.659, W = 0.000, p = 0.1797\n",
      "    Complex  : Small_mean = 0.700, Big_mean = 0.725, W = 1.000, p = 0.6547\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ² = 7.600, p = 0.0224\n",
      "    Big  : Friedman χ² = 2.000, p = 0.3679\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small = 0.080, mean_drop_Big = 0.091, W = 2.000, p = 0.5930\n",
      "    Moderate→Complex drop: mean_drop_Small = 0.095, mean_drop_Big = -0.066, W = 1.000, p = 0.2850\n",
      "    Simple→Complex drop: mean_drop_Small = 0.175, mean_drop_Big = 0.025, W = 1.000, p = 0.2500\n",
      "\n",
      "--- Metric: satisfiable_rate ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean = 0.792, Big_mean = 0.625, W = 0.000, p = 0.1088\n",
      "    Moderate : Small_mean = 0.591, Big_mean = 0.568, W = 3.000, p = 1.0000\n",
      "    Complex  : Small_mean = 0.525, Big_mean = 0.650, W = 0.000, p = 0.1797\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ² = 8.000, p = 0.0183\n",
      "    Big  : Friedman χ² = 1.500, p = 0.4724\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small = 0.201, mean_drop_Big = 0.057, W = 2.000, p = 0.5930\n",
      "    Moderate→Complex drop: mean_drop_Small = 0.066, mean_drop_Big = -0.082, W = 0.000, p = 0.1088\n",
      "    Simple→Complex drop: mean_drop_Small = 0.267, mean_drop_Big = -0.025, W = 0.000, p = 0.1088\n",
      "\n",
      "--- Metric: deterministic_rate ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean = 0.875, Big_mean = 0.750, W = 0.000, p = 0.1088\n",
      "    Moderate : Small_mean = 0.795, Big_mean = 0.659, W = 0.000, p = 0.1797\n",
      "    Complex  : Small_mean = 0.700, Big_mean = 0.725, W = 1.000, p = 0.6547\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ² = 7.600, p = 0.0224\n",
      "    Big  : Friedman χ² = 2.000, p = 0.3679\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small = 0.080, mean_drop_Big = 0.091, W = 2.000, p = 0.5930\n",
      "    Moderate→Complex drop: mean_drop_Small = 0.095, mean_drop_Big = -0.066, W = 1.000, p = 0.2850\n",
      "    Simple→Complex drop: mean_drop_Small = 0.175, mean_drop_Big = 0.025, W = 1.000, p = 0.2500\n",
      "\n",
      "--- Metric: h1_overall ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean = 0.987, Big_mean = 0.989, W = 1.000, p = 0.6547\n",
      "    Moderate : Small_mean = 0.981, Big_mean = 0.711, W = 0.000, p = 0.1088\n",
      "    Complex  : Small_mean = 0.872, Big_mean = 0.870, W = 3.000, p = 1.0000\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ² = 4.667, p = 0.0970\n",
      "    Big  : Friedman χ² = 6.500, p = 0.0388\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small = 0.006, mean_drop_Big = 0.278, W = 1.000, p = 0.2850\n",
      "    Moderate→Complex drop: mean_drop_Small = 0.109, mean_drop_Big = -0.160, W = 0.000, p = 0.1088\n",
      "    Simple→Complex drop: mean_drop_Small = 0.115, mean_drop_Big = 0.118, W = 3.000, p = 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "# ========= CONFIG =========\n",
    "# EXCEL_PATH = Path(\"all_models_consolidated_results.xlsx\")  # change if needed\n",
    "EXCEL_PATH = Path(\"balanced_models_consolidated_results.xlsx\")  # change if needed\n",
    "\n",
    "# Total # of CQs and per-complexity split (given by you)\n",
    "TOTAL_CQS = 33\n",
    "CQS_PER_COMPLEXITY = {\"Simple\": 12, \"Moderate\": 11, \"Complex\": 10}\n",
    "OUT_DIR = Path(\"analysis_plots\")  # folder to store PNGs\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "# ==========================\n",
    "\n",
    "print(f\"Loading data from: {EXCEL_PATH}\")\n",
    "screen, screen_c, h1, h1_c = load_and_clean(EXCEL_PATH)\n",
    "\n",
    "screen_c_clean = prepare_screen_with_complexity(screen_c)\n",
    "h1_c_clean = prepare_h1_with_complexity(h1_c)\n",
    "\n",
    "# Analyses\n",
    "sc_df = analyze_screen(screen_c_clean)\n",
    "per_pair_df = analyze_h1(h1_c_clean)\n",
    "\n",
    "# Extra analyses (do not change existing ones)\n",
    "model_sensitivity = summarize_model_sensitivity(per_pair_df)\n",
    "effect_size_simple_vs_complex(h1_c_clean)\n",
    "screen_h1_link = correlate_screen_and_h1(sc_df, per_pair_df)\n",
    "rankings = rank_models_by_complexity(h1_c_clean)\n",
    "\n",
    "# Plots\n",
    "print(f\"Saving plots to: {OUT_DIR.resolve()}\")\n",
    "plot_screen_boxplots(screen_c_clean)\n",
    "plot_screen_boxplots_by_kg(screen_c_clean)\n",
    "plot_h1_boxplots(h1_c_clean)\n",
    "plot_h1_trendlines(h1_c_clean)\n",
    "plot_h1_rho_heatmap(per_pair_df)\n",
    "plot_h1_boxplots_by_kg(h1_c_clean)\n",
    "plot_h1_boxplots_kg_effect_per_complexity(h1_c_clean)\n",
    "kg_effect_table = h1_kg_effect_stats(h1_c_clean)\n",
    "print(\"Done generating plots.\")\n",
    "\n",
    "# =====================================================================\n",
    "#  EXTRA TESTS TO STRENGTHEN H1 EVIDENCE\n",
    "# =====================================================================\n",
    "friedman_test_h1(h1_c_clean)\n",
    "jt_test_h1(h1_c_clean)\n",
    "kendalls_w_h1(h1_c_clean)\n",
    "pairwise_wilcoxon_h1(h1_c_clean)\n",
    "kg_complexity_interaction_tests(h1_c_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d9cd7-fbbc-4405-93cf-faa9b588df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
