{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc5a13d-de87-4e16-b693-8e15a8060440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "\n",
    "# Jonckheere–Terpstra might not exist on older SciPy; handle gracefully\n",
    "try:\n",
    "    from scipy.stats import jonckheere_terpstra\n",
    "    HAS_JT = True\n",
    "except Exception:\n",
    "    HAS_JT = False\n",
    "\n",
    "from scipy.stats import ConstantInputWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConstantInputWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0b0cec-1778-4f19-bbe8-91def2e100ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  IO + PREP\n",
    "# =====================================================================\n",
    "\n",
    "H2_METRICS = [\n",
    "    \"lexical_query_overlap\",\n",
    "    \"semantic_similarity_to_CQ\",\n",
    "    \"semantic_soft_coverage_to_CQ\",\n",
    "    \"semantic_diversity_score\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42245dd6-e650-4d4b-a63c-08dc37fb1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(path: Path):\n",
    "    \"\"\"Load H2 sheets and forward-fill KG/Model labels.\"\"\"\n",
    "    # If your sheet names differ, just change these two strings:\n",
    "    h2 = pd.read_excel(path, sheet_name=\"H2-AM\")\n",
    "    h2_c = pd.read_excel(path, sheet_name=\"H2-AM&C\")\n",
    "\n",
    "    for df in (h2, h2_c):\n",
    "        for col in [\"KG\", \"Model\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].ffill()\n",
    "\n",
    "    return h2, h2_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b0e2b6-f49f-41d2-acc5-1936e47bce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_h2_with_complexity(h2_c: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add numeric complexity level for H2 metrics.\"\"\"\n",
    "    df = h2_c.copy()\n",
    "    df[\"ComplexityLevel\"] = df[\"Complexity\"].map({\"Simple\": 1, \"Moderate\": 2, \"Complex\": 3})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadaf46e-ca47-421d-b2fb-b073fbcf7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  ANALYSIS FUNCTIONS\n",
    "# =====================================================================\n",
    "\n",
    "def analyze_h2(h2_c_clean: pd.DataFrame):\n",
    "    print(\"\\n=== H2 metrics analysis ===\\n\")\n",
    "\n",
    "    # ----- Global Kruskal–Wallis per metric across complexity -----\n",
    "    print(\"Global Kruskal–Wallis tests across Complexity for H2 metrics:\")\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            print(f\"  {metric:28s} [SKIP: column not found]\")\n",
    "            continue\n",
    "\n",
    "        valid = h2_c_clean.dropna(subset=[metric])\n",
    "        groups = [g[metric].values for _, g in valid.groupby(\"Complexity\")]\n",
    "        if len(groups) >= 2:\n",
    "            stat, p = stats.kruskal(*groups)\n",
    "            print(f\"  {metric:28s} H = {stat:.3f}, p = {p:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric:28s} [SKIP: not enough groups]\")\n",
    "    print()\n",
    "\n",
    "    # ----- Per (KG, Model) Spearman correlations vs Complexity -----\n",
    "    per_pair = []\n",
    "    for (kg, model), grp in h2_c_clean.groupby([\"KG\", \"Model\"]):\n",
    "        if grp[\"ComplexityLevel\"].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        for metric in H2_METRICS:\n",
    "            if metric not in grp.columns:\n",
    "                continue\n",
    "            g = grp.dropna(subset=[metric])\n",
    "            if g.shape[0] >= 2:\n",
    "                r, p = stats.spearmanr(g[\"ComplexityLevel\"], g[metric])\n",
    "                if not np.isnan(r):\n",
    "                    per_pair.append(\n",
    "                        {\"KG\": kg, \"Model\": model, \"metric\": metric, \"rho\": r, \"p\": p}\n",
    "                    )\n",
    "\n",
    "    per_pair_df = pd.DataFrame(per_pair)\n",
    "    if per_pair_df.empty:\n",
    "        print(\"No usable per-(KG,Model) correlations for H2 metrics.\\n\")\n",
    "        return per_pair_df\n",
    "\n",
    "    print(\"Per (KG, Model) Spearman correlations for H2 metrics (first few rows):\")\n",
    "    print(per_pair_df.head(20).to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # ----- Summary of correlations by metric -----\n",
    "    summary_corr = per_pair_df.groupby(\"metric\")[\"rho\"].agg([\"count\", \"mean\", \"median\"])\n",
    "    print(\"Summary of Spearman rho per metric (H2):\")\n",
    "    print(summary_corr.to_string())\n",
    "    print()\n",
    "\n",
    "    # ----- Sign tests: negative vs positive correlations -----\n",
    "    print(\"Sign tests for H2 metrics (is negative trend more common than positive?):\")\n",
    "    for metric, grp in per_pair_df.groupby(\"metric\"):\n",
    "        neg = (grp[\"rho\"] < 0).sum()\n",
    "        pos = (grp[\"rho\"] > 0).sum()\n",
    "        n = neg + pos\n",
    "        if n > 0:\n",
    "            p_val = stats.binomtest(neg, n, 0.5, alternative=\"greater\").pvalue\n",
    "            print(f\"  {metric:28s} neg={neg:2d}, pos={pos:2d}, n={n:2d}, p={p_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric:28s} no non-zero correlations\")\n",
    "    print()\n",
    "\n",
    "    # ----- KG-level comparison: Big vs Small -----\n",
    "    print(\"Big vs Small KG comparison on H2 metrics (Mann–Whitney U):\")\n",
    "    small_big_results = []\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "        big_vals = h2_c_clean[h2_c_clean[\"KG\"] == \"Big\"][metric].dropna()\n",
    "        small_vals = h2_c_clean[h2_c_clean[\"KG\"] == \"Small\"][metric].dropna()\n",
    "        if len(big_vals) > 0 and len(small_vals) > 0:\n",
    "            u_stat, p = stats.mannwhitneyu(big_vals, small_vals, alternative=\"two-sided\")\n",
    "            small_big_results.append(\n",
    "                {\n",
    "                    \"metric\": metric,\n",
    "                    \"Big_mean\": big_vals.mean(),\n",
    "                    \"Small_mean\": small_vals.mean(),\n",
    "                    \"U\": u_stat,\n",
    "                    \"p\": p,\n",
    "                    \"n_big\": len(big_vals),\n",
    "                    \"n_small\": len(small_vals),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if small_big_results:\n",
    "        kg_df = pd.DataFrame(small_big_results)\n",
    "        print(kg_df.to_string(index=False))\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Not enough data for Big vs Small KG comparison.\\n\")\n",
    "\n",
    "    return per_pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db592f8-4e2d-4b0b-9a0f-0ee8ca3b9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  PLOTTING FUNCTIONS\n",
    "# =====================================================================\n",
    "\n",
    "def plot_h2_boxplots(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"Boxplots of each H2 metric by Complexity.\"\"\"\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "        plt.figure()\n",
    "        data = [g[metric].values for _, g in h2_c_clean.groupby(\"Complexity\")]\n",
    "        labels = [name for name, _ in h2_c_clean.groupby(\"Complexity\")]\n",
    "        plt.boxplot(data)\n",
    "        plt.xticks(range(1, len(labels) + 1), labels)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{metric} by Complexity\")\n",
    "        out_path = OUT_DIR / f\"boxplot_H2_{metric}.png\"\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "def plot_h2_trendlines(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Line plots: ComplexityLevel vs metric for each KG–Model (separate lines).\n",
    "    One plot per metric.\n",
    "    \"\"\"\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "        plt.figure()\n",
    "        for (kg, model), grp in h2_c_clean.groupby([\"KG\", \"Model\"]):\n",
    "            g = grp.sort_values(\"ComplexityLevel\")\n",
    "            x = g[\"ComplexityLevel\"].values\n",
    "            y = g[metric].values\n",
    "            if len(x) >= 2 and not np.all(np.isnan(y)):\n",
    "                label = f\"{kg}-{model}\"\n",
    "                plt.plot(x, y, marker=\"o\", label=label)\n",
    "        plt.xticks([1, 2, 3], [\"Simple\", \"Moderate\", \"Complex\"])\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{metric} vs Complexity (per KG–Model)\")\n",
    "        plt.legend(fontsize=\"x-small\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        out_path = OUT_DIR / f\"trend_H2_{metric}.png\"\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "def plot_h2_rho_heatmap(per_pair_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Heatmap of rho values:\n",
    "      rows: (KG,Model)\n",
    "      cols: metrics\n",
    "    \"\"\"\n",
    "    if per_pair_df.empty:\n",
    "        return\n",
    "\n",
    "    pivot_data = []\n",
    "    row_labels = []\n",
    "    for (kg, model), grp in per_pair_df.groupby([\"KG\", \"Model\"]):\n",
    "        row_labels.append(f\"{kg}-{model}\")\n",
    "        row = []\n",
    "        for m in H2_METRICS:\n",
    "            sub = grp[grp[\"metric\"] == m]\n",
    "            row.append(sub[\"rho\"].iloc[0] if not sub.empty else np.nan)\n",
    "        pivot_data.append(row)\n",
    "\n",
    "    data = np.array(pivot_data, dtype=float)\n",
    "\n",
    "    plt.figure()\n",
    "    nan_mask = np.isnan(data)\n",
    "    data_display = np.where(nan_mask, 0.0, data)\n",
    "\n",
    "    im = plt.imshow(data_display, aspect=\"auto\")\n",
    "    plt.colorbar(im, label=\"Spearman rho\")\n",
    "    plt.yticks(range(len(row_labels)), row_labels, fontsize=\"x-small\")\n",
    "    plt.xticks(range(len(H2_METRICS)), H2_METRICS, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Spearman rho (Complexity vs H2 metrics)\\nper KG–Model\")\n",
    "    out_path = OUT_DIR / \"heatmap_H2_rho.png\"\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d608af31-00b6-4a83-86ea-b67f4a0b5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h2_boxplots_by_kg(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Boxplots of each H2 metric by KG (Small vs Big), pooled across complexities.\n",
    "    Saves: boxplot_H2_<metric>_by_KG.png\n",
    "    \"\"\"\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        plt.figure()\n",
    "        data = [g[metric].dropna().values for _, g in h2_c_clean.groupby(\"KG\")]\n",
    "        labels = [name for name, _ in h2_c_clean.groupby(\"KG\")]\n",
    "\n",
    "        # keep stable order Small then Big if present\n",
    "        order = []\n",
    "        if \"Small\" in labels: order.append(\"Small\")\n",
    "        if \"Big\" in labels: order.append(\"Big\")\n",
    "        if order and set(order) == set(labels):\n",
    "            data = [h2_c_clean[h2_c_clean[\"KG\"] == kg][metric].dropna().values for kg in order]\n",
    "            labels = order\n",
    "\n",
    "        plt.boxplot(data)\n",
    "        plt.xticks(range(1, len(labels) + 1), labels)\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{metric} by KG (pooled)\")\n",
    "        out_path = OUT_DIR / f\"boxplot_H2_{metric}_by_KG.png\"\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_h2_boxplots_by_kg_within_complexity(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each Complexity, boxplots of each H2 metric by KG (Small vs Big).\n",
    "    Saves: boxplot_H2_<metric>_by_KG_within_<Complexity>.png\n",
    "    \"\"\"\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        for comp, df_comp in h2_c_clean.groupby(\"Complexity\"):\n",
    "            plt.figure()\n",
    "\n",
    "            # stable KG order\n",
    "            labels = []\n",
    "            if \"Small\" in df_comp[\"KG\"].unique(): labels.append(\"Small\")\n",
    "            if \"Big\" in df_comp[\"KG\"].unique(): labels.append(\"Big\")\n",
    "            if not labels:\n",
    "                plt.close()\n",
    "                continue\n",
    "\n",
    "            data = [df_comp[df_comp[\"KG\"] == kg][metric].dropna().values for kg in labels]\n",
    "\n",
    "            plt.boxplot(data)\n",
    "            plt.xticks(range(1, len(labels) + 1), labels)\n",
    "            plt.ylabel(metric)\n",
    "            plt.title(f\"{metric} by KG within {comp}\")\n",
    "            out_path = OUT_DIR / f\"boxplot_H2_{metric}_by_KG_within_{comp}.png\"\n",
    "            plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af31135a-17aa-43be-b587-141fbd763a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  EXTRA STATS (mirrors your H1 extras)\n",
    "# =====================================================================\n",
    "\n",
    "def _cliffs_delta(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Cliff's delta effect size for two independent samples.\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    x = x[~np.isnan(x)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    gt = 0\n",
    "    lt = 0\n",
    "    for xi in x:\n",
    "        gt += np.sum(xi > y)\n",
    "        lt += np.sum(xi < y)\n",
    "    return (gt - lt) / (len(x) * len(y))\n",
    "\n",
    "def h2_kg_effect_stats(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Summarize KG effect on H2 metrics:\n",
    "      1) Overall Big vs Small pooled across complexities\n",
    "      2) Big vs Small within each Complexity\n",
    "    Also computes Cliff's delta.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: H2 KG effect summary (Big vs Small) ===\\n\")\n",
    "\n",
    "    complexities = [\"Simple\", \"Moderate\", \"Complex\"]\n",
    "    rows = []\n",
    "\n",
    "    # (A) Overall pooled\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "        big_vals = h2_c_clean[h2_c_clean[\"KG\"] == \"Big\"][metric].dropna().values\n",
    "        small_vals = h2_c_clean[h2_c_clean[\"KG\"] == \"Small\"][metric].dropna().values\n",
    "        if len(big_vals) > 0 and len(small_vals) > 0:\n",
    "            u_stat, p = stats.mannwhitneyu(big_vals, small_vals, alternative=\"two-sided\")\n",
    "            delta = _cliffs_delta(big_vals, small_vals)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"scope\": \"overall\",\n",
    "                    \"Complexity\": \"ALL\",\n",
    "                    \"metric\": metric,\n",
    "                    \"Big_mean\": np.mean(big_vals),\n",
    "                    \"Small_mean\": np.mean(small_vals),\n",
    "                    \"n_big\": len(big_vals),\n",
    "                    \"n_small\": len(small_vals),\n",
    "                    \"U\": u_stat,\n",
    "                    \"p\": p,\n",
    "                    \"cliffs_delta(Big-Small)\": delta,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # (B) Within each complexity\n",
    "    for comp in complexities:\n",
    "        df_comp = h2_c_clean[h2_c_clean[\"Complexity\"] == comp]\n",
    "        for metric in H2_METRICS:\n",
    "            if metric not in df_comp.columns:\n",
    "                continue\n",
    "            big_vals = df_comp[df_comp[\"KG\"] == \"Big\"][metric].dropna().values\n",
    "            small_vals = df_comp[df_comp[\"KG\"] == \"Small\"][metric].dropna().values\n",
    "            if len(big_vals) > 0 and len(small_vals) > 0:\n",
    "                u_stat, p = stats.mannwhitneyu(big_vals, small_vals, alternative=\"two-sided\")\n",
    "                delta = _cliffs_delta(big_vals, small_vals)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"scope\": \"within_complexity\",\n",
    "                        \"Complexity\": comp,\n",
    "                        \"metric\": metric,\n",
    "                        \"Big_mean\": np.mean(big_vals),\n",
    "                        \"Small_mean\": np.mean(small_vals),\n",
    "                        \"n_big\": len(big_vals),\n",
    "                        \"n_small\": len(small_vals),\n",
    "                        \"U\": u_stat,\n",
    "                        \"p\": p,\n",
    "                        \"cliffs_delta(Big-Small)\": delta,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if not rows:\n",
    "        print(\"Not enough data to compute KG effect stats.\\n\")\n",
    "        return None\n",
    "\n",
    "    kg_effect_df = pd.DataFrame(rows)\n",
    "    print(kg_effect_df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    out_path = OUT_DIR / \"kg_effect_H2_mannwhitney_cliffsdelta.csv\"\n",
    "    kg_effect_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved KG effect stats to: {out_path}\\n\")\n",
    "\n",
    "    return kg_effect_df\n",
    "\n",
    "def summarize_model_sensitivity_h2(per_pair_df: pd.DataFrame):\n",
    "    \"\"\"Same idea as your H1 sensitivity table, but for H2.\"\"\"\n",
    "    if per_pair_df.empty:\n",
    "        print(\"No per-pair H2 correlations to summarize.\\n\")\n",
    "        return None\n",
    "\n",
    "    pivot = (\n",
    "        per_pair_df\n",
    "        .pivot_table(index=[\"KG\", \"Model\"], columns=\"metric\", values=\"rho\")\n",
    "        .reindex(columns=[m for m in H2_METRICS if m in per_pair_df[\"metric\"].unique()])\n",
    "    )\n",
    "    pivot[\"mean_rho_across_H2\"] = pivot.mean(axis=1)\n",
    "\n",
    "    print(\"\\n=== Model complexity sensitivity (mean Spearman rho across H2 metrics) ===\")\n",
    "    print(\"(More negative = stronger decrease as complexity increases)\\n\")\n",
    "    print(pivot.sort_values(\"mean_rho_across_H2\").to_string())\n",
    "    print()\n",
    "\n",
    "    out_path = OUT_DIR / \"model_complexity_sensitivity_H2.csv\"\n",
    "    pivot.to_csv(out_path)\n",
    "    print(f\"Saved model sensitivity table to: {out_path}\\n\")\n",
    "\n",
    "    return pivot\n",
    "\n",
    "def effect_size_simple_vs_complex_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"Simple vs Complex effect sizes (Cohen's d) for each H2 metric.\"\"\"\n",
    "    print(\"\\n=== Simple vs Complex effect sizes (H2 metrics) ===\\n\")\n",
    "\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            print(f\"{metric:28s}: column not found.\")\n",
    "            continue\n",
    "\n",
    "        simple = h2_c_clean[h2_c_clean[\"Complexity\"] == \"Simple\"][metric].dropna()\n",
    "        complex_ = h2_c_clean[h2_c_clean[\"Complexity\"] == \"Complex\"][metric].dropna()\n",
    "\n",
    "        if len(simple) < 2 or len(complex_) < 2:\n",
    "            print(f\"{metric:28s}: not enough data for effect size.\")\n",
    "            continue\n",
    "\n",
    "        mean_simple = simple.mean()\n",
    "        mean_complex = complex_.mean()\n",
    "        sd_simple = simple.std(ddof=1)\n",
    "        sd_complex = complex_.std(ddof=1)\n",
    "\n",
    "        n1, n2 = len(simple), len(complex_)\n",
    "        pooled_var = ((n1 - 1) * sd_simple**2 + (n2 - 1) * sd_complex**2) / (n1 + n2 - 2)\n",
    "        pooled_sd = np.sqrt(pooled_var)\n",
    "\n",
    "        d = (mean_simple - mean_complex) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        print(\n",
    "            f\"{metric:28s}: mean_Simple={mean_simple:.3f}, mean_Complex={mean_complex:.3f}, \"\n",
    "            f\"Cohen_d={d:.3f} (n1={n1}, n2={n2})\"\n",
    "        )\n",
    "    print()\n",
    "\n",
    "def rank_models_by_complexity_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each complexity level, compute mean of each H2 metric per Model (aggregated over KGs)\n",
    "    and print rankings. Saves a CSV per metric + one combined CSV.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Model rankings by H2 metrics for each Complexity ===\\n\")\n",
    "\n",
    "    all_rows = []\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        for comp, grp in h2_c_clean.groupby(\"Complexity\"):\n",
    "            tmp = (\n",
    "                grp.groupby(\"Model\")[metric]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={metric: \"mean_value\"})\n",
    "            )\n",
    "            tmp[\"Complexity\"] = comp\n",
    "            tmp[\"metric\"] = metric\n",
    "            rows.append(tmp)\n",
    "\n",
    "        if rows:\n",
    "            ranking_df = pd.concat(rows, ignore_index=True)\n",
    "            all_rows.append(ranking_df)\n",
    "\n",
    "            for comp in [\"Simple\", \"Moderate\", \"Complex\"]:\n",
    "                sub = ranking_df[ranking_df[\"Complexity\"] == comp].sort_values(\"mean_value\", ascending=False)\n",
    "                print(f\"--- Metric: {metric} | {comp} ---\")\n",
    "                print(sub.to_string(index=False))\n",
    "                print()\n",
    "\n",
    "            out_path = OUT_DIR / f\"model_rankings_by_complexity_{metric}.csv\"\n",
    "            ranking_df.to_csv(out_path, index=False)\n",
    "            print(f\"Saved rankings for {metric} to: {out_path}\\n\")\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"No data to rank models.\\n\")\n",
    "        return None\n",
    "\n",
    "    combined = pd.concat(all_rows, ignore_index=True)\n",
    "    out_path = OUT_DIR / \"model_rankings_by_complexity_H2_ALL.csv\"\n",
    "    combined.to_csv(out_path, index=False)\n",
    "    print(f\"Saved combined rankings to: {out_path}\\n\")\n",
    "    return combined\n",
    "\n",
    "# =====================================================================\n",
    "#  EXTRA TESTS (mirrors your H1 tests)\n",
    "# =====================================================================\n",
    "\n",
    "def friedman_test_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"Friedman repeated-measures test across complexity, using (KG,Model) as blocks.\"\"\"\n",
    "    print(\"\\n=== EXTRA: Friedman Test (Repeated-Measures across models) [H2] ===\\n\")\n",
    "\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        pivot = h2_c_clean.pivot_table(\n",
    "            index=[\"KG\", \"Model\"],\n",
    "            columns=\"Complexity\",\n",
    "            values=metric\n",
    "        ).dropna(subset=[\"Simple\", \"Moderate\", \"Complex\"])\n",
    "\n",
    "        if pivot.shape[0] < 3:\n",
    "            print(f\"{metric:28s}: not enough (KG,Model) with all three complexities.\")\n",
    "            continue\n",
    "\n",
    "        S = pivot[\"Simple\"].values\n",
    "        M = pivot[\"Moderate\"].values\n",
    "        C = pivot[\"Complex\"].values\n",
    "\n",
    "        stat, p = friedmanchisquare(S, M, C)\n",
    "        print(f\"{metric:28s}: Friedman χ² = {stat:.3f}, p = {p:.4f}\")\n",
    "    print()\n",
    "\n",
    "def jt_test_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Jonckheere–Terpstra trend test for ordered categories:\n",
    "    tests monotonic trend Simple -> Moderate -> Complex.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: Jonckheere–Terpstra Trend Test [H2] ===\\n\")\n",
    "\n",
    "    if not HAS_JT:\n",
    "        print(\"SciPy version does not have jonckheere_terpstra; skipping this test.\\n\")\n",
    "        return\n",
    "\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        g_simple   = h2_c_clean[h2_c_clean[\"Complexity\"] == \"Simple\"][metric].dropna()\n",
    "        g_moderate = h2_c_clean[h2_c_clean[\"Complexity\"] == \"Moderate\"][metric].dropna()\n",
    "        g_complex  = h2_c_clean[h2_c_clean[\"Complexity\"] == \"Complex\"][metric].dropna()\n",
    "\n",
    "        groups = [g_simple, g_moderate, g_complex]\n",
    "        if any(len(g) == 0 for g in groups):\n",
    "            print(f\"{metric:28s}: insufficient data for JT test.\")\n",
    "            continue\n",
    "\n",
    "        # If your H2 hypothesis is \"decreasing\", keep decreasing.\n",
    "        # If it's \"increasing\", change to alternative=\"increasing\".\n",
    "        jt_stat, p = jonckheere_terpstra(groups, alternative=\"decreasing\")\n",
    "        print(f\"{metric:28s}: JT = {jt_stat:.3f}, p = {p:.4f}\")\n",
    "    print()\n",
    "\n",
    "def kendalls_w_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"Compute Kendall's W across complexity levels for each H2 metric.\"\"\"\n",
    "    print(\"\\n=== EXTRA: Kendall's W (agreement across complexity levels) [H2] ===\\n\")\n",
    "\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        pivot = h2_c_clean.pivot_table(\n",
    "            index=[\"KG\", \"Model\"],\n",
    "            columns=\"Complexity\",\n",
    "            values=metric\n",
    "        ).dropna(subset=[\"Simple\", \"Moderate\", \"Complex\"])\n",
    "\n",
    "        if pivot.shape[0] < 3:\n",
    "            print(f\"{metric:28s}: insufficient models for Kendall's W.\")\n",
    "            continue\n",
    "\n",
    "        ranks = pivot.rank(axis=1, ascending=True).values\n",
    "        n, k = ranks.shape\n",
    "\n",
    "        R_j = np.sum(ranks, axis=0)\n",
    "        R_bar = n * (k + 1) / 2.0\n",
    "\n",
    "        S = np.sum((R_j - R_bar) ** 2)\n",
    "        W = 12 * S / (n**2 * (k**2 - 1))\n",
    "\n",
    "        print(f\"{metric:28s}: Kendall's W = {W:.3f}\")\n",
    "    print()\n",
    "\n",
    "def pairwise_wilcoxon_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"Paired Wilcoxon tests per H2 metric across complexities, using (KG,Model) as pairs.\"\"\"\n",
    "    print(\"\\n=== EXTRA: Pairwise Wilcoxon tests (Simple/Moderate/Complex) [H2] ===\\n\")\n",
    "\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        pivot = h2_c_clean.pivot_table(\n",
    "            index=[\"KG\", \"Model\"],\n",
    "            columns=\"Complexity\",\n",
    "            values=metric\n",
    "        ).dropna(subset=[\"Simple\", \"Moderate\", \"Complex\"])\n",
    "\n",
    "        if pivot.shape[0] < 3:\n",
    "            print(f\"{metric:28s}: not enough paired (KG,Model) rows.\")\n",
    "            continue\n",
    "\n",
    "        S = pivot[\"Simple\"]\n",
    "        M = pivot[\"Moderate\"]\n",
    "        C = pivot[\"Complex\"]\n",
    "\n",
    "        stat_SM, p_SM = wilcoxon(S, M)\n",
    "        stat_MC, p_MC = wilcoxon(M, C)\n",
    "        stat_SC, p_SC = wilcoxon(S, C)\n",
    "\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Simple vs Moderate : W = {stat_SM:.3f}, p = {p_SM:.4f}\")\n",
    "        print(f\"  Moderate vs Complex: W = {stat_MC:.3f}, p = {p_MC:.4f}\")\n",
    "        print(f\"  Simple vs Complex  : W = {stat_SC:.3f}, p = {p_SC:.4f}\\n\")\n",
    "\n",
    "def kg_complexity_interaction_tests_h2(h2_c_clean: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Mirrors your H1 KG × Complexity analyses, but for H2 metrics:\n",
    "      1) Big vs Small at each Complexity (Wilcoxon, Model-level)\n",
    "      2) Complexity effect within each KG (Friedman, Model-level)\n",
    "      3) Drop comparisons (Small vs Big; Wilcoxon)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXTRA: KG × Complexity analyses for H2 metrics ===\\n\")\n",
    "\n",
    "    complexities = [\"Simple\", \"Moderate\", \"Complex\"]\n",
    "    kgs = [\"Small\", \"Big\"]\n",
    "\n",
    "    for metric in H2_METRICS:\n",
    "        if metric not in h2_c_clean.columns:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Metric: {metric} ---\")\n",
    "\n",
    "        pivot = h2_c_clean.pivot_table(\n",
    "            index=\"Model\",\n",
    "            columns=[\"KG\", \"Complexity\"],\n",
    "            values=metric\n",
    "        )\n",
    "\n",
    "        missing_cols = []\n",
    "        for kg in kgs:\n",
    "            for comp in complexities:\n",
    "                if (kg, comp) not in pivot.columns:\n",
    "                    missing_cols.append((kg, comp))\n",
    "        if missing_cols:\n",
    "            print(f\"  Skipping {metric}: missing cells {missing_cols}\")\n",
    "            continue\n",
    "\n",
    "        # (1) Big vs Small at each complexity\n",
    "        print(\"  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\")\n",
    "        for comp in complexities:\n",
    "            small_vals = pivot[(\"Small\", comp)].dropna()\n",
    "            big_vals   = pivot[(\"Big\", comp)].dropna()\n",
    "\n",
    "            common_index = small_vals.index.intersection(big_vals.index)\n",
    "            if len(common_index) < 2:\n",
    "                print(f\"    {comp:9s}: not enough paired models.\")\n",
    "                continue\n",
    "\n",
    "            s = small_vals.loc[common_index]\n",
    "            b = big_vals.loc[common_index]\n",
    "\n",
    "            stat, p = wilcoxon(s, b)\n",
    "            print(\n",
    "                f\"    {comp:9s}: Small_mean={s.mean():.3f}, Big_mean={b.mean():.3f}, \"\n",
    "                f\"W={stat:.3f}, p={p:.4f}\"\n",
    "            )\n",
    "\n",
    "        # (2) Complexity effect within each KG\n",
    "        print(\"  Complexity effect within each KG (Friedman, Model-level):\")\n",
    "        for kg in kgs:\n",
    "            sub = pivot[kg].copy().dropna(subset=complexities)\n",
    "            if sub.shape[0] < 2:\n",
    "                print(f\"    {kg:5s}: not enough complete models for Friedman.\")\n",
    "                continue\n",
    "            S = sub[\"Simple\"].values\n",
    "            M = sub[\"Moderate\"].values\n",
    "            C = sub[\"Complex\"].values\n",
    "            stat, p = friedmanchisquare(S, M, C)\n",
    "            print(f\"    {kg:5s}: Friedman χ²={stat:.3f}, p={p:.4f}\")\n",
    "\n",
    "        # (3) Drop comparisons\n",
    "        print(\"  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\")\n",
    "\n",
    "        ss = pivot[(\"Small\", \"Simple\")].dropna()\n",
    "        sm = pivot[(\"Small\", \"Moderate\")].dropna()\n",
    "        sc = pivot[(\"Small\", \"Complex\")].dropna()\n",
    "        bs = pivot[(\"Big\", \"Simple\")].dropna()\n",
    "        bm = pivot[(\"Big\", \"Moderate\")].dropna()\n",
    "        bc = pivot[(\"Big\", \"Complex\")].dropna()\n",
    "\n",
    "        common_index = (\n",
    "            ss.index.intersection(sm.index).intersection(sc.index)\n",
    "            .intersection(bs.index).intersection(bm.index).intersection(bc.index)\n",
    "        )\n",
    "\n",
    "        if len(common_index) < 2:\n",
    "            print(\"    Not enough models with all six cells (Small/Big × Simple/Moderate/Complex).\")\n",
    "            continue\n",
    "\n",
    "        ss = ss.loc[common_index]\n",
    "        sm = sm.loc[common_index]\n",
    "        sc = sc.loc[common_index]\n",
    "        bs = bs.loc[common_index]\n",
    "        bm = bm.loc[common_index]\n",
    "        bc = bc.loc[common_index]\n",
    "\n",
    "        drop_small_SM = ss - sm\n",
    "        drop_big_SM   = bs - bm\n",
    "        stat_sm, p_sm = wilcoxon(drop_small_SM, drop_big_SM)\n",
    "        print(\n",
    "            f\"    Simple→Moderate drop: mean_drop_Small={drop_small_SM.mean():.3f}, \"\n",
    "            f\"mean_drop_Big={drop_big_SM.mean():.3f}, W={stat_sm:.3f}, p={p_sm:.4f}\"\n",
    "        )\n",
    "\n",
    "        drop_small_MC = sm - sc\n",
    "        drop_big_MC   = bm - bc\n",
    "        stat_mc, p_mc = wilcoxon(drop_small_MC, drop_big_MC)\n",
    "        print(\n",
    "            f\"    Moderate→Complex drop: mean_drop_Small={drop_small_MC.mean():.3f}, \"\n",
    "            f\"mean_drop_Big={drop_big_MC.mean():.3f}, W={stat_mc:.3f}, p={p_mc:.4f}\"\n",
    "        )\n",
    "\n",
    "        drop_small_SC = ss - sc\n",
    "        drop_big_SC   = bs - bc\n",
    "        stat_sc2, p_sc2 = wilcoxon(drop_small_SC, drop_big_SC)\n",
    "        print(\n",
    "            f\"    Simple→Complex drop: mean_drop_Small={drop_small_SC.mean():.3f}, \"\n",
    "            f\"mean_drop_Big={drop_big_SC.mean():.3f}, W={stat_sc2:.3f}, p={p_sc2:.4f}\"\n",
    "        )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717326ac-4b90-42ea-b933-fbed3391435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: consolidated_results_filter_only4.xlsx\n",
      "\n",
      "=== H2 metrics analysis ===\n",
      "\n",
      "Global Kruskal–Wallis tests across Complexity for H2 metrics:\n",
      "  lexical_query_overlap        H = 5.016, p = 0.0814\n",
      "  semantic_similarity_to_CQ    H = 1.903, p = 0.3862\n",
      "  semantic_soft_coverage_to_CQ H = 0.161, p = 0.9225\n",
      "  semantic_diversity_score     H = 0.860, p = 0.6505\n",
      "\n",
      "Per (KG, Model) Spearman correlations for H2 metrics (first few rows):\n",
      "   KG       Model                       metric       rho        p\n",
      "  Big      Claude        lexical_query_overlap -0.500000 0.666667\n",
      "  Big      Claude    semantic_similarity_to_CQ  0.500000 0.666667\n",
      "  Big      Claude semantic_soft_coverage_to_CQ  0.500000 0.666667\n",
      "  Big      Claude     semantic_diversity_score  0.500000 0.666667\n",
      "  Big Deepseek R1        lexical_query_overlap -0.866025 0.333333\n",
      "  Big Deepseek R1    semantic_similarity_to_CQ  0.500000 0.666667\n",
      "  Big Deepseek R1 semantic_soft_coverage_to_CQ -0.500000 0.666667\n",
      "  Big Deepseek R1     semantic_diversity_score  0.500000 0.666667\n",
      "  Big     GPT OSS        lexical_query_overlap  0.500000 0.666667\n",
      "  Big     GPT OSS    semantic_similarity_to_CQ -0.500000 0.666667\n",
      "  Big     GPT OSS semantic_soft_coverage_to_CQ  0.500000 0.666667\n",
      "  Big     GPT OSS     semantic_diversity_score  0.500000 0.666667\n",
      "  Big        GPT5        lexical_query_overlap  0.500000 0.666667\n",
      "  Big        GPT5    semantic_similarity_to_CQ  0.500000 0.666667\n",
      "  Big        GPT5 semantic_soft_coverage_to_CQ  1.000000 0.000000\n",
      "  Big        GPT5     semantic_diversity_score  0.500000 0.666667\n",
      "Small      Claude        lexical_query_overlap  0.500000 0.666667\n",
      "Small      Claude    semantic_similarity_to_CQ -0.500000 0.666667\n",
      "Small      Claude semantic_soft_coverage_to_CQ  1.000000 0.000000\n",
      "Small      Claude     semantic_diversity_score -1.000000 0.000000\n",
      "\n",
      "Summary of Spearman rho per metric (H2):\n",
      "                              count      mean  median\n",
      "metric                                               \n",
      "lexical_query_overlap             8 -0.045753     0.0\n",
      "semantic_diversity_score          8 -0.187500     0.0\n",
      "semantic_similarity_to_CQ         8 -0.187500    -0.5\n",
      "semantic_soft_coverage_to_CQ      8  0.437500     0.5\n",
      "\n",
      "Sign tests for H2 metrics (is negative trend more common than positive?):\n",
      "  lexical_query_overlap        neg= 4, pos= 4, n= 8, p=0.6367\n",
      "  semantic_diversity_score     neg= 4, pos= 4, n= 8, p=0.6367\n",
      "  semantic_similarity_to_CQ    neg= 5, pos= 3, n= 8, p=0.3633\n",
      "  semantic_soft_coverage_to_CQ neg= 2, pos= 6, n= 8, p=0.9648\n",
      "\n",
      "Big vs Small KG comparison on H2 metrics (Mann–Whitney U):\n",
      "                      metric  Big_mean  Small_mean    U        p  n_big  n_small\n",
      "       lexical_query_overlap  0.039489    0.054123 49.5 0.203827     12       12\n",
      "   semantic_similarity_to_CQ  0.655843    0.706447 86.0 0.435531     12       12\n",
      "semantic_soft_coverage_to_CQ  0.707232    0.763185 87.5 0.386373     12       12\n",
      "    semantic_diversity_score  0.318034    0.563909  2.0 0.000060     12       12\n",
      "\n",
      "\n",
      "=== Model complexity sensitivity (mean Spearman rho across H2 metrics) ===\n",
      "(More negative = stronger decrease as complexity increases)\n",
      "\n",
      "metric             lexical_query_overlap  semantic_similarity_to_CQ  semantic_soft_coverage_to_CQ  semantic_diversity_score  mean_rho_across_H2\n",
      "KG    Model                                                                                                                                    \n",
      "Small Deepseek R1              -0.500000                       -0.5                          -0.5                      -0.5           -0.500000\n",
      "      GPT5                     -0.500000                       -1.0                           1.0                      -1.0           -0.375000\n",
      "      GPT OSS                   0.500000                       -0.5                           0.5                      -1.0           -0.125000\n",
      "Big   Deepseek R1              -0.866025                        0.5                          -0.5                       0.5           -0.091506\n",
      "Small Claude                    0.500000                       -0.5                           1.0                      -1.0            0.000000\n",
      "Big   Claude                   -0.500000                        0.5                           0.5                       0.5            0.250000\n",
      "      GPT OSS                   0.500000                       -0.5                           0.5                       0.5            0.250000\n",
      "      GPT5                      0.500000                        0.5                           1.0                       0.5            0.625000\n",
      "\n",
      "Saved model sensitivity table to: analysis_plots2/model_complexity_sensitivity_H2.csv\n",
      "\n",
      "\n",
      "=== Simple vs Complex effect sizes (H2 metrics) ===\n",
      "\n",
      "lexical_query_overlap       : mean_Simple=0.039, mean_Complex=0.037, Cohen_d=0.103 (n1=8, n2=8)\n",
      "semantic_similarity_to_CQ   : mean_Simple=0.714, mean_Complex=0.711, Cohen_d=0.127 (n1=8, n2=8)\n",
      "semantic_soft_coverage_to_CQ: mean_Simple=0.768, mean_Complex=0.769, Cohen_d=-0.051 (n1=8, n2=8)\n",
      "semantic_diversity_score    : mean_Simple=0.464, mean_Complex=0.413, Cohen_d=0.341 (n1=8, n2=8)\n",
      "\n",
      "\n",
      "=== Model rankings by H2 metrics for each Complexity ===\n",
      "\n",
      "--- Metric: lexical_query_overlap | Simple ---\n",
      "      Model  mean_value Complexity                metric\n",
      "Deepseek R1    0.065000     Simple lexical_query_overlap\n",
      "     Claude    0.031818     Simple lexical_query_overlap\n",
      "    GPT OSS    0.031012     Simple lexical_query_overlap\n",
      "       GPT5    0.028250     Simple lexical_query_overlap\n",
      "\n",
      "--- Metric: lexical_query_overlap | Moderate ---\n",
      "      Model  mean_value Complexity                metric\n",
      "    GPT OSS    0.086270   Moderate lexical_query_overlap\n",
      "     Claude    0.078750   Moderate lexical_query_overlap\n",
      "       GPT5    0.055417   Moderate lexical_query_overlap\n",
      "Deepseek R1    0.037500   Moderate lexical_query_overlap\n",
      "\n",
      "--- Metric: lexical_query_overlap | Complex ---\n",
      "      Model  mean_value Complexity                metric\n",
      "    GPT OSS    0.059444    Complex lexical_query_overlap\n",
      "       GPT5    0.037500    Complex lexical_query_overlap\n",
      "     Claude    0.030714    Complex lexical_query_overlap\n",
      "Deepseek R1    0.020000    Complex lexical_query_overlap\n",
      "\n",
      "Saved rankings for lexical_query_overlap to: analysis_plots2/model_rankings_by_complexity_lexical_query_overlap.csv\n",
      "\n",
      "--- Metric: semantic_similarity_to_CQ | Simple ---\n",
      "      Model  mean_value Complexity                    metric\n",
      "    GPT OSS    0.730238     Simple semantic_similarity_to_CQ\n",
      "       GPT5    0.711417     Simple semantic_similarity_to_CQ\n",
      "Deepseek R1    0.706667     Simple semantic_similarity_to_CQ\n",
      "     Claude    0.705909     Simple semantic_similarity_to_CQ\n",
      "\n",
      "--- Metric: semantic_similarity_to_CQ | Moderate ---\n",
      "      Model  mean_value Complexity                    metric\n",
      "    GPT OSS    0.724683   Moderate semantic_similarity_to_CQ\n",
      "       GPT5    0.714583   Moderate semantic_similarity_to_CQ\n",
      "     Claude    0.701250   Moderate semantic_similarity_to_CQ\n",
      "Deepseek R1    0.335000   Moderate semantic_similarity_to_CQ\n",
      "\n",
      "--- Metric: semantic_similarity_to_CQ | Complex ---\n",
      "      Model  mean_value Complexity                    metric\n",
      "    GPT OSS    0.720778    Complex semantic_similarity_to_CQ\n",
      "Deepseek R1    0.720000    Complex semantic_similarity_to_CQ\n",
      "       GPT5    0.702500    Complex semantic_similarity_to_CQ\n",
      "     Claude    0.700714    Complex semantic_similarity_to_CQ\n",
      "\n",
      "Saved rankings for semantic_similarity_to_CQ to: analysis_plots2/model_rankings_by_complexity_semantic_similarity_to_CQ.csv\n",
      "\n",
      "--- Metric: semantic_soft_coverage_to_CQ | Simple ---\n",
      "      Model  mean_value Complexity                       metric\n",
      "Deepseek R1    0.784167     Simple semantic_soft_coverage_to_CQ\n",
      "    GPT OSS    0.771667     Simple semantic_soft_coverage_to_CQ\n",
      "       GPT5    0.758833     Simple semantic_soft_coverage_to_CQ\n",
      "     Claude    0.757727     Simple semantic_soft_coverage_to_CQ\n",
      "\n",
      "--- Metric: semantic_soft_coverage_to_CQ | Moderate ---\n",
      "      Model  mean_value Complexity                       metric\n",
      "    GPT OSS    0.779524   Moderate semantic_soft_coverage_to_CQ\n",
      "       GPT5    0.765833   Moderate semantic_soft_coverage_to_CQ\n",
      "     Claude    0.765000   Moderate semantic_soft_coverage_to_CQ\n",
      "Deepseek R1    0.365000   Moderate semantic_soft_coverage_to_CQ\n",
      "\n",
      "--- Metric: semantic_soft_coverage_to_CQ | Complex ---\n",
      "      Model  mean_value Complexity                       metric\n",
      "    GPT OSS     0.77600    Complex semantic_soft_coverage_to_CQ\n",
      "       GPT5     0.77125    Complex semantic_soft_coverage_to_CQ\n",
      "     Claude     0.77000    Complex semantic_soft_coverage_to_CQ\n",
      "Deepseek R1     0.75750    Complex semantic_soft_coverage_to_CQ\n",
      "\n",
      "Saved rankings for semantic_soft_coverage_to_CQ to: analysis_plots2/model_rankings_by_complexity_semantic_soft_coverage_to_CQ.csv\n",
      "\n",
      "--- Metric: semantic_diversity_score | Simple ---\n",
      "      Model  mean_value Complexity                   metric\n",
      "Deepseek R1    0.508657     Simple semantic_diversity_score\n",
      "     Claude    0.470947     Simple semantic_diversity_score\n",
      "    GPT OSS    0.457642     Simple semantic_diversity_score\n",
      "       GPT5    0.417437     Simple semantic_diversity_score\n",
      "\n",
      "--- Metric: semantic_diversity_score | Moderate ---\n",
      "      Model  mean_value Complexity                   metric\n",
      "       GPT5    0.510184   Moderate semantic_diversity_score\n",
      "     Claude    0.463681   Moderate semantic_diversity_score\n",
      "    GPT OSS    0.444517   Moderate semantic_diversity_score\n",
      "Deepseek R1    0.365833   Moderate semantic_diversity_score\n",
      "\n",
      "--- Metric: semantic_diversity_score | Complex ---\n",
      "      Model  mean_value Complexity                   metric\n",
      "       GPT5    0.431291    Complex semantic_diversity_score\n",
      "Deepseek R1    0.430764    Complex semantic_diversity_score\n",
      "     Claude    0.421168    Complex semantic_diversity_score\n",
      "    GPT OSS    0.369536    Complex semantic_diversity_score\n",
      "\n",
      "Saved rankings for semantic_diversity_score to: analysis_plots2/model_rankings_by_complexity_semantic_diversity_score.csv\n",
      "\n",
      "Saved combined rankings to: analysis_plots2/model_rankings_by_complexity_H2_ALL.csv\n",
      "\n",
      "Saving plots to: /home/shared_projects/jupyter/scripts/Chris_Thesis_Stuff/Analysis/analysis_plots2\n",
      "\n",
      "=== EXTRA: H2 KG effect summary (Big vs Small) ===\n",
      "\n",
      "            scope Complexity                       metric  Big_mean  Small_mean  n_big  n_small    U        p  cliffs_delta(Big-Small)\n",
      "          overall        ALL        lexical_query_overlap  0.039489    0.054123     12       12 49.5 0.203827                -0.312500\n",
      "          overall        ALL    semantic_similarity_to_CQ  0.655843    0.706447     12       12 86.0 0.435531                 0.194444\n",
      "          overall        ALL semantic_soft_coverage_to_CQ  0.707232    0.763185     12       12 87.5 0.386373                 0.215278\n",
      "          overall        ALL     semantic_diversity_score  0.318034    0.563909     12       12  2.0 0.000060                -0.972222\n",
      "within_complexity     Simple        lexical_query_overlap  0.039896    0.038144      4        4  6.0 0.685714                -0.250000\n",
      "within_complexity     Simple    semantic_similarity_to_CQ  0.705808    0.721307      4        4  5.0 0.485714                -0.375000\n",
      "within_complexity     Simple semantic_soft_coverage_to_CQ  0.772068    0.764129      4        4 10.0 0.685714                 0.250000\n",
      "within_complexity     Simple     semantic_diversity_score  0.282151    0.645191      4        4  0.0 0.028571                -1.000000\n",
      "within_complexity   Moderate        lexical_query_overlap  0.051840    0.077128      4        4  4.5 0.383630                -0.437500\n",
      "within_complexity   Moderate    semantic_similarity_to_CQ  0.539618    0.698140      4        4  9.0 0.885714                 0.125000\n",
      "within_complexity   Moderate semantic_soft_coverage_to_CQ  0.580521    0.757158      4        4  9.0 0.885714                 0.125000\n",
      "within_complexity   Moderate     semantic_diversity_score  0.314699    0.577408      4        4  0.0 0.028571                -1.000000\n",
      "within_complexity    Complex        lexical_query_overlap  0.026731    0.047098      4        4  5.0 0.485714                -0.375000\n",
      "within_complexity    Complex    semantic_similarity_to_CQ  0.722103    0.699893      4        4 14.5 0.081429                 0.812500\n",
      "within_complexity    Complex semantic_soft_coverage_to_CQ  0.769107    0.768268      4        4  9.0 0.885714                 0.125000\n",
      "within_complexity    Complex     semantic_diversity_score  0.357251    0.469129      4        4  0.0 0.028571                -1.000000\n",
      "\n",
      "Saved KG effect stats to: analysis_plots2/kg_effect_H2_mannwhitney_cliffsdelta.csv\n",
      "\n",
      "Done generating H2 plots.\n",
      "\n",
      "=== EXTRA: Friedman Test (Repeated-Measures across models) [H2] ===\n",
      "\n",
      "lexical_query_overlap       : Friedman χ² = 8.194, p = 0.0166\n",
      "semantic_similarity_to_CQ   : Friedman χ² = 0.750, p = 0.6873\n",
      "semantic_soft_coverage_to_CQ: Friedman χ² = 3.250, p = 0.1969\n",
      "semantic_diversity_score    : Friedman χ² = 2.250, p = 0.3247\n",
      "\n",
      "\n",
      "=== EXTRA: Jonckheere–Terpstra Trend Test [H2] ===\n",
      "\n",
      "SciPy version does not have jonckheere_terpstra; skipping this test.\n",
      "\n",
      "\n",
      "=== EXTRA: Kendall's W (agreement across complexity levels) [H2] ===\n",
      "\n",
      "lexical_query_overlap       : Kendall's W = 1.488\n",
      "semantic_similarity_to_CQ   : Kendall's W = 0.141\n",
      "semantic_soft_coverage_to_CQ: Kendall's W = 0.609\n",
      "semantic_diversity_score    : Kendall's W = 0.422\n",
      "\n",
      "\n",
      "=== EXTRA: Pairwise Wilcoxon tests (Simple/Moderate/Complex) [H2] ===\n",
      "\n",
      "lexical_query_overlap:\n",
      "  Simple vs Moderate : W = 8.000, p = 0.1953\n",
      "  Moderate vs Complex: W = 0.000, p = 0.0180\n",
      "  Simple vs Complex  : W = 16.000, p = 0.8438\n",
      "\n",
      "semantic_similarity_to_CQ:\n",
      "  Simple vs Moderate : W = 8.000, p = 0.1953\n",
      "  Moderate vs Complex: W = 15.000, p = 0.7422\n",
      "  Simple vs Complex  : W = 13.000, p = 0.5469\n",
      "\n",
      "semantic_soft_coverage_to_CQ:\n",
      "  Simple vs Moderate : W = 15.000, p = 0.7422\n",
      "  Moderate vs Complex: W = 6.000, p = 0.1094\n",
      "  Simple vs Complex  : W = 15.000, p = 0.7422\n",
      "\n",
      "semantic_diversity_score:\n",
      "  Simple vs Moderate : W = 17.000, p = 0.9453\n",
      "  Moderate vs Complex: W = 8.000, p = 0.1953\n",
      "  Simple vs Complex  : W = 11.000, p = 0.3828\n",
      "\n",
      "\n",
      "=== EXTRA: KG × Complexity analyses for H2 metrics ===\n",
      "\n",
      "\n",
      "--- Metric: lexical_query_overlap ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean=0.038, Big_mean=0.040, W=5.000, p=1.0000\n",
      "    Moderate : Small_mean=0.077, Big_mean=0.052, W=1.000, p=0.2850\n",
      "    Complex  : Small_mean=0.047, Big_mean=0.027, W=1.000, p=0.2500\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ²=6.000, p=0.0498\n",
      "    Big  : Friedman χ²=2.533, p=0.2818\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small=-0.039, mean_drop_Big=-0.012, W=2.000, p=0.3750\n",
      "    Moderate→Complex drop: mean_drop_Small=0.030, mean_drop_Big=0.025, W=5.000, p=1.0000\n",
      "    Simple→Complex drop: mean_drop_Small=-0.009, mean_drop_Big=0.013, W=2.000, p=0.3750\n",
      "\n",
      "--- Metric: semantic_similarity_to_CQ ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean=0.721, Big_mean=0.706, W=4.000, p=0.8750\n",
      "    Moderate : Small_mean=0.698, Big_mean=0.540, W=5.000, p=1.0000\n",
      "    Complex  : Small_mean=0.700, Big_mean=0.722, W=0.000, p=0.1250\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ²=3.500, p=0.1738\n",
      "    Big  : Friedman χ²=0.500, p=0.7788\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small=0.023, mean_drop_Big=0.166, W=4.000, p=0.8750\n",
      "    Moderate→Complex drop: mean_drop_Small=-0.002, mean_drop_Big=-0.182, W=3.000, p=0.6250\n",
      "    Simple→Complex drop: mean_drop_Small=0.021, mean_drop_Big=-0.016, W=1.000, p=0.2500\n",
      "\n",
      "--- Metric: semantic_soft_coverage_to_CQ ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean=0.764, Big_mean=0.772, W=2.000, p=0.3750\n",
      "    Moderate : Small_mean=0.757, Big_mean=0.581, W=5.000, p=1.0000\n",
      "    Complex  : Small_mean=0.768, Big_mean=0.769, W=5.000, p=1.0000\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ²=2.000, p=0.3679\n",
      "    Big  : Friedman χ²=1.500, p=0.4724\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small=0.007, mean_drop_Big=0.192, W=2.000, p=0.3750\n",
      "    Moderate→Complex drop: mean_drop_Small=-0.011, mean_drop_Big=-0.189, W=5.000, p=1.0000\n",
      "    Simple→Complex drop: mean_drop_Small=-0.004, mean_drop_Big=0.003, W=1.000, p=0.2500\n",
      "\n",
      "--- Metric: semantic_diversity_score ---\n",
      "  Big vs Small KG at each Complexity (Wilcoxon, Model-level):\n",
      "    Simple   : Small_mean=0.645, Big_mean=0.282, W=0.000, p=0.1250\n",
      "    Moderate : Small_mean=0.577, Big_mean=0.315, W=0.000, p=0.1250\n",
      "    Complex  : Small_mean=0.469, Big_mean=0.357, W=0.000, p=0.1250\n",
      "  Complexity effect within each KG (Friedman, Model-level):\n",
      "    Small: Friedman χ²=6.500, p=0.0388\n",
      "    Big  : Friedman χ²=3.500, p=0.1738\n",
      "  KG × Complexity (drops comparison, Small vs Big; Wilcoxon):\n",
      "    Simple→Moderate drop: mean_drop_Small=0.068, mean_drop_Big=-0.033, W=4.000, p=0.8750\n",
      "    Moderate→Complex drop: mean_drop_Small=0.108, mean_drop_Big=-0.043, W=5.000, p=1.0000\n",
      "    Simple→Complex drop: mean_drop_Small=0.176, mean_drop_Big=-0.075, W=0.000, p=0.1250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/root/.local/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "# ========= CONFIG =========\n",
    "# EXCEL_PATH = Path(\"all_models_consolidated_results.xlsx\")  # change if needed\n",
    "EXCEL_PATH = Path(\"balanced_models_consolidated_results.xlsx\")  # change if needed\n",
    "\n",
    "# Total # of CQs and per-complexity split (given by you)\n",
    "TOTAL_CQS = 33\n",
    "CQS_PER_COMPLEXITY = {\"Simple\": 12, \"Moderate\": 11, \"Complex\": 10}\n",
    "OUT_DIR = Path(\"analysis_plots2\")  # folder to store PNGs\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "# ==========================\n",
    "\n",
    "print(f\"Loading data from: {EXCEL_PATH}\")\n",
    "\n",
    "h2, h2_c = load_and_clean(EXCEL_PATH)\n",
    "h2_c_clean = prepare_h2_with_complexity(h2_c)\n",
    "\n",
    "# Analyses\n",
    "per_pair_df = analyze_h2(h2_c_clean)\n",
    "\n",
    "# Extra analyses\n",
    "model_sensitivity = summarize_model_sensitivity_h2(per_pair_df)\n",
    "effect_size_simple_vs_complex_h2(h2_c_clean)\n",
    "rankings = rank_models_by_complexity_h2(h2_c_clean)\n",
    "\n",
    "# Plots\n",
    "print(f\"Saving plots to: {OUT_DIR.resolve()}\")\n",
    "plot_h2_boxplots(h2_c_clean)\n",
    "plot_h2_trendlines(h2_c_clean)\n",
    "plot_h2_rho_heatmap(per_pair_df)\n",
    "plot_h2_boxplots_by_kg(h2_c_clean)\n",
    "plot_h2_boxplots_by_kg_within_complexity(h2_c_clean)\n",
    "kg_effect_table = h2_kg_effect_stats(h2_c_clean)\n",
    "print(\"Done generating H2 plots.\")\n",
    "\n",
    "# Extra tests to strengthen evidence (mirrors your H1 extras)\n",
    "friedman_test_h2(h2_c_clean)\n",
    "jt_test_h2(h2_c_clean)\n",
    "kendalls_w_h2(h2_c_clean)\n",
    "pairwise_wilcoxon_h2(h2_c_clean)\n",
    "kg_complexity_interaction_tests_h2(h2_c_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d9cd7-fbbc-4405-93cf-faa9b588df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead7bb6-b542-4b96-9ceb-b936a844732e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
