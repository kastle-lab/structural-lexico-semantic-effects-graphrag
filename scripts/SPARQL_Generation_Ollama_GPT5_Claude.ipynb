{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /root/.local/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /root/.local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /root/.local/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-Q19ZmgtABdY"
   },
   "outputs": [],
   "source": [
    "def load_file_to_string(file_path):\n",
    "  \"\"\"Loads the content of a file into a string.\n",
    "\n",
    "  Args:\n",
    "    file_path: The path to the file.\n",
    "\n",
    "  Returns:\n",
    "    The content of the file as a string.\n",
    "  \"\"\"\n",
    "  with open(file_path, 'r') as file:\n",
    "    file_contents = file.read()\n",
    "  return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR3ldIqORK_3"
   },
   "outputs": [],
   "source": [
    "# Define the file path and load its content into the 'schema' variable\n",
    "file_path = 'big_schema_trim.ttl'\n",
    "file_string = load_file_to_string(file_path)\n",
    "schema = file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uAfRrwJ0Ir1Z"
   },
   "outputs": [],
   "source": [
    "# Define a list of competency questions (CQs)\n",
    "CQs = [\n",
    "    # Simple\n",
    "    \"Who are all the characters available?\",\n",
    "    \"What are all the movies (and/or TV shows) available?\",\n",
    "    \"What are the real names and/or primary aliases for each of the characters?\",\n",
    "    \"What are all the associated species or types (e.g., human, Asgardian, AI) for each of the characters?\",\n",
    "    \"What are all the origin locations (e.g., homeworld, birthplace, base) for each of the characters?\",\n",
    "    \"What are all the release dates (or years), if available, for each of the movies?\",\n",
    "    \"Who are all the director(s) for each of the movies?\",\n",
    "    \"What are all the listed powers or abilities for each of the characters?\",\n",
    "    \"What are all the team or organization affiliations for each of the characters?\",\n",
    "    \"What are all the locations (places) available?\",\n",
    "    \"What are all the available movies and their directors?\",\n",
    "    \"What are all the available characters and their primary alias/real name?\",\n",
    "    #  Moderate\n",
    "    \"What are all the character–movie appearance pairs (which characters appear in which movies) available?\",\n",
    "    \"What are all the actor–character pairs (which actors portray which characters) available?\",\n",
    "    \"What are all the character–team–movie triples available where a character is a member of a team and appears in a movie?\",\n",
    "    \"What are all the character–power–movie triples available with respect to characters with specific powers and the movies they appear in?\",\n",
    "    \"What are all the pairs of characters that are linked through movies and co-appear in at least two movies?\",\n",
    "    \"What are all the teams and the set of members for each team available?\",\n",
    "    \"What are all the movies and the set of teams that have at least one member appearing in them?\",\n",
    "    \"What are all locations that are associated with at least one character appearance (e.g., origin or major setting)?\",\n",
    "    \"What are all the director–actor pairs linked through movies available?\",\n",
    "    \"What are all the available distinct powers and the set of characters associated with each power?\",\n",
    "    \"What are all the pairs of characters available that have co-appeared in a movie?\",\n",
    "    # Complex\n",
    "    \"How many movies does each character appear in (character appearance count)?\",\n",
    "    \"What are all the movies and their counts for each pair of characters that co-appear in multiple movies?\",\n",
    "    \"What are all the unions of movies for each of the teams in which any of their members appear (team-level filmography)?\",\n",
    "    \"Who are the distinct characters that possess a power, their counts, and rank-ordered by popularity?\",\n",
    "    \"Who are all the bridge characters that are members of more than one team, and what are those team combinations?\",\n",
    "    \"What are all the sets of characters that have been portrayed across all movies by each of the actors?\",\n",
    "    \"What are all the locations used as settings or associated contexts for multiple movies and/or characters and the counts of those associations?\",\n",
    "    \"What is the number of unique teams, unique powers, and unique locations represented via their characters for each of the movies?\",\n",
    "    \"Who are all the other characters connected via shared movie appearances for each of the characters (character co-appearance network)?\",\n",
    "    \"What are all the distributions of powers among each team's members, grouped by teams,  for comparing teams for computability (e.g., which powers are most characteristic of each team)?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "000Sb0kA51bz"
   },
   "outputs": [],
   "source": [
    "# Define a list of competency questions (CQs)\n",
    "def fill_prompt_template(template_text, values_dict):\n",
    "    for key, value in values_dict.items():\n",
    "        template_text = template_text.replace(f\"{{{key}}}\", value)\n",
    "    return template_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "39Rk-ktQ0rNQ"
   },
   "outputs": [],
   "source": [
    "# Temperature Config for LLM\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6kBIvHfcWELb"
   },
   "outputs": [],
   "source": [
    "# Define the initial system message for the language model\n",
    "initial_system_message = \"\"\"\n",
    "You are an expert in knowledge graphs and SPARQL query generation. Your task is to generate SPARQL queries based on the provided competency questions and a given TTL schema and return only the SPARQL query.\n",
    "\n",
    "Guidelines:\n",
    "Use only the schema provided in the context block to determine appropriate classes, properties, and relationships.\n",
    " - Ensure queries follow SPARQL syntax and use prefixes correctly.\n",
    " - Generate queries that efficiently retrieve relevant data while optimizing performance but with priority on correctness and efficiency.\n",
    " - If multiple valid queries exist, choose the most concise and efficient one.\n",
    " - Preserve the intent of the competency question while ensuring syntactic correctness.\n",
    " - Give only one SPARQL query and nothing else.\n",
    " - Only use the defined relationships in the schema. Don't use external ones unless specified.\n",
    " - If the competency question cannot be answered with the provided schema, respond to a partial extent that it can be answered to or respond with \"No valid query can be generated based on the provided schema.\"\n",
    " - Don't summarize or return an analysis of the given schema but return only the respective SPARQL query for the Competency Question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OjBBcfT-WELc"
   },
   "outputs": [],
   "source": [
    "# Define the template prompt for the language model\n",
    "template_prompt = \"\"\"\n",
    "Task: Write a SPARQL query that answers the following competency question:\n",
    "{Insert_CQ_here}\n",
    "\n",
    "Requirements:\n",
    "- Use the schema to determine correct URIs and relationships.\n",
    "- Ensure the query retrieves the necessary information efficiently.\n",
    "- Provide only one full SPARQL query without placeholders.\n",
    "- Don't summarize or return an analysis of the given schema but return only the respective SPARQL query for the Competency Question.\n",
    "\n",
    "Context:\n",
    "Below is the TTL schema of the knowledge graph:\n",
    "{Insert_schema_here}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GPT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.11.1)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.13.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare batch input\n",
    "batch_input_path = f\"{file_path.split('/')[-1]}_gpt5_batch_input.jsonl\"\n",
    "with open(batch_input_path, \"w\") as f:\n",
    "    for i, cq in enumerate(CQs):\n",
    "        custom_id = f\"cq-{i}\"\n",
    "        input_data = {\n",
    "            \"Insert_CQ_here\": cq,\n",
    "            \"Insert_schema_here\": schema\n",
    "        }\n",
    "        filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
    "        prompt_payload = {\n",
    "            \"custom_id\": custom_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-5\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": initial_system_message},\n",
    "                    {\"role\": \"user\", \"content\": filled_prompt}\n",
    "                ],\n",
    "                # \"temperature\": temperature,\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(prompt_payload) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Upload file\n",
    "file_upload = openai.files.create(file=open(batch_input_path, \"rb\"), purpose=\"batch\")\n",
    "file_id = file_upload.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch started with ID: batch_6908ed66be68819084aeedc6f7a2efb3\n"
     ]
    }
   ],
   "source": [
    "# 3. Create batch\n",
    "batch = openai.batches.create(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n",
    "batch_id = batch.id\n",
    "print(f\"Batch started with ID: {batch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Batch completed with status: completed\n"
     ]
    }
   ],
   "source": [
    "# 4. Poll until done\n",
    "while True:\n",
    "    current = openai.batches.retrieve(batch_id)\n",
    "    print(f\"Status: {current.status}\")\n",
    "    if current.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
    "        print(f\"Batch completed with status: {current.status}\")\n",
    "        break\n",
    "    time.sleep(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No error file available.\n"
     ]
    }
   ],
   "source": [
    "# 5. Download error\n",
    "error_file_id = current.error_file_id\n",
    "if error_file_id is None:\n",
    "    print(\"No error file available.\")\n",
    "else:\n",
    "    error_file = openai.files.content(error_file_id)\n",
    "    batch_output = error_file.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ID: cq-0\n",
      "Response:\n",
      "{'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 499360 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "\n",
      "Custom ID: cq-1\n",
      "Response:\n",
      "{'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 499364 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "\n",
      "Custom ID: cq-2\n",
      "Response:\n",
      "{'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 499368 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Parse and match results\n",
    "results_dict = {}\n",
    "for line in batch_output.strip().split(\"\\n\"):\n",
    "    data = json.loads(line)\n",
    "    custom_id = data[\"custom_id\"]\n",
    "    response_text = data[\"response\"][\"body\"]\n",
    "    print(f\"Custom ID: {custom_id}\\nResponse:\\n{response_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Download results\n",
    "output_file_id = current.output_file_id\n",
    "if output_file_id is None:\n",
    "    raise ValueError(\"No output file available.\")\n",
    "\n",
    "output_file = openai.files.content(output_file_id)\n",
    "batch_output = output_file.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parse and match results\n",
    "results_dict = {}\n",
    "for line in batch_output.strip().split(\"\\n\"):\n",
    "    data = json.loads(line)\n",
    "    custom_id = data[\"custom_id\"]\n",
    "    response_text = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    results_dict[custom_id] = {\n",
    "        \"result\": response_text,\n",
    "        \"raw\": json.dumps(data, indent=4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Map results back to original CQs\n",
    "cq_gpt4o_results = []\n",
    "for i, cq in enumerate(CQs):\n",
    "    custom_id = f\"cq-{i}\"\n",
    "    input_data = {\n",
    "        \"Insert_CQ_here\": cq,\n",
    "        \"Insert_schema_here\": schema\n",
    "    }\n",
    "    filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
    "    res = results_dict.get(custom_id, {\"result\": \"\", \"raw\": \"{}\"})\n",
    "    cq_gpt4o_results.append((cq, filled_prompt, res[\"result\"], res[\"raw\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved to: big_one_SamCat2_Covered (1).ttl_cq_GPT4o_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 8. Save to Excel\n",
    "df = pd.DataFrame(cq_gpt4o_results, columns=[\"CQ\", \"Prompt\", \"GPT5_Result\", \"GPT5_Raw\"])\n",
    "excel_file_path = f\"{file_path.split('/')[-1]}_cq_GPT5_results.xlsx\"\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "print(f\"Excel file saved to: {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama_interface import chat_with_model, get_ollama_models\n",
    "\n",
    "def interact_with_agent(model_name, system_message, user_prompt):\n",
    "    \"\"\"Interacts with a model agent using system and user prompts.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = chat_with_model(\n",
    "        model_name=model_name,\n",
    "        messages=messages,\n",
    "        options = {\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    )\n",
    "    return response,response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granite4:latest"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data processing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "models_to_run = [\n",
    "    'deepseek-r1:latest',\n",
    "    'gpt-oss:120b',\n",
    "    'granite3.3:latest',\n",
    "    'llama3.2:latest',\n",
    "    'mistral-small3.2:latest',\n",
    "    \"granite4:latest\",\n",
    "    'phi4-mini:latest',\n",
    "    'phi4-reasoning:latest',\n",
    "    'phi4:latest',\n",
    "    'gemma3:27b',\n",
    "    'mistral-large:latest',\n",
    "    'llama4:scout',\n",
    "    'llama4:latest',\n",
    "    'llama4:maverick',\n",
    "    'deepseek-r1:671b-fp16',\n",
    "    'deepseek-v3.1:latest',\n",
    "    'qwen3:235b']\n",
    "for model in models_to_run:\n",
    "    print(model, end=\"\")\n",
    "    try:\n",
    "        # Iterate through the competency questions and perform inference\n",
    "        cq_model_results = []\n",
    "        for cq in CQs:\n",
    "          input_data = {\n",
    "                \"Insert_CQ_here\": cq,\n",
    "                \"Insert_schema_here\": schema,\n",
    "            }\n",
    "          filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
    "          model_analysis_raw, model_analysis_result = interact_with_agent(model, initial_system_message, filled_prompt)\n",
    "          cq_model_results.append((cq, filled_prompt, model_analysis_result, model_analysis_raw))\n",
    "          print(\"*\"*10)\n",
    "          print(cq)\n",
    "          print(\"*\"*10)\n",
    "          print(\"\\n\")\n",
    "        \n",
    "        # Create a pandas DataFrame from the results\n",
    "        df = pd.DataFrame(cq_model_results, columns=['CQ', 'Prompt', f'{model}_Analysis_Result', f'{model}_Analysis_Raw'])\n",
    "        \n",
    "        # Save the results to an Excel file\n",
    "        excel_file_path = f'{file_path.split(\"/\")[-1].rstrip(\".ttl\")}_cq_{model.replace(\":\",\"-\")}_results.xlsx'\n",
    "        df.to_excel(excel_file_path, index=False)\n",
    "        \n",
    "        print(f\"Excel file saved to: {excel_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"failed {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude Key\n",
    "claude_api_key = \"\"\n",
    "client = anthropic.Anthropic(api_key=claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare batch input\n",
    "batch_input_list = []\n",
    "for i, cq in enumerate(CQs):\n",
    "    custom_id = f\"cq-{i}\"\n",
    "    input_data = {\n",
    "        \"Insert_CQ_here\": cq,\n",
    "        \"Insert_schema_here\": schema\n",
    "    }\n",
    "    filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
    "    batch_input_list.append(Request(\n",
    "        custom_id = custom_id,\n",
    "        params=MessageCreateParamsNonStreaming(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=2048,\n",
    "            system = initial_system_message,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": filled_prompt,\n",
    "            }],\n",
    "            temperature = temperature,\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create batch\n",
    "batch = client.messages.batches.create(\n",
    "    requests=batch_input_list\n",
    ")\n",
    "batch_id = batch.id\n",
    "print(f\"Batch started with ID: {batch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Poll until done\n",
    "while True:\n",
    "    current = client.messages.batches.retrieve(batch_id)\n",
    "    print(f\"Status: {current.processing_status}\")\n",
    "    if current.processing_status in [\"ended\"]:\n",
    "        print(f\"Batch completed with status: {current}\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Download results\n",
    "results_dict = {}\n",
    "for result in client.messages.batches.results(batch_id):\n",
    "    custom_id = result.custom_id\n",
    "    response_text = result.result.message.content[0].text\n",
    "    results_dict[custom_id] = {\n",
    "        \"result\": response_text,\n",
    "        \"raw\": json.dumps(str(result), indent=4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Map results back to original CQs\n",
    "cq_claude_results = []\n",
    "for i, cq in enumerate(CQs):\n",
    "    custom_id = f\"cq-{i}\"\n",
    "    input_data = {\n",
    "        \"Insert_CQ_here\": cq,\n",
    "        \"Insert_schema_here\": schema\n",
    "    }\n",
    "    filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
    "    res = results_dict.get(custom_id, {\"result\": \"\", \"raw\": \"{}\"})\n",
    "    cq_claude_results.append((cq, filled_prompt, res[\"result\"], res[\"raw\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save to Excel\n",
    "df = pd.DataFrame(cq_claude_results, columns=[\"CQ\", \"Prompt\", \"Claude_Result\", \"Claude_Raw\"])\n",
    "excel_file_path = f\"{file_path.split('/')[-1]}_cq_Claude_results.xlsx\"\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "print(f\"Excel file saved to: {excel_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
