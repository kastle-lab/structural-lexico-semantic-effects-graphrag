{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61300f53-b459-43e3-850a-bf6c53000f1d",
   "metadata": {},
   "source": [
    "Generalized selector + ontology/usage coverage merger for RDF/XML → TTL.\n",
    "\n",
    "Pass 1: Selection\n",
    "  - Include ALL files whose tokens contain 'class' or 'property' (only hardcoding).\n",
    "  - For every other auto-detected category (by branching), include at least N_PER_CATEGORY files.\n",
    "\n",
    "Pass 2: Coverage\n",
    "  - Ensure every defined predicate (rdf:Property, owl:ObjectProperty, owl:DatatypeProperty, owl:AnnotationProperty)\n",
    "    appears in at least one triple (as a predicate).\n",
    "  - Ensure every defined class (rdfs:Class, owl:Class) appears as an rdf:type object at least once.\n",
    "  - Iteratively add files that contribute missing coverage from the remaining pool (up to caps).\n",
    "\n",
    "Requires:\n",
    "    pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba52fb-5e38-4249-9ab0-ea4b2b180f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from rdflib import Graph, URIRef\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "\n",
    "# ================== CONFIG ======================\n",
    "INPUT_DIR          = \"datasets/big_one\"   # folder containing your .xml files\n",
    "FILE_GLOB_PATTERN  = \"*.xml\"\n",
    "\n",
    "# Selection (Pass 1)\n",
    "N_PER_CATEGORY     = 1           # include at least this many per non-class/property category\n",
    "RANDOMIZE_PICK     = False       # True => random sample inside each category\n",
    "RANDOM_SEED        = 42\n",
    "VERBOSE            = True\n",
    "\n",
    "# Coverage (Pass 2)\n",
    "ENSURE_RELATION_COVERAGE      = True   # ensure every defined predicate is used at least once\n",
    "ENSURE_CLASS_INSTANCE_COVERAGE = True  # ensure every defined class has at least one instance (rdf:type)\n",
    "MAX_SECOND_PASS_FILES          = 1000  # hard cap on additional files to try\n",
    "STOP_WHEN_FULLY_COVERED        = True  # stop early as soon as coverage is complete\n",
    "MIN_EXAMPLES_PER_RELATION = 1   # at least this many examples for each predicate/class\n",
    "\n",
    "# Output\n",
    "OUTPUT_TTL = f\"{Path(INPUT_DIR).name}_SamCat{N_PER_CATEGORY}_Covered.ttl\"\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3782e79-64aa-4efb-9499-ad38bdbf539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_INCLUDE_TOKENS = {\"class\", \"property\"}  # only hardcoding per your constraints\n",
    "\n",
    "\n",
    "# ---------- Helpers: filename tokenization & selection ----------\n",
    "\n",
    "def split_tokens(path: Path) -> list[str]:\n",
    "    \"\"\"Hyphen-tokenize the filename stem (lowercased).\"\"\"\n",
    "    return path.stem.lower().split(\"-\")\n",
    "\n",
    "def collect_files(input_dir: str, pattern: str) -> list[Path]:\n",
    "    paths = [Path(p) for p in glob.glob(str(Path(input_dir) / pattern))]\n",
    "    return [p for p in paths if p.is_file()]\n",
    "\n",
    "def build_branching_scores(tokenized_files: dict[Path, list[str]]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    For every occurrence of token t at position i, record the next token (i+1).\n",
    "    branching[t] = number of distinct next tokens globally.\n",
    "    \"\"\"\n",
    "    nxts: dict[str, set[str]] = defaultdict(set)\n",
    "    for toks in tokenized_files.values():\n",
    "        for i, t in enumerate(toks[:-1]):\n",
    "            nxts[t].add(toks[i+1])\n",
    "    return {t: len(s) for t, s in nxts.items()}\n",
    "\n",
    "def choose_category_tokens(branching: dict[str, int]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Adaptive threshold: tokens with branching >= max(Q3, FLOOR).\n",
    "    \"\"\"\n",
    "    if not branching:\n",
    "        return set()\n",
    "    scores = sorted(branching.values())\n",
    "    q3_idx = max(0, int(0.75 * (len(scores) - 1)))\n",
    "    q3 = scores[q3_idx]\n",
    "    FLOOR = 5\n",
    "    thr = max(q3, FLOOR)\n",
    "    return {t for t, sc in branching.items() if sc >= thr}\n",
    "\n",
    "def detect_category_for_file(tokens: list[str], category_tokens: set[str]) -> str:\n",
    "    for t in tokens:\n",
    "        if t in category_tokens:\n",
    "            return t\n",
    "    return \"root\"\n",
    "\n",
    "def select_files_pass1(files: list[Path]) -> list[Path]:\n",
    "    \"\"\"Pass 1: selection based on branching categories, with 'class'/'property' fully included.\"\"\"\n",
    "    if not files:\n",
    "        return []\n",
    "\n",
    "    tokenized = {f: split_tokens(f) for f in files}\n",
    "    branching = build_branching_scores(tokenized)\n",
    "    category_tokens = choose_category_tokens(branching)\n",
    "\n",
    "    # Include ALL files containing 'class' or 'property' in their tokens\n",
    "    specials = [f for f, toks in tokenized.items() if any(t in SPECIAL_INCLUDE_TOKENS for t in toks)]\n",
    "    specials.sort(key=lambda p: p.name)\n",
    "\n",
    "    # Bucket the rest by detected category\n",
    "    buckets: dict[str, list[Path]] = defaultdict(list)\n",
    "    for f, toks in tokenized.items():\n",
    "        if f in specials:\n",
    "            continue\n",
    "        cat = detect_category_for_file(toks, category_tokens)\n",
    "        buckets[cat].append(f)\n",
    "\n",
    "    # Select at least N_PER_CATEGORY from each non-special category\n",
    "    picks: list[Path] = []\n",
    "    for cat, flist in buckets.items():\n",
    "        if not flist:\n",
    "            continue\n",
    "        if RANDOMIZE_PICK:\n",
    "            rnd = random.Random(RANDOM_SEED)\n",
    "            pool = flist[:]\n",
    "            rnd.shuffle(pool)\n",
    "        else:\n",
    "            pool = sorted(flist, key=lambda p: p.name)\n",
    "        need = N_PER_CATEGORY if len(pool) >= N_PER_CATEGORY else len(pool)\n",
    "        picks.extend(pool[:need])\n",
    "\n",
    "    # Combine & de-dup (preserve order)\n",
    "    combined = specials + picks\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in combined:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            unique.append(p)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"Pass 1 selection complete.\")\n",
    "        # Simple readout:\n",
    "        by_cat = defaultdict(int)\n",
    "        # Recompute categories for reporting only:\n",
    "        for f in unique:\n",
    "            by_cat[detect_category_for_file(split_tokens(f), category_tokens)] += 1\n",
    "        print(\"Selected per category (incl. class/property):\")\n",
    "        for c in sorted(by_cat):\n",
    "            print(f\"  - {c}: {by_cat[c]}\")\n",
    "        print(f\"Total selected: {len(unique)}\")\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "# ---------- Helpers: graph analysis for coverage ----------\n",
    "\n",
    "def defined_predicates(g: Graph) -> set[URIRef]:\n",
    "    \"\"\"All predicates declared as properties in the graph.\"\"\"\n",
    "    props = set()\n",
    "    for p_type in (RDF.Property, OWL.ObjectProperty, OWL.DatatypeProperty, OWL.AnnotationProperty):\n",
    "        for s in g.subjects(RDF.type, p_type):\n",
    "            if isinstance(s, URIRef):\n",
    "                props.add(s)\n",
    "    return props\n",
    "\n",
    "def used_predicates(g: Graph) -> set[URIRef]:\n",
    "    \"\"\"All predicates that actually occur in (s, p, o) triples (excluding rdf:type if you like).\"\"\"\n",
    "    preds = set()\n",
    "    for _, p, _ in g:\n",
    "        if isinstance(p, URIRef):\n",
    "            preds.add(p)\n",
    "    return preds\n",
    "\n",
    "def defined_classes(g: Graph) -> set[URIRef]:\n",
    "    \"\"\"All classes declared in the graph.\"\"\"\n",
    "    classes = set()\n",
    "    for c_type in (RDFS.Class, OWL.Class):\n",
    "        for s in g.subjects(RDF.type, c_type):\n",
    "            if isinstance(s, URIRef):\n",
    "                classes.add(s)\n",
    "    return classes\n",
    "\n",
    "def typed_classes(g: Graph) -> set[URIRef]:\n",
    "    \"\"\"All classes that occur as objects of rdf:type triples.\"\"\"\n",
    "    tclasses = set()\n",
    "    for _, _, o in g.triples((None, RDF.type, None)):\n",
    "        if isinstance(o, URIRef):\n",
    "            tclasses.add(o)\n",
    "    return tclasses\n",
    "\n",
    "\n",
    "# ---------- Pass 2: coverage augmentation ----------\n",
    "\n",
    "def augment_for_coverage(selected: list[Path], all_files: list[Path]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Ensure:\n",
    "      - Each defined predicate appears in at least MIN_EXAMPLES_PER_RELATION triples.\n",
    "      - Each defined class has at least MIN_EXAMPLES_PER_RELATION instances.\n",
    "    Iteratively add files that increase coverage.\n",
    "    \"\"\"\n",
    "    if not (ENSURE_RELATION_COVERAGE or ENSURE_CLASS_INSTANCE_COVERAGE):\n",
    "        return selected\n",
    "\n",
    "    g = Graph()\n",
    "    if VERBOSE:\n",
    "        print(\"\\nPass 2: Building initial graph for coverage checks...\")\n",
    "    for i, f in enumerate(selected, 1):\n",
    "        try:\n",
    "            g.parse(str(f), format=\"xml\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Initialize coverage counts\n",
    "    pred_count = defaultdict(int)\n",
    "    cls_count = defaultdict(int)\n",
    "\n",
    "    for s, p, o in g:\n",
    "        if isinstance(p, URIRef):\n",
    "            pred_count[p] += 1\n",
    "        if p == RDF.type and isinstance(o, URIRef):\n",
    "            cls_count[o] += 1\n",
    "\n",
    "    defs_pred = defined_predicates(g) if ENSURE_RELATION_COVERAGE else set()\n",
    "    defs_cls  = defined_classes(g)    if ENSURE_CLASS_INSTANCE_COVERAGE else set()\n",
    "\n",
    "    def missing_preds():\n",
    "        return {p for p in defs_pred if pred_count[p] < MIN_EXAMPLES_PER_RELATION}\n",
    "    def missing_classes():\n",
    "        return {c for c in defs_cls if cls_count[c] < MIN_EXAMPLES_PER_RELATION}\n",
    "\n",
    "    miss_pred = missing_preds()\n",
    "    miss_cls  = missing_classes()\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"Initial missing predicates: {len(miss_pred)} | classes: {len(miss_cls)}\")\n",
    "\n",
    "    selected_set = set(selected)\n",
    "    candidates = [f for f in all_files if f not in selected_set]\n",
    "\n",
    "    added = 0\n",
    "    for f in candidates:\n",
    "        if MAX_SECOND_PASS_FILES and added >= MAX_SECOND_PASS_FILES:\n",
    "            break\n",
    "        if STOP_WHEN_FULLY_COVERED and not miss_pred and not miss_cls:\n",
    "            break\n",
    "\n",
    "        temp = Graph()\n",
    "        try:\n",
    "            temp.parse(str(f), format=\"xml\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        adds_pred = False\n",
    "        adds_cls  = False\n",
    "\n",
    "        # Check predicates\n",
    "        if ENSURE_RELATION_COVERAGE and miss_pred:\n",
    "            for _, p, _ in temp:\n",
    "                if p in miss_pred:\n",
    "                    adds_pred = True\n",
    "                    break\n",
    "\n",
    "        # Check classes\n",
    "        if ENSURE_CLASS_INSTANCE_COVERAGE and miss_cls and not adds_pred:\n",
    "            for _, _, o in temp.triples((None, RDF.type, None)):\n",
    "                if o in miss_cls:\n",
    "                    adds_cls = True\n",
    "                    break\n",
    "\n",
    "        if not (adds_pred or adds_cls):\n",
    "            continue\n",
    "\n",
    "        g += temp\n",
    "        selected.append(f)\n",
    "        added += 1\n",
    "\n",
    "        # Update counts\n",
    "        for _, p, _ in temp:\n",
    "            if isinstance(p, URIRef):\n",
    "                pred_count[p] += 1\n",
    "        for _, _, o in temp.triples((None, RDF.type, None)):\n",
    "            if isinstance(o, URIRef):\n",
    "                cls_count[o] += 1\n",
    "\n",
    "        miss_pred = missing_preds()\n",
    "        miss_cls  = missing_classes()\n",
    "\n",
    "        if VERBOSE and (added % 50 == 0 or not miss_pred and not miss_cls):\n",
    "            print(f\"Added {added} files | missing preds: {len(miss_pred)} | missing classes: {len(miss_cls)}\")\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"Final added files: {added}\")\n",
    "        print(f\"Remaining missing preds: {len(miss_pred)} | classes: {len(miss_cls)}\")\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "# ---------- Merge to TTL ----------\n",
    "\n",
    "def merge_to_ttl(files: list[Path], out_path: str):\n",
    "    g = Graph()\n",
    "    total = len(files)\n",
    "    if VERBOSE:\n",
    "        print(f\"\\nMerging {total} files → {out_path}\")\n",
    "    for i, f in enumerate(files, 1):\n",
    "        if VERBOSE and (i == 1 or i % 200 == 0 or i == total):\n",
    "            print(f\"[{i}/{total}] {f.name}\")\n",
    "        try:\n",
    "            g.parse(str(f), format=\"xml\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Skipping {f.name}: {e}\")\n",
    "    if VERBOSE:\n",
    "        print(f\"Triples in merged graph: {len(g)}\")\n",
    "        print(\"Writing Turtle...\")\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    g.serialize(destination=out_path, format=\"turtle\")\n",
    "    if VERBOSE:\n",
    "        print(\"✅ Done.\")\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    all_files = collect_files(INPUT_DIR, FILE_GLOB_PATTERN)\n",
    "    if not all_files:\n",
    "        print(f\"No files found in {INPUT_DIR} matching {FILE_GLOB_PATTERN}\")\n",
    "        return\n",
    "\n",
    "    # Pass 1: base selection\n",
    "    selected = select_files_pass1(all_files)\n",
    "\n",
    "    # Pass 2: ontology & usage coverage (optional)\n",
    "    selected = augment_for_coverage(selected, all_files)\n",
    "\n",
    "    # Final merge\n",
    "    merge_to_ttl(selected, OUTPUT_TTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44738b6b-843a-490a-867d-9039575f4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 selection complete.\n",
      "Selected per category (incl. class/property):\n",
      "  - marvel: 576\n",
      "  - org: 556\n",
      "Total selected: 1132\n",
      "\n",
      "Pass 2: Building initial graph for coverage checks...\n",
      "Initial missing predicates: 126 | classes: 167\n",
      "Added 50 files | missing preds: 99 | missing classes: 125\n",
      "Added 100 files | missing preds: 82 | missing classes: 81\n",
      "Added 150 files | missing preds: 39 | missing classes: 40\n",
      "Added 200 files | missing preds: 4 | missing classes: 5\n",
      "Final added files: 207\n",
      "Remaining missing preds: 1 | classes: 1\n",
      "\n",
      "Merging 1339 files → big_one_SamCat2_Covered.ttl\n",
      "[1/1339] dbkwik-webdatacommons-org-marvel-wikia-com-class-1st-real-name.xml\n",
      "[200/1339] dbkwik-webdatacommons-org-marvel-wikia-com-property-box1image.xml\n",
      "[400/1339] dbkwik-webdatacommons-org-marvel-wikia-com-resource-category-uncanny-x-men-first-class-vol-1-7-images.xml\n",
      "[600/1339] marvel-wikia-com-wiki-special-filepath-5cnx-men-first-class-vol-1-4-textless-jpg.xml\n",
      "[800/1339] marvel-wikia-com-wiki-special-filepath-janos-quested-earth-10005-from-x-men-first-class-film-0006-jpg.xml\n",
      "[1000/1339] marvel-wikia-com-wiki-special-filepath-wolverine-first-class-vol-1-21-jpg.xml\n",
      "[1200/1339] dbkwik-webdatacommons-org-marvel-wikia-com-resource-ken-barr.xml\n",
      "[1339/1339] dbkwik-webdatacommons-org-marvel-wikia-com-resource-sentry-fallen-sun-vol-1.xml\n",
      "Triples in merged graph: 17161\n",
      "Writing Turtle...\n",
      "✅ Done.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721d544-2e5a-4593-9cf6-5c39fb3ff883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
