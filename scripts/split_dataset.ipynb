{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be42ac8-f25b-4227-9a11-8f179ba98bdd",
   "metadata": {},
   "source": [
    "Split an RDF/XML file into one file per <rdf:Description>, preserving the exact\n",
    "header (everything before the first <rdf:Description>) in each output.\n",
    "\n",
    "- Input:  .xml\n",
    "- Output: .xml files in OUT_DIR, each with:\n",
    "    [original header text]\n",
    "    <rdf:Description ...> ... </rdf:Description>\n",
    "    </rdf:RDF>\n",
    "\n",
    "No assumptions about \"13 lines\" â€” the header is detected dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbdff3-a09e-442e-875f-012aa4c37cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_FILE = \"datasets/small_one.xml\"          # path to your big RDF/XML\n",
    "OUT_DIR    = \"datasets/small_one\"   # destination folder for per-subject XMLs\n",
    "BASENAME   = \"subject\"            # fallback base name if rdf:about not present\n",
    "# -------------\n",
    "\n",
    "DESC_START_RE = re.compile(r\"<rdf:Description\\b\", re.IGNORECASE)\n",
    "DESC_END_RE   = re.compile(r\"</rdf:Description\\s*>\", re.IGNORECASE)\n",
    "ABOUT_ATTR_RE = re.compile(r'rdf:about\\s*=\\s*\"(.*?)\"')  # capture subject URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b3b9ef-dc52-4a81-9ee4-ded42afcdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(value: str, maxlen: int = 160) -> str:\n",
    "    # Make a filesystem-safe name from the subject URI\n",
    "    v = value\n",
    "    v = re.sub(r\"^[a-z]+://\", \"\", v, flags=re.I)\n",
    "    v = v.strip().lower()\n",
    "    v = re.sub(r\"[^a-z0-9]+\", \"-\", v).strip(\"-\")\n",
    "    return (v or BASENAME)[:maxlen]\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def write_one_output(header_text: str, block_lines: list[str], idx: int):\n",
    "    # Try to extract rdf:about for file naming (prefer from the first line of block)\n",
    "    first = block_lines[0] if block_lines else \"\"\n",
    "    m = ABOUT_ATTR_RE.search(first)\n",
    "    if not m:\n",
    "        # Search entire block if needed\n",
    "        joined = \"\".join(block_lines)\n",
    "        m = ABOUT_ATTR_RE.search(joined)\n",
    "    name = slugify(m.group(1)) if m else f\"{BASENAME}-{idx:06d}\"\n",
    "    out_path = os.path.join(OUT_DIR, f\"{name}.xml\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "        fout.write(header_text)\n",
    "        fout.writelines(block_lines)\n",
    "        # Ensure newline before closing tag if the block didn't end with one\n",
    "        if not (block_lines and block_lines[-1].endswith(\"\\n\")):\n",
    "            fout.write(\"\\n\")\n",
    "        fout.write(\"</rdf:RDF>\\n\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c595f00-09c5-40e1-9afd-ff4f674ac868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ensure_dir(OUT_DIR)\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\", newline=\"\") as fin:\n",
    "        header_chunks: list[str] = []\n",
    "        first_desc_found = False\n",
    "        carry = \"\"  # to handle cases where <rdf:Description> starts mid-line\n",
    "\n",
    "        # 1) Read until the FIRST <rdf:Description> appears; everything before is header\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if line == \"\":\n",
    "                raise RuntimeError(\"No <rdf:Description> found in the file.\")\n",
    "            test = carry + line\n",
    "            m = DESC_START_RE.search(test)\n",
    "            if m:\n",
    "                # Split: header_text = before the first <rdf:Description>\n",
    "                start_idx = m.start()\n",
    "                header_text = \"\".join(header_chunks) + test[:start_idx]\n",
    "                # Normalize: make sure header_text ends *before* any descriptions\n",
    "                # We'll start processing descriptions beginning at the match\n",
    "                remainder = test[start_idx:]\n",
    "                first_desc_found = True\n",
    "                break\n",
    "            else:\n",
    "                header_chunks.append(test)\n",
    "                carry = \"\"  # we've committed carry into header\n",
    "\n",
    "        # Optional sanity: ensure header contains the opening <rdf:RDF ...> and not its closing\n",
    "        if \"</rdf:RDF>\" in header_chunks[-1]:\n",
    "            raise RuntimeError(\"Detected </rdf:RDF> in header; input may be malformed.\")\n",
    "\n",
    "        # 2) Process all <rdf:Description> blocks from remainder + rest of file\n",
    "        outputs = 0\n",
    "        current_block: list[str] = []\n",
    "        in_block = False\n",
    "\n",
    "        def flush_block():\n",
    "            nonlocal outputs, current_block\n",
    "            if current_block:\n",
    "                out_path = write_one_output(header_text, current_block, outputs + 1)\n",
    "                outputs += 1\n",
    "                current_block = []\n",
    "                return out_path\n",
    "            return None\n",
    "\n",
    "        # Helper to process text chunk that may contain multiple blocks\n",
    "        def feed_text(chunk: str):\n",
    "            nonlocal in_block, current_block\n",
    "            pos = 0\n",
    "            while pos < len(chunk):\n",
    "                if not in_block:\n",
    "                    mstart = DESC_START_RE.search(chunk, pos)\n",
    "                    if not mstart:\n",
    "                        # no new block start in the rest of this chunk\n",
    "                        return\n",
    "                    # start a new block at this index\n",
    "                    in_block = True\n",
    "                    current_block.append(chunk[mstart.start():])\n",
    "                    # move pos to end (we've appended the rest; we'll check for end in next step)\n",
    "                    pos = len(chunk)\n",
    "                else:\n",
    "                    # we are inside a block; look for its end\n",
    "                    mend = DESC_END_RE.search(chunk, pos)\n",
    "                    if mend:\n",
    "                        # Append up to and including the end tag to the current block\n",
    "                        current_block[-1] += chunk[pos:mend.end()]\n",
    "                        # If there is trailing content after the end tag on this same chunk, keep it\n",
    "                        tail = chunk[mend.end():]\n",
    "                        # Flush the completed block\n",
    "                        flush_block()\n",
    "                        in_block = False\n",
    "                        # If tail contains another start, loop will catch it (since pos < len(tail))\n",
    "                        chunk = tail\n",
    "                        pos = 0\n",
    "                        continue\n",
    "                    else:\n",
    "                        # No end in this chunk; just append and wait for more\n",
    "                        current_block[-1] += chunk[pos:]\n",
    "                        pos = len(chunk)\n",
    "\n",
    "        # Feed the remainder that began at the first <rdf:Description>\n",
    "        feed_text(remainder)\n",
    "\n",
    "        # Stream the rest of the file\n",
    "        for line in fin:\n",
    "            feed_text(line)\n",
    "\n",
    "        # 3) If the file ended while inside a block, finalize it\n",
    "        if in_block:\n",
    "            flush_block()\n",
    "\n",
    "    print(f\"Done. Wrote {outputs} files to: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f10cd2d9-0120-447a-9550-c99a7c3cfd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote 137784 files to: dataset/small_one\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c572d2-2f3f-4793-9cc9-4023a956acd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
