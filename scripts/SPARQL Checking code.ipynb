{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "006efec0-be28-4f73-8c81-5c4f25dc9a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SPARQLWrapper in /root/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib in /root/.local/lib/python3.10/site-packages (7.1.4)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /root/.local/lib/python3.10/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /root/.local/lib/python3.10/site-packages (from rdflib) (3.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install SPARQLWrapper rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "851c023a-2918-4168-b953-782acfa29e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, math, hashlib, statistics, argparse\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Internal helpers (used by metrics)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _run(endpoint: str, query: str, headers: Optional[Dict[str,str]]=None):\n",
    "    \"\"\"\n",
    "    Execute a SPARQL query on a given endpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoint : str\n",
    "        SPARQL endpoint URL.\n",
    "    query : str\n",
    "        A SPARQL query string (SELECT/ASK/CONSTRUCT/DESCRIBE).\n",
    "    headers : Optional[Dict[str,str]]\n",
    "        Extra HTTP headers (e.g., to toggle reasoning modes if your store supports it).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (bindings, raw) : (List[Dict], Dict)\n",
    "        bindings : list of row dicts in SPARQL JSON format (for SELECT).\n",
    "        raw      : the full parsed JSON result from the endpoint. For ASK queries,\n",
    "                   many engines return {'boolean': True/False} at the top level.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This helper normalizes responses to the part we need most often (bindings).\n",
    "    - For ASK queries, you will typically read `raw.get(\"boolean\")`.\n",
    "    \"\"\"\n",
    "    s = SPARQLWrapper(endpoint)\n",
    "    s.setReturnFormat(JSON)\n",
    "    s.setQuery(query)\n",
    "    if headers:\n",
    "        for k,v in headers.items(): s.addCustomHttpHeader(k,v)\n",
    "    res = s.query().convert()\n",
    "    return res.get(\"results\", {}).get(\"bindings\", []), res\n",
    "\n",
    "\n",
    "def _normalize_binding(b):\n",
    "    \"\"\"\n",
    "    Normalize one row (binding dict) into simple comparable strings.\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    - Make rows comparable across runs by folding language tags and datatypes into the string.\n",
    "    - Used by checksums and set-similarity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict : { var_name -> normalized_string }\n",
    "           where literal values include \"@lang\" and/or \"^^<datatype>\" suffixes.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, v in b.items():\n",
    "        s = v[\"value\"]\n",
    "        if \"xml:lang\" in v: s += f\"@{v['xml:lang']}\"\n",
    "        if \"datatype\" in v: s += f\"^^<{v['datatype']}>\"\n",
    "        out[k] = s\n",
    "    return out\n",
    "\n",
    "\n",
    "def _result_checksum(bindings: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Order-insensitive checksum of a result set.\n",
    "\n",
    "    Interpretation\n",
    "    --------------\n",
    "    - Identical checksums across repeated runs → same multiset of rows (stable results).\n",
    "    - Useful for determinism testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sha256 hex string for the normalized, sorted representation of rows.\n",
    "    \"\"\"\n",
    "    norm = [tuple(sorted(_normalize_binding(b).items())) for b in bindings]\n",
    "    norm.sort()\n",
    "    return hashlib.sha256(repr(norm).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def _jaccard(a, b) -> float:\n",
    "    \"\"\"\n",
    "    Jaccard similarity of two result sets (0..1).\n",
    "\n",
    "    Interpretation\n",
    "    --------------\n",
    "    - 1.0 → exactly identical sets (or both empty).\n",
    "    - 0.0 → no overlap at all.\n",
    "    - Useful when comparing results across snapshots or regimes.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    - This is unused in the minimal API below but kept for extensibility.\n",
    "    \"\"\"\n",
    "    A = {tuple(sorted(_normalize_binding(x).items())) for x in a}\n",
    "    B = {tuple(sorted(_normalize_binding(x).items())) for x in b}\n",
    "    return 1.0 if not (A or B) else len(A & B) / len(A | B)\n",
    "\n",
    "\n",
    "def _extract_where(q: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the top-level WHERE {...} body using naive brace matching.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        The substring inside the OUTERMOST WHERE braces, or None if not found.\n",
    "\n",
    "    Caveats\n",
    "    -------\n",
    "    - This is a pragmatic extractor for typical SELECT queries.\n",
    "    - It will not handle very exotic formatting or multiple top-level groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    stack, start, end = 0, None, None\n",
    "    for i,ch in enumerate(q):\n",
    "        if ch == \"{\":\n",
    "            if stack==0 and start is None: start = i\n",
    "            stack += 1\n",
    "        elif ch == \"}\":\n",
    "            stack -= 1\n",
    "            if stack==0: end = i\n",
    "    return q[start+1:end] if start is not None and end is not None else None\n",
    "\n",
    "def _extract_prefix_block(q: str) -> str:\n",
    "    \"\"\"\n",
    "    Capture PREFIX and BASE declarations from the query (anywhere).\n",
    "    Returns them joined with newlines so they can be reused in derived queries (e.g., ASK).\n",
    "    \"\"\"\n",
    "    # Grab full lines starting with PREFIX/BASE (case-insensitive)\n",
    "    lines = re.findall(r\"(?im)^\\s*(?:PREFIX|BASE)\\s+[^\\r\\n]+\", q)\n",
    "    return \"\\n\".join(lines) + (\"\\n\" if lines else \"\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Syntax & lint metrics\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def syntax_valid(query: str) -> Tuple[bool,str]:\n",
    "    \"\"\"\n",
    "    Quick syntax sanity check (lightweight).\n",
    "\n",
    "    WHAT\n",
    "    ----\n",
    "    - Checks for a non-empty query, balanced braces, and presence of a SPARQL form keyword.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    (ok, message) : (bool, str)\n",
    "        ok       : True if the basic checks pass; False otherwise.\n",
    "        message  : \"OK\" on success or a human-readable reason on failure.\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - True,\"OK\"     → passes minimal structural sanity checks.\n",
    "    - False,reason  → likely a malformed or incomplete query string.\n",
    "\n",
    "    NOTE\n",
    "    ----\n",
    "    - For strict SPARQL 1.1 parsing, use rdflib/jena parsers. This function\n",
    "      aims to be fast and dependency-light.\n",
    "    \"\"\"\n",
    "    if not query.strip(): return False,\"Empty query\"\n",
    "    if query.count(\"{\") != query.count(\"}\"): return False,\"Unbalanced braces\"\n",
    "    if not re.search(r\"\\b(SELECT|ASK|CONSTRUCT|DESCRIBE)\\b\", query, re.I):\n",
    "        return False,\"No SPARQL form found\"\n",
    "    return True,\"OK\"\n",
    "\n",
    "\n",
    "# Common lints that catch portability and determinism pitfalls.\n",
    "ANTI_PATTERNS = [\n",
    "    # Pagination should be ordered; OFFSET without ORDER BY can reorder nondeterministically.\n",
    "    (r\"\\bOFFSET\\s+\\d+\\b(?![\\s\\S]*\\bORDER\\s+BY\\b)\", \"OFFSET without ORDER BY\"),\n",
    "    # Explicit SELECT list improves readability and prevents accidental wide projections.\n",
    "    (r\"\\bSELECT\\s+\\*\\b\", \"SELECT * (prefer explicit projection)\"),\n",
    "    # FILTER(!BOUND(?x)) is often misused to simulate NOT EXISTS; can make OPTIONAL effectively mandatory.\n",
    "    (r\"FILTER\\s*\\(\\s*!?\\s*BOUND\\s*\\(\\?\\w+\\)\\s*\\)\", \"FILTER(!BOUND) at tail\"),\n",
    "    # Vendor-specific functions harm portability across triple stores.\n",
    "    (r\"\\b(bif:|sql:|pragma:)\\w+\", \"Vendor-specific function detected\"),\n",
    "]\n",
    "\n",
    "def lint_query(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lint for common anti-patterns and unused prefixes.\n",
    "\n",
    "    WHAT\n",
    "    ----\n",
    "    - Scans the query text for patterns that often cause portability/logic issues.\n",
    "    - Flags unused PREFIX declarations.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    issues : List[str]\n",
    "        Human-readable warnings/errors. Empty list means \"no issues found\".\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - Longer lists indicate more potential maintainability/portability problems.\n",
    "    - Not all lints are fatal; treat them as prompts to review the query.\n",
    "    \"\"\"\n",
    "    issues=[]\n",
    "    for pat,msg in ANTI_PATTERNS:\n",
    "        if re.search(pat,query,re.I|re.M): issues.append(msg)\n",
    "    declared = set(re.findall(r\"PREFIX\\s+(\\w+):\", query, re.I))\n",
    "    used = set(re.findall(r\"(\\w+):\\w+\", query))\n",
    "    for p in declared - used:\n",
    "        issues.append(f\"Unused PREFIX: {p}:\")\n",
    "    return issues\n",
    "\n",
    "\n",
    "def complexity_stats(query: str) -> Dict[str,int]:\n",
    "    \"\"\"\n",
    "    Very rough structural complexity metrics.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    dict with:\n",
    "      - triple_patterns : int    # heuristic count of triple-like patterns\n",
    "      - optional_blocks : int    # number of OPTIONAL occurrences\n",
    "      - union_blocks    : int    # number of UNION occurrences\n",
    "      - subqueries      : int    # number of nested SELECTs (approx)\n",
    "      - property_paths  : int    # occurrences of /, //, +, * in paths (approx)\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - Higher numbers generally indicate greater cognitive/computational complexity.\n",
    "    - Useful as maintainability proxies and for stratifying queries by difficulty.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"triple_patterns\": len(re.findall(r\"\\w+\\s+\\w+[:\\w/<>=\\-\\.\\?]+\", query)),\n",
    "        \"optional_blocks\": len(re.findall(r\"\\bOPTIONAL\\b\", query, re.I)),\n",
    "        \"union_blocks\": len(re.findall(r\"\\bUNION\\b\", query, re.I)),\n",
    "        \"subqueries\": max(0, len(re.findall(r\"\\bSELECT\\b\", query, re.I))-1),\n",
    "        \"property_paths\": len(re.findall(r\"[/?][+*]{1,2}\", query)),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Logic checks (reference-free)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def satisfiable(endpoint: str, query: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether the WHERE body can match at least one solution on the dataset.\n",
    "\n",
    "    WHAT\n",
    "    ----\n",
    "    - Converts the main WHERE block to an ASK query: if ASK returns True,\n",
    "      the query is \"satisfiable\" on the current data (not necessarily that\n",
    "      the SELECT will return rows after projection/filters).\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    bool\n",
    "        True  → At least one solution exists for the core pattern.\n",
    "        False → The core pattern/filters likely make it impossible on current data.\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - False can indicate over-constraining patterns, bad joins, wrong IRIs, or a too-restrictive FILTER.\n",
    "    - True does NOT guarantee the final SELECT returns non-empty (e.g., projection might drop all vars).\n",
    "    \"\"\"\n",
    "    \n",
    "    body = _extract_where(query)\n",
    "    if not body: return False\n",
    "    prefixes = _extract_prefix_block(query)\n",
    "    ask = f\"{prefixes}ASK WHERE {{ {body} }}\"\n",
    "    _, raw = _run(endpoint, ask)\n",
    "    return raw.get(\"boolean\", False)\n",
    "\n",
    "\n",
    "def deterministic(endpoint: str, query: str, runs:int=3) -> bool:\n",
    "    \"\"\"\n",
    "    Re-run the same query multiple times and check exact result-set stability.\n",
    "\n",
    "    WHAT\n",
    "    ----\n",
    "    - Compares order-insensitive checksums of the entire result set across runs.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    bool\n",
    "        True  → Identical results across runs (good sign of determinism).\n",
    "        False → Results differ (could be due to missing ORDER BY with OFFSET,\n",
    "                nondeterministic engine behavior, or non-stable data).\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - Prefer True for production queries users will page through or cache.\n",
    "    - If False, review ORDER BY / pagination strategy and data volatility.\n",
    "    \"\"\"\n",
    "    checksums=[_result_checksum(_run(endpoint,query)[0]) for _ in range(runs)]\n",
    "    return len(set(checksums))==1\n",
    "\n",
    "\n",
    "def mutation_sensitivity(endpoint: str, query: str) -> float:\n",
    "    \"\"\"\n",
    "    Apply small syntactic/semantic mutations and see if results change.\n",
    "\n",
    "    WHAT\n",
    "    ----\n",
    "    - Mutations simulate common logic errors (remove DISTINCT, flip = to !=, strip a FILTER).\n",
    "    - We count how many mutations change the result vs the original.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    ratio : float in [0,1]\n",
    "        = changed / tested\n",
    "        - 1.0 → Every mutation altered behavior (your query/tests are discriminative).\n",
    "        - 0.0 → Mutations had no effect (the query may be too weak or mutations irrelevant).\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - Higher ratio is generally better: the query is \"meaningful\" and not trivially insensitive.\n",
    "    - Low ratio can indicate that important constraints are missing (e.g., DISTINCT not needed,\n",
    "      FILTERs unused, or joins not selective).\n",
    "    \"\"\"\n",
    "    mutations=[\n",
    "        # Remove DISTINCT → should often increase duplicates (change results).\n",
    "        lambda q: re.sub(r\"\\bDISTINCT\\b\",\"\",q,flags=re.I),\n",
    "        # Flip first equality to inequality → should change rows if the condition is effective.\n",
    "        lambda q: re.sub(r\"=\\s*([^\\s)]+)\",r\"!= \\1\",q,1),\n",
    "        # Remove first FILTER → results may broaden if filter was effective.\n",
    "        lambda q: re.sub(r\"\\bFILTER\\s*\\([^()]+\\)\",\"\",q,1,flags=re.I),\n",
    "    ]\n",
    "    base_cs=_result_checksum(_run(endpoint,query)[0])\n",
    "    tested,changed=0,0\n",
    "    for m in mutations:\n",
    "        mq=m(query)\n",
    "        if mq!=query:\n",
    "            tested+=1\n",
    "            try:\n",
    "                if _result_checksum(_run(endpoint,mq)[0])!=base_cs:\n",
    "                    changed+=1\n",
    "            except Exception:\n",
    "                # If the mutation becomes invalid/throws, we treat as changed behavior.\n",
    "                changed+=1\n",
    "    return changed/tested if tested else 0.0\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Performance metrics\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def latency(endpoint: str, query: str, repeats:int=5) -> Dict[str,float]:\n",
    "    \"\"\"\n",
    "    Measure wall-clock execution time across multiple runs.\n",
    "\n",
    "    WHAT\n",
    "    ----\n",
    "    - Executes the query `repeats` times and aggregates basic latency percentiles.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    dict with (milliseconds):\n",
    "      - p50_ms : float  # median latency (typical)\n",
    "      - p95_ms : float  # tail latency (slow outliers)\n",
    "      - p99_ms : float  # extreme tail\n",
    "      - mean_ms: float  # average latency\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - p50_ms ≈ everyday experience; p95_ms is critical for SLOs.\n",
    "    - Compare across queries/datasets or cold vs warm cache scenarios.\n",
    "    \"\"\"\n",
    "    times=[]\n",
    "    for _ in range(repeats):\n",
    "        t0=time.time(); _run(endpoint,query); times.append((time.time()-t0)*1000)\n",
    "    times.sort()\n",
    "    pct=lambda p: times[min(len(times)-1, math.ceil(p*len(times))-1)]\n",
    "    return {\n",
    "        \"p50_ms\": pct(0.5),\n",
    "        \"p95_ms\": pct(0.95),\n",
    "        \"p99_ms\": pct(0.99),\n",
    "        \"mean_ms\": statistics.mean(times)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Maintainability proxies\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def variable_hygiene(query: str) -> Dict[str,object]:\n",
    "    \"\"\"\n",
    "    Simple variable usage hygiene statistics.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    dict with:\n",
    "      - all_vars      : List[str]  # all variable names found (e.g., ['s','x'])\n",
    "      - unused_vars   : List[str]  # variables that appear outside WHERE or not truly used\n",
    "      - name_entropy  : float      # avg(#unique characters) across variable names\n",
    "\n",
    "    HOW TO INTERPRET\n",
    "    ----------------\n",
    "    - Many `unused_vars` can indicate dead code or accidental cross-joins.\n",
    "    - Higher `name_entropy` suggests more informative names (very rough proxy).\n",
    "    - These are heuristics; use alongside lints/complexity for maintainability.\n",
    "    \"\"\"\n",
    "    vars_all=set(re.findall(r\"\\?([A-Za-z_]\\w+)\",query))\n",
    "    body=_extract_where(query) or \"\"\n",
    "    where_use=set(re.findall(r\"\\?([A-Za-z_]\\w+)\",body))\n",
    "    return {\n",
    "        \"all_vars\":sorted(vars_all),\n",
    "        \"unused_vars\":sorted(vars_all-where_use),\n",
    "        \"name_entropy\":statistics.mean([len(set(v)) for v in vars_all]) if vars_all else 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83c87b8a-a390-4296-ad50-1a469609d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# CLI demo: run all metrics on a query and return a dict\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def run_eval(endpoint: str, query: str):\n",
    "    \"\"\"\n",
    "    Convenience wrapper that runs all metrics and prints/returns the results.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    dict\n",
    "      {\n",
    "        \"syntax_valid\":          (bool,str),      # minimal syntax sanity\n",
    "        \"lint_query\":            List[str],       # list of warnings/errors (empty = clean)\n",
    "        \"complexity_stats\":      Dict[str,int],   # structural counts\n",
    "        \"satisfiable\":           bool,            # ASK-converted satisfiability\n",
    "        \"deterministic\":         bool,            # stable results across runs\n",
    "        \"mutation_sensitivity\":  float,           # [0,1] higher = more discriminative\n",
    "        \"latency\":               Dict[str,float], # p50/p95/p99/mean in ms\n",
    "        \"variable_hygiene\":      Dict[str,object] # var usage & name entropy\n",
    "      }\n",
    "\n",
    "    HOW TO INTERPRET (quick guide)\n",
    "    ------------------------------\n",
    "    - syntax_valid: True/\"OK\" is expected. False indicates a malformed query.\n",
    "    - lint_query:   Aim for []. Address findings to improve portability/clarity.\n",
    "    - complexity_stats: Higher counts → more complex; compare relative to peers.\n",
    "    - satisfiable:  False often means over-constrained or wrong graph patterns.\n",
    "    - deterministic: True is preferred for predictable pagination/caching.\n",
    "    - mutation_sensitivity: Aim higher (e.g., ≥0.5) to avoid \"toothless\" queries.\n",
    "    - latency:      Watch p95_ms against your SLOs.\n",
    "    - variable_hygiene: Fewer `unused_vars` is better; moderate/high `name_entropy` is nice.\n",
    "    \"\"\"\n",
    "    print(\"Syntax:\", syntax_valid(query))\n",
    "    print(\"Lint:\", lint_query(query))\n",
    "    print(\"Complexity:\", complexity_stats(query))\n",
    "    print(\"Satisfiable:\", satisfiable(endpoint, query))\n",
    "    print(\"Deterministic:\", deterministic(endpoint, query))\n",
    "    print(\"Mutation sensitivity:\", mutation_sensitivity(endpoint, query))\n",
    "    print(\"Latency:\", latency(endpoint, query))\n",
    "    print(\"Variable hygiene:\", variable_hygiene(query))\n",
    "    return {\n",
    "        \"syntax_valid\": syntax_valid(query),\n",
    "        \"lint_query\": lint_query(query),\n",
    "        \"complexity_stats\": complexity_stats(query),\n",
    "        \"satisfiable\": satisfiable(endpoint, query),\n",
    "        \"deterministic\": deterministic(endpoint, query),\n",
    "        \"mutation_sensitivity\": mutation_sensitivity(endpoint, query),\n",
    "        \"latency\": latency(endpoint, query),\n",
    "        \"variable_hygiene\": variable_hygiene(query)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7568303-fec1-41ae-a8ac-5ca2bbdbf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_POINT = \"localhost:3030/currkg/query\"\n",
    "QUERY = \"\"\"\n",
    "PREFIX edu-ont: <https://edugate.cs.wright.edu/lod/ontology/>\n",
    "\n",
    "SELECT ?module ?moduleTitle ?level ?levelName\n",
    "WHERE {\n",
    "    ?module a edu-ont:Module ;\n",
    "            edu-ont:hasTitle ?moduleTitle ;\n",
    "            edu-ont:hasLevel ?level .\n",
    "    \n",
    "    ?level edu-ont:asString ?levelName .\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d3e9f36-4212-4487-a918-eb3531abdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax: (True, 'OK')\n",
      "Lint: []\n",
      "Complexity: {'triple_patterns': 4, 'optional_blocks': 0, 'union_blocks': 0, 'subqueries': 0, 'property_paths': 0}\n",
      "Satisfiable: True\n",
      "Deterministic: True\n",
      "Mutation sensitivity: 0.0\n",
      "Latency: {'p50_ms': 25.217771530151367, 'p95_ms': 28.643369674682617, 'p99_ms': 28.643369674682617, 'mean_ms': 24.33462142944336}\n",
      "Variable hygiene: {'all_vars': ['level', 'levelName', 'module', 'moduleTitle'], 'unused_vars': [], 'name_entropy': 6}\n"
     ]
    }
   ],
   "source": [
    "result = run_eval(END_POINT, QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8d28edc-b795-4ab9-95be-f53b9f3b987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'syntax_valid': (True, 'OK'),\n",
       " 'lint_query': [],\n",
       " 'complexity_stats': {'triple_patterns': 4,\n",
       "  'optional_blocks': 0,\n",
       "  'union_blocks': 0,\n",
       "  'subqueries': 0,\n",
       "  'property_paths': 0},\n",
       " 'satisfiable': True,\n",
       " 'deterministic': True,\n",
       " 'mutation_sensitivity': 0.0,\n",
       " 'latency': {'p50_ms': 21.464109420776367,\n",
       "  'p95_ms': 22.48072624206543,\n",
       "  'p99_ms': 22.48072624206543,\n",
       "  'mean_ms': 21.421289443969727},\n",
       " 'variable_hygiene': {'all_vars': ['level',\n",
       "   'levelName',\n",
       "   'module',\n",
       "   'moduleTitle'],\n",
       "  'unused_vars': [],\n",
       "  'name_entropy': 6}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e2bd46d-da07-4c8c-a5d6-02fab92f4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Single-Graph CQ Evaluator — evaluate(cq_text, sparql)\n",
    "# Transformers embeddings via Sentence-Transformers (auto-downloads model)\n",
    "# No hardcoded prefixes; dynamic label discovery; semantic NL↔Query scores.\n",
    "# Depends on your existing helpers: _run, _extract_where, syntax_valid,\n",
    "# satisfiable, deterministic, latency\n",
    "# ======================================================================\n",
    "\n",
    "import os, re, math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- basics ----------------\n",
    "_RE_CAMEL = re.compile(r\"(?<!^)(?=[A-Z])\")\n",
    "_RE_NONALNUM = re.compile(r\"[^0-9a-zA-Z@^<>:/# ]+\")\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    t = _RE_CAMEL.sub(\" \", str(s)).replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    t = _RE_NONALNUM.sub(\" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
    "    return t\n",
    "\n",
    "def _dup_ratio(values: List[str]) -> float:\n",
    "    n = len(values)\n",
    "    return 0.0 if n <= 1 else max(0.0, 1.0 - (len(set(values)) / n))\n",
    "\n",
    "def _entropy(counts: Counter) -> float:\n",
    "    n = sum(counts.values())\n",
    "    if n == 0: return 0.0\n",
    "    H = 0.0\n",
    "    for c in counts.values():\n",
    "        p = c / n\n",
    "        H -= p * math.log(p + 1e-12, 2)\n",
    "    return H\n",
    "\n",
    "def _lex_overlap(a: str, b: str) -> float:\n",
    "    A = set(_norm_text(a).split())\n",
    "    B = set(_norm_text(b).split())\n",
    "    return 1.0 if not (A or B) else len(A & B) / len(A | B)\n",
    "\n",
    "def _is_uri(v: dict) -> bool:\n",
    "    return v.get(\"type\") == \"uri\"\n",
    "\n",
    "def _tail(u: str) -> str:\n",
    "    u = str(u)\n",
    "    if \"#\" in u: u = u.rsplit(\"#\", 1)[-1]\n",
    "    if \"/\" in u: u = u.rsplit(\"/\", 1)[-1]\n",
    "    return u\n",
    "\n",
    "def _project_vars(query: str) -> List[str]:\n",
    "    m = re.search(r\"(?is)SELECT\\s+(DISTINCT\\s+)?(.+?)\\s+WHERE\\s*\\{\", query)\n",
    "    if not m:\n",
    "        body = _extract_where(query) or \"\"\n",
    "        return sorted(set(re.findall(r\"\\?([A-Za-z_]\\w+)\", body)))\n",
    "    proj = m.group(2)\n",
    "    if \"*\" in proj:\n",
    "        body = _extract_where(query) or \"\"\n",
    "        return sorted(set(re.findall(r\"\\?([A-Za-z_]\\w+)\", body)))\n",
    "    return re.findall(r\"\\?([A-Za-z_]\\w+)\", proj)\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    return a @ b.T\n",
    "\n",
    "# ======== SEMANTIC NL↔QUERY MATCH (uses evaluator's embedder) ========\n",
    "\n",
    "def _localnames_from_query(query_text: str) -> List[str]:\n",
    "    locs: List[str] = []\n",
    "    # IRI tails\n",
    "    for iri in re.findall(r\"<([^>]+)>\", query_text):\n",
    "        tail = iri.rsplit(\"#\", 1)[-1].rsplit(\"/\", 1)[-1]\n",
    "        locs.append(_norm_text(tail))\n",
    "    # QNames\n",
    "    for _, local in re.findall(r\"\\b([A-Za-z_][\\w\\-]*):([A-Za-z_][\\w\\-]*)\\b\", query_text):\n",
    "        locs.append(_norm_text(local))\n",
    "    # dedupe, drop empties\n",
    "    return list(dict.fromkeys([t for t in locs if t]))\n",
    "\n",
    "def _vars_from_query(query_text: str) -> List[str]:\n",
    "    vs = [m for m in re.findall(r\"\\?([A-Za-z_]\\w+)\", query_text)]\n",
    "    return list(dict.fromkeys([_norm_text(v) for v in vs]))\n",
    "\n",
    "def _content_tokens(text: str) -> List[str]:\n",
    "    toks = [t for t in re.split(r\"\\s+\", _norm_text(text)) if t]\n",
    "    return [t for t in toks if len(t) > 2] or toks\n",
    "\n",
    "# --------------- embeddings: Sentence-Transformers (auto-download) ---------------\n",
    "class _EmbedderBase:\n",
    "    def embed(self, texts: List[str]) -> np.ndarray: raise NotImplementedError\n",
    "\n",
    "class _SBERT(_EmbedderBase):\n",
    "    \"\"\"\n",
    "    Uses Sentence-Transformers. Auto-downloads the model from Hugging Face\n",
    "    the first time it’s needed (cached afterwards).\n",
    "    Set SBERT_MODEL to override the model id/path.\n",
    "      e.g., SBERT_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id: Optional[str] = None):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        # Pick model from env or a strong, lightweight default.\n",
    "        self.model_id = (model_id or os.environ.get(\"SBERT_MODEL\") \n",
    "                         or \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        # This will download automatically if not present locally.\n",
    "        self.m = SentenceTransformer(self.model_id)\n",
    "\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        if not texts: \n",
    "            # dimension will be inferred on first non-empty call; 1-col zero is OK for empties\n",
    "            return np.zeros((0, 1), dtype=np.float32)\n",
    "        vecs = self.m.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)\n",
    "        return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "def _pick_embedder() -> _EmbedderBase:\n",
    "    # Single, explicit strategy: use Sentence-Transformers and let it fetch the model.\n",
    "    # No local search heuristics; model id can be overridden via SBERT_MODEL.\n",
    "    return _SBERT()\n",
    "\n",
    "def _embed_same_space(embedder: _EmbedderBase, groups: List[List[str]]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    With SBERT (fixed-size embeddings), each group is embedded independently.\n",
    "    This ensures empty groups are returned with the correct dimensionality.\n",
    "    \"\"\"\n",
    "    outs=[]\n",
    "    max_dim=None\n",
    "    # first pass: embed non-empty groups to learn dim\n",
    "    for g in groups:\n",
    "        if g:\n",
    "            X=embedder.embed(g)\n",
    "            outs.append(X)\n",
    "            max_dim = X.shape[1] if max_dim is None else max(max_dim, X.shape[1])\n",
    "        else:\n",
    "            outs.append(None)\n",
    "    if max_dim is None:\n",
    "        # all empty\n",
    "        return [np.zeros((0,1), dtype=np.float32) for _ in groups]\n",
    "    # second pass: fill empties with zeros of learned dim\n",
    "    final=[]\n",
    "    for X in outs:\n",
    "        if X is None:\n",
    "            final.append(np.zeros((0,max_dim), dtype=np.float32))\n",
    "        elif X.shape[1]==max_dim:\n",
    "            final.append(X)\n",
    "        else:\n",
    "            # very unlikely mismatch; pad/truncate to max_dim defensively\n",
    "            pad=np.zeros((X.shape[0], max_dim), dtype=np.float32)\n",
    "            pad[:, :min(X.shape[1], max_dim)] = X[:, :min(X.shape[1], max_dim)]\n",
    "            final.append(pad)\n",
    "    return final\n",
    "\n",
    "def nl_query_semantic_scores(cq_text: str, sparql_query: str, embedder: _EmbedderBase) -> dict:\n",
    "    \"\"\"\n",
    "    Two semantic scores in [0,1], using a shared embedding space:\n",
    "      - semantic_similarity_to_CQ: CQ sentence vs mean of query terms\n",
    "      - semantic_soft_coverage_to_CQ: avg over CQ tokens of max sim to any query term\n",
    "    \"\"\"\n",
    "    where_body = _extract_where(sparql_query) or \"\"\n",
    "    query_terms = (\n",
    "        _vars_from_query(sparql_query)\n",
    "        + _localnames_from_query(sparql_query)\n",
    "        + _content_tokens(where_body)\n",
    "    )\n",
    "    query_terms = list(dict.fromkeys([t for t in query_terms if t]))\n",
    "    cq_toks = _content_tokens(cq_text)\n",
    "\n",
    "    if not query_terms:\n",
    "        return {\"semantic_similarity_to_CQ\": 0.0, \"semantic_soft_coverage_to_CQ\": 0.0}\n",
    "\n",
    "    E_terms, E_cq, E_tok = _embed_same_space(embedder, [query_terms, [cq_text], cq_toks])\n",
    "\n",
    "    if E_terms.shape[0] == 0:\n",
    "        return {\"semantic_similarity_to_CQ\": 0.0, \"semantic_soft_coverage_to_CQ\": 0.0}\n",
    "\n",
    "    # centroid of query terms\n",
    "    E_terms_mean = E_terms.mean(axis=0, keepdims=True)\n",
    "    E_terms_mean /= (np.linalg.norm(E_terms_mean, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    # whole-sentence similarity\n",
    "    if E_cq.shape[0] == 0:\n",
    "        sim01 = 0.0\n",
    "    else:\n",
    "        sim = float(np.clip((E_cq @ E_terms_mean.T).squeeze(), -1.0, 1.0))\n",
    "        sim01 = 0.5 * (sim + 1.0)\n",
    "\n",
    "    # token soft coverage\n",
    "    if E_tok.shape[0] == 0:\n",
    "        cov01 = sim01\n",
    "    else:\n",
    "        S = E_tok @ E_terms.T\n",
    "        best = S.max(axis=1)\n",
    "        cov01 = float(np.clip(np.mean(0.5 * (best + 1.0)), 0.0, 1.0))\n",
    "\n",
    "    return {\n",
    "        \"semantic_similarity_to_CQ\": round(sim01,2),\n",
    "        \"semantic_soft_coverage_to_CQ\": round(cov01,2)\n",
    "    }\n",
    "\n",
    "def _set_cohesion(texts: List[str], embedder: _EmbedderBase) -> float:\n",
    "    uniq = list({ _norm_text(t) for t in texts if str(t).strip() })\n",
    "    if len(uniq) <= 1: return 1.0\n",
    "    emb = embedder.embed(uniq)\n",
    "    if emb.shape[0] <= 1: return 1.0\n",
    "    S = _cosine(emb, emb)\n",
    "    np.fill_diagonal(S, -1.0)\n",
    "    return float(S.max(axis=1).mean())\n",
    "\n",
    "# --------- dynamic label discovery (schema-agnostic) ---------\n",
    "def _labels_for_uris(endpoint: str, uris: List[str], headers: Optional[Dict[str,str]]=None) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    Discover a label-like literal per URI with no prefix/property assumptions.\n",
    "    \"\"\"\n",
    "    uri2label: Dict[str,str] = {}\n",
    "    if not uris: return uri2label\n",
    "\n",
    "    # chunk VALUES size safely (budget proportional to average URI length)\n",
    "    avg_len = max(16, int(sum(len(u) for u in uris) / max(1, len(uris))))\n",
    "    budget = avg_len * 200\n",
    "    chunk, size = [], 0\n",
    "    chunks: List[List[str]] = []\n",
    "    for u in sorted(set(uris)):\n",
    "        tok = f\"<{u}>\"\n",
    "        if size + len(tok) + 1 > budget and chunk:\n",
    "            chunks.append(chunk); chunk = [u]; size = len(tok) + 1\n",
    "        else:\n",
    "            chunk.append(u); size += len(tok) + 1\n",
    "    if chunk: chunks.append(chunk)\n",
    "\n",
    "    predicate_counts: Counter = Counter()\n",
    "    per_subj: Dict[str, List[Tuple[str,str,dict]]] = defaultdict(list)\n",
    "\n",
    "    for group in chunks:\n",
    "        values = \" \".join(f\"<{u}>\" for u in group)\n",
    "        q = f\"\"\"\n",
    "SELECT ?s ?p ?o ?lang ?dt WHERE {{\n",
    "  VALUES ?s {{ {values} }}\n",
    "  ?s ?p ?o .\n",
    "  FILTER(isLiteral(?o))\n",
    "  BIND(LANG(?o) AS ?lang)\n",
    "  BIND(DATATYPE(?o) AS ?dt)\n",
    "}}\"\"\"\n",
    "        try:\n",
    "            rows, _ = _run(endpoint, q, headers=headers)\n",
    "        except Exception:\n",
    "            rows = []\n",
    "        for r in rows:\n",
    "            s = r[\"s\"][\"value\"]; p = r[\"p\"][\"value\"]; o = r[\"o\"][\"value\"]\n",
    "            lang = r.get(\"lang\", {}).get(\"value\", \"\")\n",
    "            dt = r.get(\"dt\", {}).get(\"value\", \"\")\n",
    "            predicate_counts[p] += 1\n",
    "            per_subj[s].append((p, o, {\"lang\": lang, \"dt\": dt}))\n",
    "\n",
    "    if not predicate_counts:\n",
    "        return uri2label\n",
    "\n",
    "    maxc = max(predicate_counts.values())\n",
    "    pred_score = {p: c/maxc for p,c in predicate_counts.items()}\n",
    "    all_lengths = [len(o) for lst in per_subj.values() for _,o,_ in lst]\n",
    "    median_len = float(np.median(all_lengths)) if all_lengths else 20.0\n",
    "\n",
    "    def lit_score(p: str, o: str, meta: dict) -> float:\n",
    "        f = pred_score.get(p, 0.0)\n",
    "        L = 1.0 if meta.get(\"lang\") else 0.0\n",
    "        ell = len(o)\n",
    "        closeness = 1.0 - (abs(ell - median_len) / (median_len + 1e-6))\n",
    "        closeness = float(np.clip(closeness, 0.0, 1.0))\n",
    "        punct_pen = 1.0 - (len(re.findall(r\"[^0-9A-Za-z\\s]\", o)) / max(1.0, ell))\n",
    "        return f * max(0.0, L*0.5 + 0.5) * ((closeness + punct_pen) / 2.0)\n",
    "\n",
    "    for s, cand in per_subj.items():\n",
    "        if not cand: continue\n",
    "        best = max(cand, key=lambda t: lit_score(*t))\n",
    "        uri2label[s] = best[1]\n",
    "    return uri2label\n",
    "\n",
    "# ---------------- two-arg public API ----------------\n",
    "class SingleGraphCQEvaluator:\n",
    "    \"\"\"\n",
    "    Construct with endpoint (and optional headers). Then call:\n",
    "        evaluate(cq_text, sparql_query)\n",
    "    - Embeddings: Sentence-Transformers only (auto-downloads; override with SBERT_MODEL).\n",
    "    - Labels: discovered dynamically; no prefixes/properties hardcoded.\n",
    "    \"\"\"\n",
    "    def __init__(self, endpoint: str, headers: Optional[Dict[str,str]]=None, model_id: Optional[str]=None):\n",
    "        self.endpoint = endpoint\n",
    "        self.headers = headers or {}\n",
    "        # If you want to pin a specific model in code, pass model_id.\n",
    "        # Otherwise SBERT_MODEL env var or default 'sentence-transformers/all-MiniLM-L6-v2' is used.\n",
    "        self.embedder = _SBERT(model_id=model_id)\n",
    "\n",
    "    def evaluate(self, cq_text: str, sparql: str) -> Dict[str, Any]:\n",
    "        ok, msg = syntax_valid(sparql)\n",
    "        if not ok:\n",
    "            return {\"cq_text\": cq_text, \"syntax_ok\": False, \"syntax_msg\": msg}\n",
    "\n",
    "        where_body = _extract_where(sparql) or \"\"\n",
    "        sat = satisfiable(self.endpoint, sparql)\n",
    "        det = deterministic(self.endpoint, sparql, runs=2)\n",
    "        lat = latency(self.endpoint, sparql, repeats=3)\n",
    "\n",
    "        bindings, _ = _run(self.endpoint, sparql, headers=self.headers)\n",
    "        vars_ = _project_vars(sparql) or sorted(set(re.findall(r\"\\?([A-Za-z_]\\w+)\", where_body)))\n",
    "\n",
    "        per_var_vals: Dict[str, List[dict]] = {v: [] for v in vars_}\n",
    "        uris: List[str] = []\n",
    "        for r in bindings:\n",
    "            for v in vars_:\n",
    "                if v in r:\n",
    "                    per_var_vals[v].append(r[v])\n",
    "                    if _is_uri(r[v]): uris.append(r[v][\"value\"])\n",
    "\n",
    "        uri2label = _labels_for_uris(self.endpoint, uris, headers=self.headers)\n",
    "\n",
    "        per_var: List[Dict[str, Any]] = []\n",
    "        always_unbound: List[str] = []\n",
    "\n",
    "        for v in vars_:\n",
    "            vals = per_var_vals[v]\n",
    "            rows_n = len(bindings)\n",
    "            bound = len(vals)\n",
    "            br = (bound / rows_n) if rows_n else 0.0\n",
    "            if br == 0.0:\n",
    "                always_unbound.append(v)\n",
    "                per_var.append({\n",
    "                    \"var\": v, \"rows\": rows_n, \"distinct\": 0,\n",
    "                    \"null_rate\": 1.0, \"label_resolution_rate\": 0.0,\n",
    "                    \"diversity_entropy_norm\": 0.0, \"semantic_cohesion\": 1.0,\n",
    "                    \"dup_ratio\": 0.0, \"bound_rate\": 0.0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # surface strings (dynamic labels)\n",
    "            texts: List[str] = []\n",
    "            resolved = 0\n",
    "            for b in vals:\n",
    "                raw = b[\"value\"]\n",
    "                if _is_uri(b):\n",
    "                    lab = uri2label.get(raw)\n",
    "                    if lab: texts.append(lab); resolved += 1\n",
    "                    else:   texts.append(_tail(raw))\n",
    "                else:\n",
    "                    s = raw\n",
    "                    if \"xml:lang\" in b: s += f\"@{b['xml:lang']}\"\n",
    "                    if \"datatype\" in b: s += f\"^^<{b['datatype']}>\"\n",
    "                    texts.append(s)\n",
    "\n",
    "            norm_texts = [_norm_text(t) for t in texts]\n",
    "            counts = Counter(norm_texts)\n",
    "            H = _entropy(counts)\n",
    "            H_norm = H / math.log(len(counts) + 1e-12, 2) if counts else 0.0\n",
    "            coh = _set_cohesion(texts, self.embedder)\n",
    "            dup = _dup_ratio(norm_texts)\n",
    "            denom_uri = sum(1 for b in vals if _is_uri(b))\n",
    "            label_rate = (resolved / denom_uri) if denom_uri else 1.0\n",
    "            nulls = rows_n - bound\n",
    "\n",
    "            per_var.append({\n",
    "                \"var\": v,\n",
    "                \"rows\": rows_n,\n",
    "                \"distinct\": len(counts),\n",
    "                \"null_rate\": (nulls / rows_n) if rows_n else 0.0,\n",
    "                \"label_resolution_rate\": label_rate,\n",
    "                \"diversity_entropy_norm\": round(H_norm, 2),\n",
    "                \"semantic_cohesion\": round(coh, 2),\n",
    "                \"dup_ratio\": round(dup, 2),\n",
    "                \"bound_rate\": br\n",
    "            })\n",
    "\n",
    "        # Tuple cohesion (uses same embedder)\n",
    "        tuple_coh: Optional[float] = None\n",
    "        if bindings and vars_:\n",
    "            tuples = []\n",
    "            for r in bindings:\n",
    "                parts = []\n",
    "                for v in vars_:\n",
    "                    if v in r:\n",
    "                        raw = r[v][\"value\"]\n",
    "                        if _is_uri(r[v]):\n",
    "                            parts.append(_norm_text(uri2label.get(raw, _tail(raw))))\n",
    "                        else:\n",
    "                            s = raw\n",
    "                            if \"xml:lang\" in r[v]: s += f\"@{r[v]['xml:lang']}\"\n",
    "                            if \"datatype\" in r[v]: s += f\"^^<{r[v]['datatype']}>\"\n",
    "                            parts.append(_norm_text(s))\n",
    "                    else:\n",
    "                        parts.append(\"\")\n",
    "                tuples.append(\" | \".join(parts))\n",
    "            uniq = list(dict.fromkeys(tuples))\n",
    "            emb = self.embedder.embed(uniq)\n",
    "            if emb.shape[0] <= 1:\n",
    "                tuple_coh = 1.0\n",
    "            else:\n",
    "                S = _cosine(emb, emb)\n",
    "                np.fill_diagonal(S, -1.0)\n",
    "                tuple_coh = float(S.max(axis=1).mean())\n",
    "\n",
    "        # Semantic NL↔Query alignment (embeddings, not string match)\n",
    "        sem = nl_query_semantic_scores(cq_text, sparql, self.embedder)\n",
    "\n",
    "        return {\n",
    "            \"cq_text\": cq_text,\n",
    "            \"syntax_ok\": True,\n",
    "            \"satisfiable\": sat,\n",
    "            \"deterministic\": det,\n",
    "            \"latency\": {\"p50_ms\": round(lat[\"p50_ms\"],2), \"p95_ms\": round(lat[\"p95_ms\"],2), \"mean_ms\": round(lat[\"mean_ms\"],2)},\n",
    "            \"rows\": len(bindings),\n",
    "            \"vars\": len(vars_),\n",
    "            \"always_unbound_vars\": always_unbound,\n",
    "            \"lexical_query_overlap\": round(_lex_overlap(cq_text, where_body),2),  # legacy lexical metric (optional)\n",
    "            **sem,  # adds semantic_similarity_to_CQ & semantic_soft_coverage_to_CQ\n",
    "            \"tuple_cohesion\": round(tuple_coh),\n",
    "            \"variables\": per_var,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f9cccaa-09a5-479e-a8b7-d1949b5b1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = SingleGraphCQEvaluator(endpoint=END_POINT)\n",
    "\n",
    "# (cq_text, sparql_query)\n",
    "res = evaluator.evaluate(\n",
    "    \"What are all the available modules and their levels\",\n",
    "    QUERY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a2e681e-1fb6-415a-887b-e9077e97ebd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cq_text': 'What are all the available modules and their levels',\n",
       " 'syntax_ok': True,\n",
       " 'satisfiable': True,\n",
       " 'deterministic': True,\n",
       " 'latency': {'p50_ms': 20.14, 'p95_ms': 26.99, 'mean_ms': 22.22},\n",
       " 'rows': 45,\n",
       " 'vars': 4,\n",
       " 'always_unbound_vars': [],\n",
       " 'lexical_query_overlap': 0.0,\n",
       " 'semantic_similarity_to_CQ': 0.67,\n",
       " 'semantic_soft_coverage_to_CQ': 0.74,\n",
       " 'tuple_cohesion': 0.6227847933769226,\n",
       " 'variables': [{'var': 'module',\n",
       "   'rows': 45,\n",
       "   'distinct': 45,\n",
       "   'null_rate': 0.0,\n",
       "   'label_resolution_rate': 1.0,\n",
       "   'diversity_entropy_norm': 1.0,\n",
       "   'semantic_cohesion': 0.56,\n",
       "   'dup_ratio': 0.0,\n",
       "   'bound_rate': 1.0},\n",
       "  {'var': 'moduleTitle',\n",
       "   'rows': 45,\n",
       "   'distinct': 45,\n",
       "   'null_rate': 0.0,\n",
       "   'label_resolution_rate': 1.0,\n",
       "   'diversity_entropy_norm': 1.0,\n",
       "   'semantic_cohesion': 0.56,\n",
       "   'dup_ratio': 0.0,\n",
       "   'bound_rate': 1.0},\n",
       "  {'var': 'level',\n",
       "   'rows': 45,\n",
       "   'distinct': 3,\n",
       "   'null_rate': 0.0,\n",
       "   'label_resolution_rate': 1.0,\n",
       "   'diversity_entropy_norm': 0.81,\n",
       "   'semantic_cohesion': 0.5,\n",
       "   'dup_ratio': 0.93,\n",
       "   'bound_rate': 1.0},\n",
       "  {'var': 'levelName',\n",
       "   'rows': 45,\n",
       "   'distinct': 3,\n",
       "   'null_rate': 0.0,\n",
       "   'label_resolution_rate': 1.0,\n",
       "   'diversity_entropy_norm': 0.81,\n",
       "   'semantic_cohesion': 0.5,\n",
       "   'dup_ratio': 0.93,\n",
       "   'bound_rate': 1.0}]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9b4dc-c360-42c0-a3b7-6fb66008e986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178402af-faab-4c29-a6e1-b5a776190cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
