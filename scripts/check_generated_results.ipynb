{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb236dd-509d-4a05-95c5-07649a1593fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from sparql_eval_module import SingleGraphCQEvaluator\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfe22d-d195-4ee3-ad63-9189f7eb3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Initialize Nones\n",
    "# ============================================================\n",
    "PREFIX_TTL_FILE = None\n",
    "END_POINT = None\n",
    "evaluator = None\n",
    "graph_id = None\n",
    "input_file = None\n",
    "\n",
    "END_POINT_DICT = {\n",
    "    \"Small\": \"localhost:3030/temp_chris_thesis1/sparql\",\n",
    "    \"Big\":  \"localhost:3030/temp_chris_thesis2/sparql\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bdb6a2-7e61-407b-ab7d-7775e33c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper: extract SPARQL from LLM analysis text\n",
    "#    - Handles ```sparql ...``` blocks\n",
    "#    - Falls back to first query-like substring\n",
    "# ============================================================\n",
    "\n",
    "cache_prefixes = dict()\n",
    "\n",
    "def load_prefixes_from_ttl(ttl_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load prefixes from a TTL file with lines like:\n",
    "      @prefix ex: <http://example.com/> .\n",
    "    Returns: { 'ex': 'http://example.com/' , ... }\n",
    "    \"\"\"\n",
    "    if cache_prefixes.get(ttl_path,0):\n",
    "        return cache_prefixes[ttl_path]\n",
    "    prefixes = {}\n",
    "    try:\n",
    "        with open(ttl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                # match lines: @prefix foo: <URI> .\n",
    "                m = re.match(\n",
    "                    r\"\\s*@prefix\\s+([A-Za-z][A-Za-z0-9_-]*):\\s*<([^>]+)>\\s*\\.\",\n",
    "                    line\n",
    "                )\n",
    "                if m:\n",
    "                    pfx, uri = m.group(1), m.group(2)\n",
    "                    prefixes[pfx] = uri\n",
    "    except FileNotFoundError:\n",
    "        print(f\"TTL prefix file not found: {ttl_path} (continuing without extra prefixes)\")\n",
    "    cache_prefixes[ttl_path] = prefixes\n",
    "    return prefixes\n",
    "\n",
    "def extract_sparql(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract SPARQL query from LLM output:\n",
    "      - remove <think>...</think> blocks entirely\n",
    "      - unescape \\\\n, \\\\t, \\\\r\n",
    "      - prefer fenced ```sparql ...``` or ``` ...``` blocks\n",
    "      - otherwise take earliest of PREFIX/SELECT/ASK/CONSTRUCT/DESCRIBE\n",
    "      - preserve PREFIX declarations\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. Remove <think>...</think> blocks (case-insensitive)\n",
    "    # -----------------------------------------------------------\n",
    "    text = re.sub(\n",
    "        r\"<think>[\\s\\S]*?</think>\",\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    ).strip()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. Unescape literal \\n, \\t, \\r if present\n",
    "    # -----------------------------------------------------------\n",
    "    if \"\\\\n\" in text or \"\\\\t\" in text or \"\\\\r\" in text:\n",
    "        text = (\n",
    "            text\n",
    "            .replace(\"\\\\r\", \"\\r\")\n",
    "            .replace(\"\\\\n\", \"\\n\")\n",
    "            .replace(\"\\\\t\", \"\\t\")\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3. Fenced code blocks: ```sparql ...``` or ``` ...```\n",
    "    # -----------------------------------------------------------\n",
    "    fenced = re.findall(\n",
    "        r\"```(?:sparql)?\\s*(.*?)```\",\n",
    "        text,\n",
    "        flags=re.DOTALL | re.IGNORECASE,\n",
    "    )\n",
    "    if fenced:\n",
    "        return fenced[0].strip()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. Prefix-aware extraction:\n",
    "    #    earliest of PREFIX/SELECT/ASK/CONSTRUCT/DESCRIBE\n",
    "    # -----------------------------------------------------------\n",
    "    lowered = text.lower()\n",
    "    keywords = [\"prefix\", \"select\", \"ask\", \"construct\", \"describe\"]\n",
    "\n",
    "    start_idx = None\n",
    "    for kw in keywords:\n",
    "        i = lowered.find(kw)\n",
    "        if i != -1:\n",
    "            if start_idx is None or i < start_idx:\n",
    "                start_idx = i\n",
    "\n",
    "    if start_idx is not None:\n",
    "        return text[start_idx:].strip()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5. Fallback: trimmed text\n",
    "    # -----------------------------------------------------------\n",
    "    return text.strip()\n",
    "\n",
    "def add_missing_prefixes(query: str, ttl_prefixes: dict) -> str:\n",
    "    \"\"\"\n",
    "    Add PREFIX declarations from ttl_prefixes that are not already\n",
    "    declared in the query. Do NOT override anything already present.\n",
    "\n",
    "    ttl_prefixes: dict { 'ex': 'http://example.com/' , ... }\n",
    "    \"\"\"\n",
    "    if not query or not ttl_prefixes:\n",
    "        return query\n",
    "\n",
    "    # Find existing prefixes in query (PREFIX or @prefix)\n",
    "    existing_pfx = set()\n",
    "    for m in re.finditer(\n",
    "        r\"(?i)(?:@prefix|prefix)\\s+([A-Za-z][A-Za-z0-9_-]*):\\s*<([^>]+)>\",\n",
    "        query\n",
    "    ):\n",
    "        existing_pfx.add(m.group(1))\n",
    "\n",
    "    extra_lines = []\n",
    "    for pfx, uri in ttl_prefixes.items():\n",
    "        if pfx not in existing_pfx:\n",
    "            extra_lines.append(f\"PREFIX {pfx}: <{uri}>\")\n",
    "\n",
    "    if not extra_lines:\n",
    "        return query\n",
    "\n",
    "    # Put extra prefixes at the very top, before any existing content\n",
    "    return \"\\n\".join(extra_lines) + \"\\n\" + query.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b507cc9-ff4f-4439-9a5c-ed47fe15c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eval(input_file):\n",
    "    graph_id = \"Small\" if input_file.lower().split(\"/\")[-1].startswith(\"small_\") else \"Big\"\n",
    "    END_POINT = END_POINT_DICT[graph_id]\n",
    "    evaluator = SingleGraphCQEvaluator(endpoint=END_POINT)\n",
    "    # ============================================================\n",
    "    # 3. Main processing\n",
    "    # ============================================================\n",
    "    PREFIX_TTL_FILE = f\"{graph_id.lower()}_schema_for_prefix.ttl\"  # <-- TTL file with @prefix declarations\n",
    "    ttl_prefixes = load_prefixes_from_ttl(PREFIX_TTL_FILE)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Temp + final filenames (NEW)\n",
    "    # ------------------------------------------------\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    temp_file = f\"temp_eval_{base_name}.xlsx\"\n",
    "    \n",
    "    # -------------------------\n",
    "    # 3.1 Read input spreadsheet\n",
    "    #     (resume from temp if exists)\n",
    "    # -------------------------\n",
    "    if os.path.exists(temp_file):\n",
    "        print(f\"Resuming from temp file: {temp_file}\")\n",
    "        df = pd.read_excel(temp_file)\n",
    "    else:\n",
    "        print(f\"Starting new eval from original file: {input_file}\")\n",
    "        df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Expect at least these stable column names\n",
    "    required_base_cols = [\"CQ\", \"Prompt\"]\n",
    "    for col in required_base_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "            \n",
    "    # ---------------------------------------------\n",
    "    # 3.2 Detect model-specific analysis columns\n",
    "    #     (suffix-based, model prefix can change)\n",
    "    # ---------------------------------------------\n",
    "    raw_col = None\n",
    "    result_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        lc = col.lower()\n",
    "        if lc.endswith(\"_raw\"):\n",
    "            raw_col = col\n",
    "        elif lc.endswith(\"_result\"):\n",
    "            result_col = col\n",
    "    \n",
    "    if raw_col is None or result_col is None:\n",
    "        raise ValueError(\n",
    "            \"Could not locate columns ending with '_Raw' and \"\n",
    "            \"'_Result'.\\nColumns available: \"\n",
    "            f\"{list(df.columns)}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Detected analysis columns:\")\n",
    "    print(f\"  RAW:    {raw_col}\")\n",
    "    print(f\"  RESULT: {result_col}\")\n",
    "    \n",
    "    # Derive model name from the result column (prefix before suffix)\n",
    "    model_name_suffix = \"_Result\"\n",
    "    if result_col.endswith(model_name_suffix):\n",
    "        model_name = result_col[: -len(model_name_suffix)]\n",
    "    else:\n",
    "        model_name = \"model\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3.3 Initialize / add output columns to dataframe\n",
    "    # ------------------------------------------------\n",
    "    # Basic info\n",
    "    df[\"sparql_query\"] = \"\"\n",
    "    df[\"eval_json\"] = \"\"\n",
    "    \n",
    "    # Top-level booleans/ints\n",
    "    df[\"syntax_ok\"] = \"\"\n",
    "    df[\"satisfiable\"] = \"\"\n",
    "    df[\"deterministic\"] = \"\"\n",
    "    df[\"rows\"] = \"\"\n",
    "    df[\"vars\"] = \"\"\n",
    "    \n",
    "    # Latency metrics\n",
    "    df[\"latency_p50_ms\"] = \"\"\n",
    "    df[\"latency_p95_ms\"] = \"\"\n",
    "    df[\"latency_mean_ms\"] = \"\"\n",
    "    \n",
    "    # Additional metrics from sample JSON\n",
    "    df[\"lexical_query_overlap\"] = \"\"\n",
    "    df[\"semantic_similarity_to_CQ\"] = \"\"\n",
    "    df[\"semantic_soft_coverage_to_CQ\"] = \"\"\n",
    "    df[\"tuple_cohesion\"] = \"\"\n",
    "    df[\"always_unbound_vars\"] = \"\"\n",
    "    df[\"variables\"] = \"\"  # store variables list as JSON string\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3.3.5 Temp + final filenames  (ADDED)\n",
    "    # ------------------------------------------------\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]  # e.g., Claude_big_...\n",
    "    temp_file = f\"temp_eval_{base_name}.xlsx\"                      # <<< temp file\n",
    "    output_file = f\"evaluation_results_{model_name}_{graph_id}.xlsx\"  # final file (same name as before)  # <<< moved up\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 3.4 Iterate through rows and run evaluation\n",
    "    # ------------------------------------------\n",
    "    rows_done = 0\n",
    "    if \"eval_json\" in df.columns:\n",
    "        rows_done = (df[\"eval_json\"].astype(str).str.strip() != \"\").sum()\n",
    "    print(f\"Rows already processed: {rows_done}\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip row if eval_json already has something (resumable part)\n",
    "        if \"eval_json\" in df.columns:\n",
    "            current_val = str(df.at[idx, \"eval_json\"])\n",
    "            print(current_val)\n",
    "            if current_val.strip():   # non-empty => already processed\n",
    "                continue\n",
    "        \n",
    "        cq_text = row[\"CQ\"]\n",
    "        raw_text = row[raw_col]\n",
    "        analysis_text = row[result_col]\n",
    "    \n",
    "        # Extract SPARQL query from Raw or Result columns\n",
    "        sparql_query = extract_sparql(analysis_text) or extract_sparql(raw_text) \n",
    "    \n",
    "        # Add prefixes from TTL, unless already present\n",
    "        sparql_query = add_missing_prefixes(sparql_query, ttl_prefixes)\n",
    "        \n",
    "        df.at[idx, \"sparql_query\"] = sparql_query\n",
    "    \n",
    "        if not sparql_query:\n",
    "            df.at[idx, \"eval_json\"] = \"NO SPARQL FOUND\"\n",
    "        else:\n",
    "            # ------------------------------------\n",
    "            # Per-row timeout: 15 minutes (900 s)\n",
    "            # ------------------------------------\n",
    "            try:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                    future = executor.submit(evaluator.evaluate, cq_text, sparql_query)\n",
    "                    # raises concurrent.futures.TimeoutError if > 900 seconds\n",
    "                    result = future.result(timeout=1800)\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                # Treat as timeout + invalid query\n",
    "                timeout_err = TimeoutError(\"Timeout after 30 minutes: invalid query\")\n",
    "                df.at[idx, \"eval_json\"] = f\"ERROR: {timeout_err}\"\n",
    "            except Exception as e:\n",
    "                df.at[idx, \"eval_json\"] = f\"ERROR: {e}\"\n",
    "            else:\n",
    "                # If the evaluator returns a JSON string, parse it\n",
    "                if isinstance(result, str):\n",
    "                    try:\n",
    "                        result = json.loads(result)\n",
    "                    except Exception:\n",
    "                        # Keep raw string if not valid JSON\n",
    "                        df.at[idx, \"eval_json\"] = result\n",
    "                        # <<< continue not needed; we still want to temp-save\n",
    "                    else:\n",
    "                        df.at[idx, \"eval_json\"] = json.dumps(result, indent=2)\n",
    "                else:\n",
    "                    df.at[idx, \"eval_json\"] = json.dumps(result, indent=2)\n",
    "    \n",
    "                # ---- Top-level fields ----\n",
    "                if isinstance(result, dict):\n",
    "                    df.at[idx, \"syntax_ok\"] = result.get(\"syntax_ok\", False)\n",
    "                    df.at[idx, \"satisfiable\"] = result.get(\"satisfiable\", False)\n",
    "                    df.at[idx, \"deterministic\"] = result.get(\"deterministic\", False)\n",
    "                    df.at[idx, \"rows\"] = result.get(\"rows\", 0)\n",
    "                    df.at[idx, \"vars\"] = result.get(\"vars\", 0)\n",
    "    \n",
    "                    # ---- Latency ----\n",
    "                    latency = result.get(\"latency\", {}) or {}\n",
    "                    df.at[idx, \"latency_p50_ms\"] = latency.get(\"p50_ms\")\n",
    "                    df.at[idx, \"latency_p95_ms\"] = latency.get(\"p95_ms\")\n",
    "                    df.at[idx, \"latency_mean_ms\"] = latency.get(\"mean_ms\")\n",
    "    \n",
    "                    # ---- Additional numeric metrics ----\n",
    "                    df.at[idx, \"lexical_query_overlap\"] = result.get(\"lexical_query_overlap\", 0)\n",
    "                    df.at[idx, \"semantic_similarity_to_CQ\"] = result.get(\"semantic_similarity_to_CQ\", 0)\n",
    "                    df.at[idx, \"semantic_soft_coverage_to_CQ\"] = result.get(\"semantic_soft_coverage_to_CQ\", 0)\n",
    "                    df.at[idx, \"tuple_cohesion\"] = result.get(\"tuple_cohesion\", 0)\n",
    "    \n",
    "                    # ---- Variables-related info ----\n",
    "                    always_unbound_vars = result.get(\"always_unbound_vars\", [])\n",
    "                    df.at[idx, \"always_unbound_vars\"] = json.dumps(always_unbound_vars)\n",
    "    \n",
    "                    variables = result.get(\"variables\", [])\n",
    "                    df.at[idx, \"variables\"] = json.dumps(variables)\n",
    "                else:\n",
    "                    result = dict()\n",
    "                    df.at[idx, \"syntax_ok\"] = result.get(\"syntax_ok\", False)\n",
    "                    df.at[idx, \"satisfiable\"] = result.get(\"satisfiable\", False)\n",
    "                    df.at[idx, \"deterministic\"] = result.get(\"deterministic\", False)\n",
    "                    df.at[idx, \"rows\"] = result.get(\"rows\", 0)\n",
    "                    df.at[idx, \"vars\"] = result.get(\"vars\", 0)\n",
    "    \n",
    "                    # ---- Latency ----\n",
    "                    latency = result.get(\"latency\", {}) or {}\n",
    "                    df.at[idx, \"latency_p50_ms\"] = latency.get(\"p50_ms\")\n",
    "                    df.at[idx, \"latency_p95_ms\"] = latency.get(\"p95_ms\")\n",
    "                    df.at[idx, \"latency_mean_ms\"] = latency.get(\"mean_ms\")\n",
    "    \n",
    "                    # ---- Additional numeric metrics ----\n",
    "                    df.at[idx, \"lexical_query_overlap\"] = result.get(\"lexical_query_overlap\", 0)\n",
    "                    df.at[idx, \"semantic_similarity_to_CQ\"] = result.get(\"semantic_similarity_to_CQ\", 0)\n",
    "                    df.at[idx, \"semantic_soft_coverage_to_CQ\"] = result.get(\"semantic_soft_coverage_to_CQ\", 0)\n",
    "                    df.at[idx, \"tuple_cohesion\"] = result.get(\"tuple_cohesion\", 0)\n",
    "    \n",
    "                    # ---- Variables-related info ----\n",
    "                    always_unbound_vars = result.get(\"always_unbound_vars\", [])\n",
    "                    df.at[idx, \"always_unbound_vars\"] = json.dumps(always_unbound_vars)\n",
    "    \n",
    "                    variables = result.get(\"variables\", [])\n",
    "                    df.at[idx, \"variables\"] = json.dumps(variables)\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # TEMP SAVE EVERY FEW ROWS  (ADDED)\n",
    "        # ------------------------------------------\n",
    "        if idx % 1 == 0:                       # change 5 to 1 if you want *every* row\n",
    "            df.to_excel(temp_file, index=False)\n",
    "            print(f\"Temp saved at row {idx} -> {temp_file}\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 3.5 Save new spreadsheet with model in name\n",
    "    # ------------------------------------------\n",
    "    output_file = f\"evaluation_results_{model_name}_{graph_id}.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"Saved: {output_file}\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 3.6 Remove temp file when complete (ADDED)\n",
    "    # ------------------------------------------\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "        print(f\"Removed temp: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ebcd1f-c12c-462a-b56b-f94e615b820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files_with_extension(directory, extension):\n",
    "    # List all files in the directory with the given extension\n",
    "    files = [f for f in os.listdir(directory) if f.endswith(extension)]\n",
    "    return files\n",
    "\n",
    "# Example usage\n",
    "directory = 'temp_set_2'  # Replace with your directory path\n",
    "extension = '.xlsx'  # Replace with your desired file extension\n",
    "files = get_files_with_extension(directory, extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd732ff-41f2-47d8-9d2f-112b9fa0f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_schema_trim_cq_gemma3-27b_results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new eval from original file: temp_set_2/small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "Detected analysis columns:\n",
      "  RAW:    gemma3:27b_Analysis_Raw\n",
      "  RESULT: gemma3:27b_Analysis_Result\n",
      "Rows already processed: 0\n",
      "\n",
      "Temp saved at row 0 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 1 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 2 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 3 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 4 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 5 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 6 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 7 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 8 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 9 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 10 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 11 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 12 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 13 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 14 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 15 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 16 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 17 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 18 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 19 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 20 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 21 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 22 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 23 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 24 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 25 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 26 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 27 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 28 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 29 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 30 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 31 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 32 -> temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "Saved: evaluation_results_gemma3:27b_Analysis_Small.xlsx\n",
      "Removed temp: temp_eval_small_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "big_schema_trim_cq_gemma3-27b_results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/root/.local/lib/python3.10/site-packages/SPARQLWrapper/Wrapper.py:794: RuntimeWarning: Sending Accept header '*/*' because unexpected returned format 'json' in a 'DESCRIBE' SPARQL query form\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new eval from original file: temp_set_2/big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "Detected analysis columns:\n",
      "  RAW:    gemma3:27b_Analysis_Raw\n",
      "  RESULT: gemma3:27b_Analysis_Result\n",
      "Rows already processed: 0\n",
      "\n",
      "Temp saved at row 0 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 1 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 2 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 3 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 4 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 5 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 6 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 7 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 8 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 9 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/SPARQLWrapper/Wrapper.py:794: RuntimeWarning: Sending Accept header '*/*' because unexpected returned format 'json' in a 'CONSTRUCT' SPARQL query form\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp saved at row 10 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 11 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 12 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 13 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 14 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 15 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 16 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 17 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 18 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 19 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 20 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 21 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 22 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 23 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 24 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 25 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 26 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 27 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 28 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 29 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 30 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 31 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "\n",
      "Temp saved at row 32 -> temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n",
      "Saved: evaluation_results_gemma3:27b_Analysis_Big.xlsx\n",
      "Removed temp: temp_eval_big_schema_trim_cq_gemma3-27b_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "for file in sorted(files, reverse=True):\n",
    "    if not (\"gemma\" in file):\n",
    "    # if not (\"Claude\" in file and \"big\" in file):\n",
    "        continue\n",
    "    print(file)\n",
    "    input_file = os.path.join(directory,file)\n",
    "    perform_eval(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ca13b-9131-41d6-bb44-016d03ae9cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c249cb-ae59-4b8c-8991-a6fd5758cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_results_gemma3:27b_Analysis_Small.xlsx\n",
      "\n",
      "=== Per-difficulty summary (H1 + semantic_diversity + latency, etc.) ===\n",
      "difficulty  latency_p50_ms  latency_p95_ms  latency_mean_ms  rows     vars  lexical_query_overlap  semantic_similarity_to_CQ  semantic_soft_coverage_to_CQ  tuple_cohesion  determinism_score  satisfiability_binding_score  h1_overall  semantic_diversity_score  syntax_ok_rate  satisfiable_rate  deterministic_rate\n",
      "   complex            7.62           17.14            10.74   0.0 0.125000               0.000000                   0.081250                      0.087500             0.0           0.300000                      0.200000    0.250000                  0.133333        0.300000          0.200000            0.300000\n",
      "  moderate            8.08           16.39            10.81   0.0 0.111111               0.000000                   0.076667                      0.076667             0.0           0.272727                      0.181818    0.227273                  0.121212        0.272727          0.181818            0.272727\n",
      "    simple            7.91           17.19            10.91   0.0 0.083333               0.003333                   0.055000                      0.065000             0.0           0.083333                      0.000000    0.041667                  0.027778        0.083333          0.000000            0.083333\n",
      "\n",
      "=== Overall summary for this model ===\n",
      "difficulty  latency_p50_ms  latency_p95_ms  latency_mean_ms  rows     vars  lexical_query_overlap  semantic_similarity_to_CQ  semantic_soft_coverage_to_CQ  tuple_cohesion  determinism_score  satisfiability_binding_score  h1_overall  semantic_diversity_score  syntax_ok_rate  satisfiable_rate  deterministic_rate\n",
      "       ALL            7.87       16.906667            10.82   0.0 0.103448               0.001379                   0.068966                      0.074828             0.0           0.212121                      0.121212    0.166667                  0.090909        0.212121          0.121212            0.212121\n",
      "\n",
      "Enriched per-CQ results written to: temp_set_2/evaluation_results_gemma3:27b_Analysis_Small_with_rubric.xlsx\n",
      "evaluation_results_gemma3:27b_Analysis_Big.xlsx\n",
      "\n",
      "=== Per-difficulty summary (H1 + semantic_diversity + latency, etc.) ===\n",
      "difficulty  latency_p50_ms  latency_p95_ms  latency_mean_ms  rows  vars  lexical_query_overlap  semantic_similarity_to_CQ  semantic_soft_coverage_to_CQ  tuple_cohesion  determinism_score  satisfiability_binding_score  h1_overall  semantic_diversity_score  syntax_ok_rate  satisfiable_rate  deterministic_rate\n",
      "   complex             NaN             NaN              NaN   0.0   0.0                    0.0                        0.0                           0.0             0.0           0.400000                      0.400000    0.400000                  0.200000        0.400000          0.400000            0.400000\n",
      "  moderate             NaN             NaN              NaN   0.0   0.0                    0.0                        0.0                           0.0             0.0           0.090909                      0.090909    0.090909                  0.045455        0.090909          0.090909            0.090909\n",
      "    simple             NaN             NaN              NaN   0.0   0.0                    0.0                        0.0                           0.0             0.0           0.333333                      0.333333    0.333333                  0.166667        0.333333          0.333333            0.333333\n",
      "\n",
      "=== Overall summary for this model ===\n",
      "difficulty  latency_p50_ms  latency_p95_ms  latency_mean_ms  rows  vars  lexical_query_overlap  semantic_similarity_to_CQ  semantic_soft_coverage_to_CQ  tuple_cohesion  determinism_score  satisfiability_binding_score  h1_overall  semantic_diversity_score  syntax_ok_rate  satisfiable_rate  deterministic_rate\n",
      "       ALL             NaN             NaN              NaN   0.0   0.0                    0.0                        0.0                           0.0             0.0           0.272727                      0.272727    0.272727                  0.136364        0.272727          0.272727            0.272727\n",
      "\n",
      "Enriched per-CQ results written to: temp_set_2/evaluation_results_gemma3:27b_Analysis_Big_with_rubric.xlsx\n"
     ]
    }
   ],
   "source": [
    "from hypothesis_specific_eval import add_hyp_scores\n",
    "directory = \"temp_set_2\"\n",
    "files = get_files_with_extension(directory, extension)\n",
    "files = [file for file in files if \"evaluation\" in file and file.endswith(\"xlsx\")]\n",
    "\n",
    "for file in sorted(files, reverse=True):\n",
    "    print(file)\n",
    "    input_file = file\n",
    "    add_hyp_scores(f\"{directory}/{input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92da6a7-1167-4451-804a-681b0e7457f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
